## Introduction {.page_break_before}

Rare disease research is increasingly dependent on high-throughput profiling of samples and would greatly benefit from machine learning (ML) analytics. 
A systematic review of application of ML in rare disease in the last 10 years uncovered 211 human data studies in 74 different rare diseases employing ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) [@doi:10.1186/s13023-020-01424-6]. 
While the review points to the increasing popularity of using ML methods in rare disease, there are various hurdles that are inherent to such datasets.
ML based methods benefit from using large datasets, but analyzing high dimensional data from rare diseases datasets that typically contain 20 to 99 samples is challenging [@https://www.fda.gov/media/99546/download; @doi:10.1186/s13023-020-01424-6].
Small datasets lead to a lack of statistical power and magnify the susceptibility of ML methods to misinterpretation and unstable performance.
Additionally, successful training of ML models require training datasets made of “gold standard” data where the diagnosis or label of a data point has very little uncertainty (or “label-noise”) associated with it [@doi:10.1093/jamia/ocw028]. 
Due to limited understanding of the biology of rare diseases, the symptoms or disease labels often come with significant label-noise (a _silver standard_ dataset) [@doi:10.1109/tnnls.2013.2292894]. [TODO: we use "label-noise" and "high label-uncertainty" somewhat interchangably, or drop "label-noise" altogether when we get to the model complexity section]
Thus, specialized computational methods that can learn patterns from small datasets and can generalize to newly acquired data are required for rare disease applications [@doi:10.1016/j.ebiom.2019.08.027]. 
In this perspective, we first highlight ML approaches that address or better tolerate the limitations of rare disease data, and then discuss the future of ML applications in rare disease.
