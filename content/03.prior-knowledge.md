### Techniques that build on prior knowledge and indirectly related data are necessary for many rare disease applications {.page_break_before}
This section will highlight promising approaches for analyzing rare disease data to extract biological insights. 
We will discuss techniques like cascade learning, transfer learning, representation learning, integrative analysis, and knowledge-graph creation that leverage other knowledge and data sources to construct testable hypotheses from rare diseases datasets with limited sample sizes.

Implementing machine learning on data with low sample size and high label uncertainty can lead to unstable predictions. 
In such cases where single predicting algorithms fail, various machine learning methods together or _ensemble learning_ may help increase accuracy of prediction.
_Ensemble learning_ methods like random forests use bootstrap aggregation (or _bagging_) of independent decision trees that use similar parameters but different paths for the selection of features to form a consensus about the important predictive features[@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324, @doi:10.1214/aos/1031689014, @doi:10.1177/2045894019890549, @doi:10/btzfh6].
However, successful application of consensus based ensemble learning requires "gold standard" data where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "label-noise") leading to a _silver standard_ dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
In such cases, ensemble learning with multiple methods leveraging distinct underlying assumptions are used in tandem to capture stable patterns existing in the _silver standard_ data and reduce uncertainty.
Such _cascade learning_ classifiers have been widely used in image recognition where initially a small subset of image features are used to classify images (e.g. features identifying a face like eyes, nose, mouth). The initial classification is then augmented by more complex features and algorithms like AdaBoost (_boosting_ ) that weight the various features implemented to detect the content of the image (e.g. features identifying a human face like relative distance between eyes etc.)[@doi:10.1109/CVPR.2001.990537, @doi:10.1007/978-3-540-75175-5_16, @doi:10.1109/ICPR.2004.1334680].

#### Cascade learning in rare disease

In rare diseases, a variation of _cascade learning_ was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a different independent learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to resolve sample labels and reiterating step (2).
In this implementation the algorithm was able to identify rare disease patients due to the robustness conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
The synergy of separate algorithms underlying the label prediction step, the classification step, and the prediction refinement step was able to perform better than other ensemble methods when implemented on this silver standard dataset.

Most cascade classifiers follow _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But instances where the need for high prediction accuracy for one class outweighs other classes (e.g. malignant tumor-types, or severe psychiatric cases) require further modification of the cascade learning efforts.
An example of this was implemented for triaging psychiatric patients where the identification of one class of psychiatric patients ("severe") far outweighed the need for optimized overall classification accuracy[@pmid:30380082].
Due to the requirements of the problem, they adapted a _one-class-at-a-time_ approach for cascade learning, where at each stage a binary classifier is used to predict a specific class against all others.
The final model implemented all models together each identifying one class sequentially and the final prediction was the union of the predictions at all the different models.
The cascade classifiers using the _one-class-at-a-time_ approach were found to perform better than multi-class ensemble classifiers in most cases.

Thus ensemble learning can be helpful in producing stable predictions from rare disease data, but the choice of using _bagging_, _boosting_, independent algorithmic steps, or _one-class-at-a-time_ approach depends on the nature of the prediction question.
So far the limited success of the _bagging_ approach in rare disease has encouraged various methodological modifications as discussed above.
