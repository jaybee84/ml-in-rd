### Manage model complexity while preserving the value of machine learning


<!-- TODO: stick to one example method of each approach so that we get to describe why we need to use it --->

Machine learning methods are generally accompanied by a few critical assumptions. 
Assumption 1, the dataset contains equal number of samples for all classes in a dataset (i.e. no imbalance among classes). 
Assumption 2, the dataset is complete with adequate samples for all features (lack of sparsity).
Assumption 3 (for supervised learning methods), there is no ambiguity about the labels for the samples in the dataset (i.e. no label-noise).
In reality, the rare disease datasets violate many of these assumptions.
There is generally a high class imbalance due to small number of samples for specific classes, abundant label-noise due to incomplete understanding of the disease, and in many cases sparsity in data.
All of these contribute to low signal to noise ratio in rare disease datasets.
Thus applying ML to rare disease data without any accomodations for the above shortcomings may lead to models that lack stability required for reproducibility or simplicity required for interpretation.
In this section we highlight a few common ML techniques that can accomodate the three above mentioned aspects and improve the stability and simplicity of ML models applied to rare disease data.

Stability in predictions while accomodating class imbalance in datasets can be achieved through decision tree based ensemble learning methods (Figure[@fig:2]A-B).
Random forests use resampling (with replacement) based techniques like _bagging_ (bootstrap aggregation) of independent decision trees to form a consensus about the important predictive features [@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6]. 
Additional approaches like combining random forests with resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
But recent studies suggest that there are limitations to decision tree-based ensemble methods when applied to rare disease datasets with substantial class imbalance and label-noise [@doi:10.1007/s11634-019-00354-x; @pmid:30815073]. 
This has led to the adoption of cascade learning, a variant of ensemble learning, where multiple methods leveraging distinct underlying assumptions are used in tandem; and augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
For example, a cascade learning approach for identifying rare disease patients from electronic health record data utilized independent steps for feature extraction (word2vec [@arxiv:1301.3781]), preliminary prediction with ensembled decision trees, and prediction refinement using data similarity metrics [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets also decreases the available data points per class. 
In such cases with class imbalance, one may also use inverse class weighting or oversampling (SMOTE) or cascade learning approaches that adequately represent the rarer classes. (Figure[@fig:2]D)

Presence of label-noise and sparsity in the data can lead to overfitting of models to the training data, such that the models show high prediction accuracy on training data but low prediction accuracy on encountering new data points. 
Thus features identified as predictive by such overfit models are often found to be tailored to the training dataset making the model overtly complex, and not generalizable to new data points. [@doi:10.1073/pnas.1900654116]
Regularization can not only protect ML models from poor generalizability caused by overfitting, but also reduce model complexity by reducing the feature space available for training. (Figure[@fig:2]C)
Regularization is often used in rare variant discovery and immune cell signature discovery studies which are useful examples that also need to accomodate sparsity of data like rare diseases.
For example, among various regularization methods (ridge regression, LASSO regression, and elastic-net)[@doi:10.1111/j.1467-9868.2005.00503.x], hybrid applications of LASSO for capturing combinations of rare variants have proven beneficial [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113] .
In the context of rare immune cell signature discovery, elastic-net regression was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1186/s12859-019-2994-z]. 
Thus regularization methods like LASSO or elastic-net have been methods of choice while working with rare observations; use of these regularization approaches should be considered while working with rare disease datasets. 


![Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) A schematic showing the concept of bootstrap, B) A schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) A schematic showing the concept of regularization to selectively learn relevant features, D) A schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](images/figures/pdfs/statistical-techniques.png){#fig:2}
