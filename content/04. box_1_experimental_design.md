## Box 1: Understanding experimental design of ML to inform requirements for data

### Components of ML experiments

Machine learning (ML) algorithms identify patterns that explain or fit a given dataset.
Every ML algorithm goes through _training_, where it identifies underlying patterns in a given dataset to create a "trained" algorithm (a _model_), and a _testing_ phase where the model applies the identified patterns to unseen data points.
Typically, a ML algorithm is provided with: 1. a _training dataset_ , 2. an _evaluation dataset_ , 3. a _held-out validation dataset_.
These input data can be images, text, numbers, or other types of data which are typically encoded as a numerical representation of the input data. 
A _training dataset_ is used by the model to learn underlying patterns from the features present in the data of interest.
An _evaluation dataset_ is a small and previously unused dataset which is used during the training phase to help the model iteratively update its parameters (i.e., _hyperparameter tuning_ or _model tuning_).
In many cases, a large training set may be subdivided to form a smaller training dataset and the evaluation dataset, both of which are used to train the model.
In the testing phase, a completely new or unseen test dataset or _held-out validation set_ is used to test whether the patterns learned by the model hold true in new data (i.e., they are _generalizable_).
While the evaluation dataset helps us refine a model's "knowledge" of patterns in the training data, the held-out validation set helps us test the generalizability of the model.

If a model is generalizable, it is able to make accurate predictions on new data.
High generalizability of a model on previously unseen data suggests that the model has identified important patterns in the data that are not unique to the data used for training and tuning.
Generalizability can be affected if _data leakage_ occurs during training of the model, i.e., if a model is exposed to the same or related data points in both the training set and the held-out test set.
Ensuring absence of any overlap or relatedness among data points or samples used in the training set and evaluation set is important to avoid data leakage during model training.
Specifically, in cases of rare genetic diseases where, for example, many samples can contain familial relationships or data from the same patient could be collected by multiple specialists at different clinical facilities, special care should be taken while crafting the training and testing sets to ensure that no data leakage occurs and the trained model has high generalizability.

### Training and testing

The implementation of a ML experiment begins with splitting a single dataset of interest into data used for training (generally subdivided into the _training dataset_ and the _evaluation dataset_), and data used for testing or validation (as the _held-out validation dataset_). 
It is common to use 90% of a dataset for training and 10% for testing (also referred to as a 90/10 split).
This ensures that all of the datasets involved in training and testing a model have uniform features.
In rare diseases where multiple datasets may be combined to make a large enough training dataset, special care should be taken to standardize the features and the patterns therein.
The iterative training phase helps the model learn important patterns in the training dataset and then use the evaluation dataset to test for errors in prediction and update its learning parameters (_hyperparameter tuning_).
The method by which the evaluation dataset tests the performance of the trained model and helps update the hyperparameters is called _cross-validation_.
There are multiple approaches that can be deployed to maximally utilize the available data when generating training and evaluation datasets e.g. leave-p-out cross-validation, leave-one-out cross-validation, k-fold cross-validation, Monte-Carlo random subsampling cross-validation [@doi:10.1007/978-1-4614-6849-3].
In the case of k-fold cross-validation, a given dataset is shuffled randomly and split into _k_ parts.
One of the _k_ parts is reserved as the _evaluation dataset_ and the rest are cumulatively used as the _training dataset_.
In the next iteration, a different part is used as the _evaluation dataset_, while the rest are used for training.
Once the model has iterated through all _k_ parts of the training and evaluation datasets, it is ready to be tested on the _held-out validation_ dataset. (Figure {@fig:1}b)

The held-out validation dataset is exposed to the model only once to estimate the accuracy of the model.
High accuracy of a model during cross-validation but low accuracy on the held-out dataset is a sign that the model has become overfit to the training set and has low generalizability.
If this is encountered, the modeller should revisit the construction of the dataset to make sure they meet the best practices outlined above.