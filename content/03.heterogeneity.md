<!--TODO: I don't know about this title, but I think "there are too many dang dimensions" would be even worse; that's what I think this section should be mostly about, with assessing heterogeneity as a specific use case -->

### Characterizing structure in high-dimensional rare disease data ðŸŽƒ

<!-- TODO: Talk about curse of dimensionality, etc. or even how interpreting many features for biological discovery can be somewhat challenging. -->
<!-- TODO: If we talk about *feature selection* in addition to feature/representation learning, that might set up model complexity nicely! --> 

#### Dimensionality reduction and representation learning

<!-- TODO: Use the bits about dimensionality reduction that were in this section originally to talk about dimensionality reduction in general. Readers who are new practicioners might be familiar with or have used PCA - that's ML, too! -->

<!-- TODO: What is dimensionality reduction? -->

<!-- This is the section on dimensionality reduction that was here before when this was only about heterogeneity, largely unaltered. -->

Dimensionality reduction methods can be used to visualize heterogeneity and confounders, including multidimensional scaling, principal components analysis, t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP), among others. [@doi:10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202; @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @arXiv:1802.03426]
Dimensionality reduction techniques are not restricted to 'omic' data - they can also be used in rare disease applications to characterize the structure and heterogeneity of imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], and others.
All of these methods can be used to identify batch effects and other structure in the data, though some (like t-SNE and UMAP) require parameters that can affect the output [@doi:10.23915/distill.00002;@arXiv:1802.03426].
Therefore, obtaining a clear interpretation from these methods requires understanding the underlying approach and parameters.
Another important consideration is discussed by Way, et. al. [@doi:10.1186/s13059-020-02021-3]: a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; testing multiple methods may result in a more comprehensive portrait of the data.

<!-- This is the representation learning section that was previously in the prior knowledge section with the in-depth discussion of Dincer et al. removed. It should be adapted to reflect that some of the methods described above are forms of representation learning and probably revised because I just took out the most compelling part to save for later, in my opinion. -->

Representation learning, also called feature learning, is the process of learning features from raw data, where a feature is an individual variable.
An algorithm or approach will construct features as part of training and, in a supervised application, use those features to predict labels on input data.
Using an example from transcriptomics, an unsupervised method such as matrix factorization can be used to extract a low-dimensional representation of the gene-level data, learning features that are a combination of input genes' expression levels [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can then be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8].
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1098/rsif.2017.0387]), which are trained to predict labels or classes, are also an example of representation learning.
Learned features in the medical imaging domain may be a series of edges representing a blood vessel formation that discriminates between disease states.
Features learned from transcriptomic data could be coordinated sets of genes involved in a biological process that are descriptive in some way [@doi:10.1038/s41467-020-14666-6].
Representation learning tends to be data-intensive; many samples are required.
In later sections, we will introduce methods that can leverage data that do not directly assay a rare disease of interest; representation learning underlies many of time.

#### Visualization and correction of technical effects

<!-- TODO: Use the section above to introduce this (assessment of batch effects, etc.) as a specific use case/motivation for the methods introduced above that is perhaps exacerbated in rare diseases. -->

<!-- There can be structure in our data that is not related to what we want to study... -->

<!-- TODO: Refs! -->
Rare disease data often suffers from artifacts introduced by non-biological phenomena such as batch or assay platform.[@doi:10.1016/j.cels.2019.04.003, @doi:10.1186/s13023-020-01376-x, @doi:10.1038/s41591-019-0457-8]
The consequences of these artifacts are amplified when there are few samples and heterogeneous phenotypes.
Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables.
Collaboration with domain experts may result in unexpected insight into potential sources of variation.
As an example, consider a study of neurofibromatosis type 1 (NF1) datasets. <!-- TODO: REF -->
The NF1 datasets were comprised of samples obtained with different surgical techniques, resulting in differences that were a consequence of sample collection method rather than differences in biology. <!-- TODO: not sure how this works here - can we add a take-home point? -->
Consequently, careful assessment of and accounting for confounding factors is critical to identifying meaningful features within a dataset.

Assessment of confounding factors and heterogeneity is perhaps most easily performed using unsupervised learning approaches.
K-means clustering or hierarchical clustering can be used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497;@doi:10.1109/JBHI.2013.2276766] <!-- TODO: Make reference to the dimensionality reduction section above -->

Once the nature of the non-biological heterogeneity has been established, different techniques can be used to correct the differences.
Common approaches include reprocessing the raw data using a single analysis pipeline if the data are obtained from different sources, application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25].
It is also important to be realistic when working with rare disease data.
For various reasons including ethical constraints, funding, and limited biospecimens, experimental design and the resulting data will often be less-than-ideal.
In these cases, it may be prudent to take a step back, re-evaluate the data, and identify methods that can operate within the constraints of the data, rather than expecting the data to conform to a method of choice.