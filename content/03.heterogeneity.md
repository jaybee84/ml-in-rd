### Manage complex high-dimensional rare disease data

In rare diseases, the ability to get an enormous number of measurements from a vanishingly small number of samples using high-throughput methods is both the upside and the downfall of these methods. These ‘omic’ methods generate highly dimensional data - that is, data with many features (e.g. all of the mRNA transcripts in a sample). 
However, statistical interrogation of  this large number of measurements requires an abundance of samples or observations, which is often not the case in rare disease. The sparsity of samples in rare diseases gives rise to the “curse of dimensionality” (i.e. few samples but many features), which can be a major impediment in analyzing feature-rich data in sample-deficient contexts [@doi:10.1038/nrc2294] [TODO: Clarify further why this can be an impediment]. 
This problem is further aggravated by the presence of highly correlated features that relate to disease relevant information. Furthermore, rare disease data collection and aggregation methods can add to these challenges by introducing technical variability into the data at hand. 
In this section, we will discuss strategies like simplifying data and addressing technical artifacts through dimension reduction  which can help mitigate these challenges. 
Dimensionality reduction methods including unsupervised approaches like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) [@doi: 10.1007/978-3-540-33037-0_14
; @doi:10.1098/rsta.2015.0202, @http:https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @https:https://arxiv.org/abs/1802.03426] can help ‘compress’ information from a large number of features into a smaller number of features. 
These  techniques can be applied to characterize imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], ‘omics data, and others. These methods not only help in reducing the number of features, but can also be used to visualize structure or artifacts in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), define subgroups within data (e.g. [@doi:10.1038/s41467-020-15351-4], or for feature selection/extraction during application of specific machine learning models[@doi:10.1007/978-3-030-03243-2_299-1].

Rare disease datasets, like many other scenarios, can contain structure unrelated to the biology of the disease; e.g. structure related to batch, sample preparation methodology, or sequencing platform [@doi:10.23915/distill.00002]. 
The consequences of these artifacts are amplified when samples are rare and the cohort contains several phenotypes. Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables. 
We can leverage dimensionality reduction methods like PCA, MDS, t-SNE, and UMAP to identify the effect of these variables on the data. All of these methods can be used to identify batch effects and other structures in the data, though some (like t-SNE and UMAP) may require tuning of hyperparameters that affect the output [@http:https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002]. 
Way, et. al. [@doi:10.1186/s13059-020-02021-3] further suggests that a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; thus testing multiple methods may result in a more comprehensive portrait of the data. 
Additional important considerations for using dimensionality reduction methods such as criteria for selecting a dimensionality reduction method and interpretation of results are discussed in detail by Nguyen and Holmes.[@doi:10.1371/journal.pcbi.1006907] 

Beyond dimensionality reduction, unsupervised learning approaches such as k-means clustering or hierarchical clustering have also been used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766] 
If non-biological heterogeneity is detectable, common approaches like  reprocessing the raw data using a single analysis pipeline (if the data are obtained from different sources), application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25] may be required to obtain value from these datasets. 

Dimensionality reduction is, in fact, a type of representation learning (or feature learning), a process of learning low-dimensional representations or composites of features from raw data. 
Each learned composite feature becomes a new variable - representing a combination of original features - thereby reducing the dimension of the dataset. 
Representation learning through matrix factorization can extract composite features from transcriptomics datasets made of combinations of gene expression levels found in the training data that are descriptive in some way [@doi: 10.1038/s41467-020-14666-6], and use them to interpret test input data [@doi:10.1093/bioinformatics/btq503 ; @doi:10.1186/s13059-020-02021-3]. 
Putting constraints on the features learned by the model, through regularization, can help ensure that the learned representations are biologically relevant [TODO: REF]. 
Low-dimensional representations trained on a collection of transcriptomic data can also be used as input to supervised machine learning methods[@doi:10.1186/s12859-020-3427-8; @doi:10.1016/j.procs.2016.07.014; @doi:10.1098/rsif.2017.0387]

Representation learning generally requires many samples  in complex biological systems and thus may appear to aggravate the curse of dimensionality. 
But it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In the later sections of this perspective, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer. 
