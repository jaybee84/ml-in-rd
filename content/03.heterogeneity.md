### Manage complex high-dimensional rare disease data

In rare diseases, the high throughput ‘omic’ methods generate highly dimensional data – data with many features, such as all of the mRNA transcripts in a sample – from a vanishingly small number of samples.
A lack of samples gives rise to the “curse of dimensionality” (i.e., few samples but many features), which is an impediment in analyzing feature-rich data in sample-deficient contexts such as rare disease.[@doi:10.1038/nrc2294] (Figure {@fig:1}A-B)
In particular, increased numbers of features results in increased sparsity (missing observations), more dissimilarity between samples, and increased redundancy between individual features or combinations of features [@doi:10.1038/s41592-018-0019-x], which creates a challenging prediction problem. 
Furthermore, rare disease data collection and aggregation methods can add to these challenges by introducing technical variability into the data at hand.
In this section, we will discuss strategies for reducing the feature space and addressing technical artifacts through dimensionality reduction.

Dimensionality reduction methods like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) can help ‘compress’ information from a large number of features into a smaller number of features in an unsupervised manner [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://www.jmlr.org/papers/v9/vandermaaten08a.html; @https://arxiv.org/abs/1802.03426]. (Figure {@fig:1}C)
These methods not only help in reducing the number of features in various types of data [@doi:10.1016/j.media.2020.101660; @doi:10.1038/ncomms14825], but can also be used to visualize structure or artifacts in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), to define sample subgroups (e.g. [@doi:10.1038/s41467-020-15351-4], or for feature selection and extraction during application of specific machine learning models.[@doi:10.1007/978-3-030-03243-2_299-1] (Figure {@fig:1}D)

Rare disease datasets are often combined from multiple small studies leading to the confounding of biological characteristics with technical variables such as batch, sample preparation methodology, or sequencing platform [@doi:10.23915/distill.00002]. 
Methods like PCA, MDS, t-SNE, and UMAP can successfully identify the effect of these variables on the original data, though t-SNE and UMAP may require tuning of hyperparameters that may effect the output [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002].
Furthermore, testing multiple dimensionality reduction methods, rather than a single method, may be necessary to obtain a more comprehensive portrait of the data [@doi:10.1186/s13059-020-02021-3]. 
Nguyen and Holmes discuss additional important considerations for using dimensionality reduction methods such as selection criteria and interpretation of results [@doi:10.1371/journal.pcbi.1006907].
Beyond dimensionality reduction, other unsupervised learning approaches such as k-means clustering or hierarchical clustering have been used to characterize the structure present in genomic or imaging data [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766].
Other approaches like reprocessing the data using a single pipeline (when data are obtained from multiple sources), using batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalizing raw values [@doi:10.1186/gb-2010-11-3-r25] may be necessary to obtain meaningful insights from the data. 

Dimensionality reduction, or more fundamentally, representation learning, learns low-dimensional representations (composite features) from the raw data. 
For example, representation learning through matrix factorization can extract features from transcriptomics datasets that are made of combinations of gene expression values found in the training data [@doi:10.1038/s41467-020-14666-6], and use them to interpret test data [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
To ensure that the learned representations are generalizable to other data, the features learned by the model can be constrained through methods like regularization [@doi:10.1371/journal.pgen.1004754, @doi:10.1002/sim.6782]. 
Representation learning generally requires many samples when applied to complex biological systems and therefore may appear to aggravate the curse of dimensionality. 
However, it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In later sections, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer learning. 

![Dimension reduction can help manage the curse of dimensionality in rare disease data. A) Multiple datasets (shapes) with multiple phenotypes (purple, green) are combined for an analysis. The data (e.g., transcriptomic data) are highly dimensional, having thousands of features (f1-f100000). B) Evaluating the features, it appears that a combination of features (e.g., expressed genes) partition the purple samples from the green samples. C) Applying a dimensionality reduction method (e.g., PCA) condenses these features into new features (e.g., New Feature 1, a combination of f1, f2 .... f100000, and New Feature 2, a different combination of f1, f2 .... f100000). New Feature 1 describes the difference in input dataset (shapes) while New Feature 2 describes the difference in phenotype (color). D) New features (F1-F1000) can be used to interrogate the biology of the input samples, develop classification models, or use other analytical techniques that would have been more difficult with the original dataset dimensions.](images/figures/pdfs/dimensionality-reduction.png){#fig:1}
