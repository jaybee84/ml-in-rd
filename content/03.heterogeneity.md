### Manage complex high-dimensional rare disease data

The great irony of studying rare diseases with genomics, transcriptomics, or other similar methods that the ability to get an enormous number of measurements from a vanishingly small number of samples, is both the upside and the downfall of these methods.
These 'omic' methods generate highly dimensional data - that is, data with many features - such as all of the mRNA transcripts in a sample. 
However, to make sense of these many features (in other words, to be able to statistically interrogate these measurements), we must have many samples, or observations, which is often not the case in rare disease. 
This is known as the "curse of dimensionality," and can be a major impediment to the use of data-rich methods in sample-deficient contexts [@doi:10.1038/nrc2294].
Additionally, simply untangling the many correlated features and interpreting how they relate to the biological question at hand can make the use of highly dimensional rare disease data for discovery daunting. 
Furthermore, rare disease data collection and aggregation methods can further complicate these challenges by introducing technical variability into the data at hand. 
In this section, we'll discuss strategies to consider that can help mitigate these challenges such as simplifying data by reducing the dimensionality of high-dimensional datasets as well as detecting and correcting technical artifacts. 

<!-- TODO: If we talk about *feature selection* in addition to feature/representation learning, that might set up model complexity nicely! --> 

#### Dimensionality reduction and representation learning

Dimensionality reduction simply describes any method that 'compresses' information from many features into a smaller number of features.
Methods such as multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) - which are, themselves, unsupervised machine learning methods - can be used to compress data from many dimensions to a few. [@doi:10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202; @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @arXiv:1802.03426]
Of note, these techniques are not restricted to 'omic' data - they can also be used in rare disease applications to characterize imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], and others.
The output of these  can then be used to visualize structure in the data, used to define subgroups within data (either biological, or technical, as discussed in the next section), or can be used as part of a machine learning procedure. 
<!--RJA TODO: Add examples? Not sure how useful-->

In fact, dimensionality reduction is the core concept underpinning a class of ML called representation learning (or feature learning).
Representation learning is the process of learning features from raw data, where a feature is an individual variable.
An algorithm or approach will construct features as part of training and, in a supervised application, use those features to predict labels on input data.
Using an example from transcriptomics, an unsupervised method such as matrix factorization can be used to extract a low-dimensional representation of the gene-level data, learning features that are a combination of input genes' expression levels [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can then be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8].
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1098/rsif.2017.0387]), which are trained to predict labels or classes, are also an example of representation learning.
Learned features in the medical imaging domain may be a series of edges representing a blood vessel formation that discriminates between disease states.
Features learned from transcriptomic data could be coordinated sets of genes involved in a biological process that are descriptive in some way [@doi:10.1038/s41467-020-14666-6].
<!-- RJA NOTE: this next sentence seems to be in conflict with the crux of the intro I wrote to this section - not sure how best to fix this --> 
Representation learning tends to be data-intensive; many samples are required.
In later sections, we will introduce methods that can leverage data that do not directly assay a rare disease of interest; representation learning underlies many of time.

#### Visualization and correction of technical effects

Another application of dimensionality reduction methods is to assess the presence or absence of unwanted signal in the data. 
There is sometimes structure in our data that is not related to what we want to study; for example, structure related to batch, sample preparation methodology, or sequencing platform. 
Structure introduced by these variables can obscure the biologically-important information that we seek. 
However, we can leverage dimensionality reduction methods such as PCA, MDS, t-SNE, and UMAP to identify the effect of these variables on the data. 
All of these methods can be used to identify batch effects and other structure in the data, though some (like t-SNE and UMAP) require parameters that can affect the output [@doi:10.23915/distill.00002;@arXiv:1802.03426].
Therefore, obtaining a clear interpretation from these methods requires understanding the underlying approach and parameters.
Another important consideration is discussed by Way, et. al. [@doi:10.1186/s13059-020-02021-3]: a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; testing multiple methods may result in a more comprehensive portrait of the data.

<!-- TODO: Refs! -->
Rare disease data often suffers from artifacts introduced by non-biological phenomena such as batch or assay platform.[@doi:10.1016/j.cels.2019.04.003, @doi:10.1186/s13023-020-01376-x, @doi:10.1038/s41591-019-0457-8]
The consequences of these artifacts are amplified when there are few samples and heterogeneous phenotypes.
Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables.
Collaboration with domain experts may result in unexpected insight into potential sources of variation.
As an example, consider a study of neurofibromatosis type 1 (NF1) datasets.[@doi:10.3390/genes11020226]
These datasets were, unbeknownst to the computational biologists, generated from samples obtained with vastly different surgical techniques (laser ablation and excision vs standard excision), resulting in substantial biological differences that are a consequence of process, not reality. One might expect, in this example, that this technical decision would result in profound changes in the underlying biology, such as the activation of heat shock protein related pathways, unfolded protein responses, and so on. 
Consequently, careful assessment of and accounting for confounding factors is critical to identifying meaningful features within a dataset.

Assessment of confounding factors and heterogeneity is perhaps most easily performed using unsupervised learning approaches.
K-means clustering or hierarchical clustering can be used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497;@doi:10.1109/JBHI.2013.2276766] <!-- TODO: Make reference to the dimensionality reduction section above -->

Once the nature of the non-biological heterogeneity has been established, different techniques can be used to correct the differences.
Common approaches include reprocessing the raw data using a single analysis pipeline if the data are obtained from different sources, application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25].
It is also important to be realistic when working with rare disease data.
For various reasons including ethical constraints, funding, and limited biospecimens, experimental design and the resulting data will often be less-than-ideal.
In these cases, it may be prudent to take a step back, re-evaluate the data, and identify methods that can operate within the constraints of the data, rather than expecting the data to conform to a method of choice.
