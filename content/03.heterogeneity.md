### Manage complex high-dimensional rare disease data

The great irony of studying rare diseases with genomics, transcriptomics, or other similar high-throughput methods is that the ability to get an enormous number of measurements from a vanishingly small number of samples, is both the upside and the downfall of these methods.
These 'omic' methods generate highly dimensional data - that is, data with many features (e.g. all of the mRNA transcripts in a sample). 
However, to be able to statistically interrogate these measurements, one needs many samples, or observations, which is often not the case in rare disease. 
This is known as the "curse of dimensionality," and can be a major impediment in analyzing feature-rich data in sample-deficient contexts [@doi:10.1038/nrc2294].
Additionally, identifying various correlated features and interpreting how they relate to the biological question of interest can make using highly dimensional rare disease data for discovery challenging. 
Furthermore, rare disease data collection and aggregation methods can further complicate these challenges by introducing technical variability into the data at hand. 
In this section, we will discuss strategies like simplifying data by reducing the dimensionality of high-dimensional datasets as well as detecting and correcting technical artifacts, that can help mitigate these challenges. 

Dimensionality reduction methods including unsupervised machine learning methods like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) can help ‘compress’ information from large number of features into a smaller number of features. [@doi:10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202; @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @arXiv:1802.03426]
Use of these techniques is not restricted to 'omics' data, they can also be used in rare disease applications to characterize imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], and others.
These methods not only help in reducing the number of features, but can also be used to visualize structure in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), used to define subgroups within data (e.g. [@doi:10.1038/s41467-020-15351-4], or can be used for feature selection/extraction during application of specific machine learning models[@doi:10.1007/978-3-030-03243-2_299-1]. 

In fact, dimensionality reduction is a core concept underpinning a class of ML called representation learning (or feature learning).
Representation learning is the process of learning low-dimensional representations or _composites_ of features from raw data, where each learned feature becomes an individual variable thus reducing the dimensions of the dataset.
One such unsupervised approach, matrix factorization, can extract composite features from transcriptomics datasets made of combinations of gene expression levels, found in the training data that are descriptive in some way, and use them to interpret test input data [@doi:10.1038/s41467-020-14666-6; @doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can also be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8].
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1098/rsif.2017.0387]), also use representation learning.
Learned composite features in the medical imaging domain may include a series of edges representing a blood vessel formation that discriminates between disease states.

Representation learning generally tends to be data-intensive (i.e. many samples are required) and thus may seem to aggravate the curse of dimensionality.
But representation learning when applied to learn low dimensional patterns from large datasets and then applying those patterns to smaller but related datasets can be a powerful tool for dimensionality reduction for smaller datasets.
In the later sections of this perspective, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as _feature-representation-transfer_ .
In later sections, we will introduce methods that can leverage data that do not directly assay a rare disease of interest; representation learning underlies many of time.

Another application of dimensionality reduction methods is to assess the presence or absence of unwanted signal in the data. 
Rare disease datasets can sometimes contain structure unrelated to the biology of the disease, e.g. structure related to batch, sample preparation methodology, or sequencing platform [@doi:10.1016/j.cels.2019.04.003, @doi:10.1186/s13023-020-01376-x, @doi:10.1038/s41591-019-0457-8]. 
The consequences of these artifacts are amplified when there are few samples and heterogeneous phenotypes. 
Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables. 
We can leverage dimensionality reduction methods like PCA, MDS, t-SNE, and UMAP to identify the effect of these variables on the data. 
All of these methods can be used to identify batch effects and other structure in the data, though some (like t-SNE and UMAP) may require parameters that can affect the output [@doi:10.23915/distill.00002;@arXiv:1802.03426].
Therefore, obtaining a clear interpretation from these methods requires understanding their underlying approach and parameters.
Way, et. al. [@doi:10.1186/s13059-020-02021-3] further suggests that a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; thus testing multiple methods may result in a more comprehensive portrait of the data.

In addition to methodological considerations, collaboration with domain experts may result in unexpected insight into potential sources of variation.
As an example, consider a study of neurofibromatosis type 1 (NF1) datasets.[@doi:10.3390/genes11020226]
These datasets were, unbeknownst to the computational biologists, generated from samples obtained with vastly different surgical techniques (laser ablation and excision vs standard excision), resulting in substantial biological differences that are a consequence of process, not reality. One might expect, in this example, that this technical decision would result in profound changes in the underlying biology, such as the activation of heat shock protein related pathways, unfolded protein responses, and so on. 
Consequently, careful assessment of and accounting for confounding factors is critical to identifying meaningful features within a dataset.

Assessment of confounding factors and heterogeneity is perhaps most efficiently performed using unsupervised learning approaches.
K-means clustering or hierarchical clustering can be used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497;@doi:10.1109/JBHI.2013.2276766] <!-- TODO: Make reference to the dimensionality reduction section above -->

Once the nature of the non-biological heterogeneity has been established, different techniques can be used to correct the differences.
Common approaches include reprocessing the raw data using a single analysis pipeline if the data are obtained from different sources, application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25].
It is also important to be realistic when working with rare disease data.
For various reasons including ethical constraints, funding, and limited biospecimens, experimental design and the resulting data will often be less-than-ideal.
In these cases, it may be prudent to take a step back, re-evaluate the data, and identify methods that can operate within the constraints of the data, rather than expecting the data to conform to a method of choice.
