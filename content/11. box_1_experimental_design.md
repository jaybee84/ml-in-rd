## Box 1: Understanding experimental design of ML to inform requirements for data

### Components of ML experiments

Machine learning (ML) algorithms identify patterns that explain or fit a given dataset. 
Every ML algorithm goes through _training_, where it identifies underlying patterns in a given dataset to create a “trained” algorithm (a model), and _testing_, where the model applies the identified patterns to unseen data points. 
Typically, a ML algorithm is provided with: 1. a _training dataset_ , 2. an _evaluation dataset_ , 3. a _held-out validation dataset_. 
These input data can be images, text, numbers, or other types of data which are typically encoded as a numerical representation of the input data. 
A training dataset is used by the model to learn underlying patterns from the features present in the data of interest. 
An evaluation dataset is a small and previously unused dataset which is used during the training phase to help the model iteratively update its parameters (i.e., _hyperparameter tuning_ or _model tuning_). 
In many cases, a large training set may be subdivided to form a smaller training dataset and the evaluation dataset, both of which are used to train the model. 
In the testing phase, a completely new or unseen test dataset or held-out validation set is used to test whether the patterns learned by the model hold true in new data (i.e., they are _generalizable_). 
While the evaluation dataset helps us refine a model’s fit to patterns in the training data, the held-out validation set helps us test the generalizability of the model.

If a model is generalizable, it is able to make accurate predictions on new data. 
High generalizability of a model on previously unseen data suggests that the model has identified important patterns in the data that are not unique to the data used for training and tuning. 
Generalizability can be affected if _data leakage_ occurs during training of the model, i.e., if a model is exposed to the same or related data points in both the training set and the held-out test set. 
Ensuring absence of any overlap or relatedness among data points or samples used in the training set and evaluation set is important to avoid data leakage during model training. 
Specifically, in cases of rare genetic diseases where, for example, many samples can contain familial relationships or data from the same patient could be collected by multiple specialists at different clinical facilities, special care should be taken while crafting the training and testing sets to ensure that no data leakage occurs and the trained model has high generalizability.

### Training and testing

The implementation of a ML experiment begins with splitting a single dataset of interest such that a large proportion of the data (e.g., 70-90%) is used for training (generally subdivided into the training dataset and the evaluation dataset), and the remaining data is used for testing or validation (as the held-out validation dataset). 
Ideally, a _held-out validation dataset_ is an entirely new study or cohort, as researchers typically aim to build models that generalize to unseen, newly generated data. 
In rare diseases where multiple datasets may be combined to make a large enough training dataset, special care should be taken to standardize the features and the patterns therein. 
Although ML algorithms generally expect that datasets have uniform features, normalizing training and testing data together may introduce similarities between samples (causing inadvertent data leakage) that hamper the goal of training models that are highly generalizable.

The iterative training phase helps the model learn important patterns in the training dataset and then use the evaluation dataset to test for errors in prediction and update its learning parameters (hyperparameter tuning). 
The method by which the evaluation dataset tests the performance of the trained model and helps update the hyperparameters is called cross-validation. 
There are multiple approaches that can be deployed to maximally utilize the available data when generating training and evaluation datasets e.g., leave-p-out cross-validation, leave-one-out cross-validation, k-fold cross-validation, Monte-Carlo random subsampling cross-validation.[@doi:10.1007/978-1-4614-6849-3]
In the case of k-fold cross-validation, a given dataset is shuffled randomly and split into _k_ parts. 
One of the k parts is reserved as the _evaluation dataset_ and the rest are cumulatively used as the _training dataset_. 
In the next iteration, a different part is used as the evaluation dataset, while the rest are used for training. 
To avoid data leakage, and to promote generalization of models to new studies, researchers can use _study-wise cross-validation_, such that all samples from a study are in the same fold and no individual study is represented in both the training and evaluation datasets. 
Once the model has iterated through all k parts of the training and evaluation datasets, it is ready to be tested on the held-out validation dataset.(Figure {@fig:1}b)

The held-out validation dataset is exposed to the model only once to estimate the accuracy of the model. 
High accuracy of a model during cross-validation but low accuracy on the held-out dataset is a sign that the model has become overfit to the training set and has low generalizability. 
If this is encountered, researchers should revisit the construction of the dataset to make sure they meet the best practices outlined above.