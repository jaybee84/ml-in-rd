### Manage model complexity without sacrificing the value of machine learning

Machine learning is a great tool to capture complex patterns that underlie a dataset.
But for fruitful translation of patterns extracted using machine learning into testable hypotheses, there are a few pre-requisites: the models need to be a) stable i.e. the same predicted features should surface from the data if the model is run multiple times and, b) simple to improve interpretability and avoid misinterpretation due to technical artifacts.
Fulfilling these pre-requisites become even more important in case of rare disease dataset where there is high label-uncertainty (i.e. where the label given to a data point may not be correct due to imperfect understanding of the disease).
In this section we highlight few techniques which can help fulfill the above mentioned pre-requisites.

Techniques like bootstrapping and ensemble learning can increase stability in machine learning predictions.

#### Bootstrapping

Bootstrapping is a powerful statistical technique where resampling the data with replacements can help estimate population values from datasets of limited sample size [@doi:10.1080/01621459.1997.10474007].
Such resampling with replacement is used in various learning methods to find the most informative models (e.g. bootstrap aggregating or _bagging_ used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
A variation where resampling of a rare disease dataset was done _without replacement_, generated confidence intervals for the model predictions as they were exposed to incomplete datasets (mimicking the real life case where most rare disease datasets are incomplete) [@doi:10.3390/genes11020226].

#### Ensemble learning

Stability in predictions can also be achieved by combining various machine learning methods together (_ensemble learning_).
Ensemble learning methods like random forests use bagging of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@doi:10.1186/1472-6947-13-134; @doi:10.1023/A:1010933404324; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6].
But such methods have shown limited success in rare disease datasets where the label-uncertainty can be high due to imperfect understanding of the disease (i.e.silver standard datasets).
This has led to the adoption of cascade learning, where multiple methods leveraging distinct underlying assumptions are used in tandem. 
The methods are augmented with algorithms like AdaBoost (_boosting_) to capture stable patterns existing in the silver standard data [@doi:10.1109/CVPR.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/ICPR.2004.1334680].
A variation of cascade learning implemented to identify rare disease patients from electronic health records from the general population utilized independent steps for feature extraction (using word2vec [@arXiv:1301.3781v3]), preliminary prediction (ensemble of decision trees with penalization for excessive tree-depth), and prediction refinement (using similarity of data points to resolve sample labels) [@pmid:30815073].
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.

Techniques like regularization and binary predictions can help simplify models by making the feature space proportionate with the sample space.

#### Regularization

Regularization can not only protect ML models from _overfitting_ (where the model performs well for the training data but poorly for new test data) [@doi:10.1073/pnas.1900654116], but also help reduce the feature space to help build simpler models using limited datasets.
The three main methods of regularization include Ridge regression, LASSO, and Elastic-net.
While ridge regression can minimize the magnitude of the features, it cannot remove unimportant features.
LASSO regression, on the other hand, works well for selecting few important features since it can minimize the magnitude of some features more than the others[@doi:10.1038/nmeth.4014].
A combination of LASSO and Ridge, Elastic-net regression[@doi:10.1111/j.1467-9868.2005.00503.x] efficiently selects the most useful features, especially in presence of large number of correlated features.

While regularization has not been used extensively in rare disease yet, examples in rare variant discovery and immune cell signature discovery can provide insights into their possible application in rare disease.
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448].
Hybrid applications of LASSO have also been tested in rare variant discovery, including boosting the signal of rare variants by capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-S9-S113], integration with a probabilistic logistic Bayesian approach [@doi:10.4137/CIN.S17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250].
In immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1186/s12859-019-2994-z].
Thus, regularization methods like LASSO or elastic-net have been methods of choice in making models simpler by reducing the feature space; these methods should be explored while working with rare disease datasets.

#### One-class-at-a-time classification

In rare diseases like neurofibromatosis, the presence of more than one phenotype (or class) further decreases the number of data-points per class and introduces additional label-uncertainty due to related phenotypes.
In datasets with multiple classes, the classical ensemble or cascade classifiers approach follows a _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But instances where the need for high prediction accuracy for one class outweighs other classes, modification of the cascade learning method into a _one-class-at-a-time_ approach (where at each stage a binary classifier predicts a specific class against all others) has proved to be beneficial [@pmid:30380082].
In this instance, the final model implemented all models together each identifying one class sequentially and then reporting the union of the predictions of all the different models as the final prediction.
The cascade classifiers using the one-class-at-a-time approach were found to perform better than multi-class ensemble classifiers in most cases.

By employing bootstrapping, ensemble learning, and regularization methods, researchers may be able to better generate stable, simple models that identify reliable biological phenomena in rare diseases.
