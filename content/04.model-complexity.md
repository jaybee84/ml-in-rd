### Manage model complexity while preserving the value of machine learning

Fruitful translation of patterns extracted from a rare disease dataset using machine learning into testable hypotheses requires the applied models to be a) stable i.e. the same predicted features should surface from the data if the model is run multiple times and, b) simple to improve interpretability and avoid misinterpretation due to technical challenges. Fulfilling these prerequisites is challenging in rare disease datasets where there is high label-uncertainty (i.e. where the label given to a data point may not be correct due to imperfect understanding of the disease). 
In this section we highlight a few common ML techniques that can help improve the stability and simplicity of ML models applied to rare disease data.
Techniques like bootstrapping and ensemble learning can increase stability in machine learning predictions. Bootstrapping is a powerful statistical technique where resampling the data with replacements can help estimate population values from datasets of limited sample size [@doi:10.1080/01621459.1997.10474007].
While  resampling with replacement is commonly used to find models that are robust to overfitting ([@https://doi.org/10.1023/A:1010933404324; @doi:10.1198/0003130043277; @doi:10.1016/s0925-2312(01)00650-6; @doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]); resampling without replacement, when applied to rare disease data generated confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets (mimicking real world cases where most rare disease datasets are incomplete) [@doi:10.3390/genes11020226].

Stability in predictions can also be achieved by combining various ML methods together (ensemble learning). 
Ensemble learning methods like random forests use bagging of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10.1016/s0031-3203(02)00169-3]. 
But such methods have shown limited success in rare disease datasets where the label-uncertainty can be high due to imperfect understanding of the disease (i.e. silver standard datasets). 
This has led to the adoption of cascade learning, where multiple methods leveraging distinct underlying assumptions are used in tandem. The methods may be augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in the silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
Cascade learning implemented to identify rare disease patients from electronic health records from the general population utilized independent steps for feature extraction (using natural language processing based word2vec [@https://arxiv.org/abs/1301.3781v343]), preliminary prediction (using an ensemble of decision trees with penalization for excessive tree-depth), and prediction refinement (using similarity of data points to resolve sample labels) [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets further decreases the available data-points per class. 
In such cases, a one-class-at-a-time cascade learning approach (where at each stage a binary classifier predicts a specific class against all others) has been found to produce simpler models that perform better compared to multi-class ensemble classifiers [@doi:10.1093/jamia/ocy109]. 

Simplification of models by making the feature space proportionate with the sample space can also be achieved through regularization. 
Regularization can not only protect ML models from poor generalizability that results from overfitting (where the model performs well for the training data but poorly for new test data) [@doi:10.1073/pnas.1900654116], but also help penalize model complexity and reduce the feature space to build simpler models using limited datasets. 
Three popular  regularized methods include ridge regression, LASSO, and elastic-net. Ridge regression can minimize the magnitude of the features, but cannot remove unimportant features. 
LASSO regression, on the other hand, works well for selecting few important features since it can minimize the magnitude of some features more than the others [@doi:10.1038/nmeth.4014]. 
A combination of LASSO and ridge, elastic-net regression [@doi:10.1111/j.1467-9868.2005.00503.x] selects the most useful features, especially in presence of a large number of correlated features.
Studies leveraging regularization in rare variant discovery and immune cell signature discovery can provide important insights into its possible application in rare disease. 
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO have also been tested in rare variant discovery, including boosting the signal of rare variants by capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113], integration with a probabilistic logistic Bayesian approach [@doi:10.4137/cin.s17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250]. 
In immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198 ; @doi:10.1186/s12859-019-2994-z]. 
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space; these methods should be explored while working with rare disease datasets. 

Thus by employing bootstrapping, ensemble learning, and regularization methods, researchers may be able to better generate stable, simple models that identify reliable biological phenomena underlying rare diseases .
