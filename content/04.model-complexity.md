### Techniques and procedures must be implemented to manage model complexity without sacrificing the value of machine learning
Inherent challenges posed by low sample numbers in rare diseases are further aggravated by disease heterogeneity, poorly defined disease phenotypes, and often a lack of control (i.e. normal) data. 
Machine learning approaches must be carefully designed to address these challenges. 
We discuss how to implement methodological solutions like bootstrapping sample data, regularization methods for deep learning, and hyper-ensemble techniques to minimize misinterpretation of the data. 

#### Bootstrapping

Bootstrap or resampling computation is a powerful statistical technique that can be used for estimating population values from datasets of limited sample size by resampling the data to generate a distribution of the population statistic and minimize estimation error [@doi:10.1080/01621459.1997.10474007].
Bootstrap based techniques are used in conjunction with various learning methods to find the most informative models given a specific dataset (e.g. bootstrap aggregating or bagging used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
In addition to model selection, rare disease datasets also necessitate the use of bootstrap to form an informative dataset [@doi:10.3390/genes11020226].
In this study, bootstrapping the training sample without replacement simulated separate incomplete datasets that helped expose the learning models (in this case random forests) to the incompleteness of the data.
Thus additional bootstrapping of the training data helped create confidence intervals for the predictions and the important predictors originating from the ensemble models run on the incomplete training data.

#### Regularization

Incompleteness of data in rare disease has led to efforts for collating data points dispersed over sites or time points to produce a statistically powerful dataset. 
Given a dataset with strong preexisting study-specific technical differences between groups of samples, machine learning methods may model dataset-specific features instead of true biology, leading to high prediction accuracy for training data but poor performance in new test data (an "overfit" model) [@doi:10.1073/pnas.1900654116].
Minimization of overfitting can be accomplished by cross-validation (to reduce variance in predictions) and regularization (to reduce low bias in models) methodologies. 
Regularization makes models less reliant on training data by adding a small penalty (determined by cross-validation), and can not only minimize overfitting but can additionally help in predicting outcomes using a limited number of samples. 

Regularization can be of three main types, each with their particular strengths and weaknesses. 
Ridge regression aims to minimize the magnitude of the features, but cannot completely remove unimportant features and thus may not be ideal for reducing the feature space. 
LASSO or Least Absolute Shrinkage and Selection Operator regression on the other hand works well for selecting few important features since it can minimize the magnitude of some features more than the others[@doi:10.1038/nmeth.4014]. 
Elastic-Net regression is a combination of LASSO and ridge regression[@doi:10.1111/j.1467-9868.2005.00503.x], and helps to select most useful features, especially in presence of large number of correlated features. 

While regression based regularization has not been used extensively in rare disease, examples of combinations of above strategies implemented in rare variant discovery and immune cell signature discovery can provide an insight into their possible use in rare disease.
In rare variant discovery, adaptive ridge regression was utilized to combine rare variants into a single score to increase the signal of rare variants[@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/ low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid approaches of LASSO including boosting the signal of rare variants by capturing linear combinations of variants by gene or chromosome location in 5% of subjects [@doi:10.1002/gepi.21746; @doi:10.1186/1753-6561-5-S9-S100; @doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-S9-S113; @doi:10.1186/1753-6561-5-S9-S100], integration with the probabilistic logistic bayesian approach [@doi:10.4137/CIN.S17290], and combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694] have also been tested in rare variant discovery.
Another interesting approach which incorporated prior knowledge into the regularization (called sparse-group LASSO) worked well to select the driver genes in a pathway of interest where only few genes in a pathway were true predictors of a phenotype [@doi:10.1080/10618600.2012.681250]. 

In immune cell signature discovery, Elastic-net regression (a combination of LASSO and ridge regression) has been used to reduce the feature space and was found to outperform the other regression approaches [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of elastic-net, where a two-step regularized logistic regression was used to pre-select an optimal number of genes before implementing elastic-net regularization for gene selection, identified immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z].  
Thus robust regularizations methods like LASSO or elastic-net have been methods of choice where the the profiled feature space have outnumbered the number of samples or patients by a magnitude and should be explored while working with rare disease datasets.

#### Hyperensemble
