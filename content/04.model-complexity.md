### Manage model complexity while preserving the value of machine learning

Translating machine learning findings into testable hypotheses requires the ML models to be both stable – the same predicted features should surface from the data if the model is run multiple times – and simple, as simple models guard against misinterpretation. 
Meeting these requirements is challenging in rare disease datasets where label-noise is abundant. 
In this section we highlight a few common ML techniques that can help improve the stability and simplicity of ML models applied to rare disease data.

Techniques like resampling and combining various ML methods together (ensemble learning) can help achieve stability in predictions (Figure[@fig:2]A-B). 
Resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
Alternatively, resampling with replacement (bootstrapping) helps estimate population values from datasets of limited size, and is also commonly used to find robust models when multiple models are combined into an ensemble ([@doi:10.1080/01621459.1997.10474007; @https://doi.org/10.1023/A:1010933404324; @doi:10.1198/0003130043277; @doi:10/c8xpqz; @doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
Ensemble learning methods like random forests use _bagging_ (bootstrap aggregation) of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6]. 
However, standard ensemble learning has shown limited success in rare disease datasets with substantial label-noise. [TODO: citation?] 
This has led to the adoption of cascade learning, a variant of ensemble learning, where multiple methods leveraging distinct underlying assumptions are used in tandem; and augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
For example, a cascade learning approach for identifying rare disease patients from electronic health record data utilized independent steps for feature extraction (word2vec [@arxiv:1301.3781v343]), preliminary prediction with ensembled decision trees, and prediction refinement using data similarity metrics [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets also decreases the available data points per class. 
In such cases, a one-class-at-a-time cascade learning approach (where at each stage a binary classifier predicts a specific class against all others) has been found to produce simpler models that perform better compared to multi-class ensemble classifiers [@doi:10.1093/jamia/ocy109]. (Figure[@fig:2]D)

Regularization simplifies models by making the feature space proportionate with the sample space. (Figure[@fig:2]C)
Regularization can not only protect ML models from poor generalizability caused by overfitting (where the model performs well on held-out training data but poorly on new test data) [@doi:10.1073/pnas.1900654116], but also reduces  model complexity and the feature space to build simpler models. 
Three popular regularized methods, ridge regression, LASSO regression, and elastic-net regression, differ predominantly in how they modify features of the input data. 
Ridge regression can minimize the magnitude of the features, but cannot entirely remove features. 
LASSO regression, on the other hand, works well for selecting a few important features since it can minimize the magnitude of some features more than the others [@doi:10.1038/nmeth.4014]. 
A combination of LASSO and ridge, elastic-net regression [@doi:10.1111/j.1467-9868.2005.00503.x] selects the most useful features, especially in presence of a large number of correlated features.

Rare variant discovery and immune cell signature discovery studies, like rare diseases, face challenges of the sparsity of observations, and may be useful models for examining the utility of regularization in scenarios with limited signal.
For example, ridge regression has been used to combine rare variants into a single score to increase the signal of these variants [@doi:10.1371/journal.pone.0044173], while LASSO has been implemented along with group penalties to identify gene variants [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO in rare variant discovery studies like capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113], integrating with a probabilistic logistic Bayesian approach [@doi:10.4137/cin.s17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250] have also proven beneficial.
On the other hand, in the context of rare immune cell signature discovery, elastic-net regression was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1186/s12859-019-2994-z]. 
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space in data with rare observations; use of these regularization approaches should be considered while working with rare disease datasets. 

By employing bootstrapping, ensembling, and regularization, researchers may be able to generate more stable and simpler models to characterize the biological phenomena underlying rare diseases.

![Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) A schematic showing the concept of bootstrap, B) A schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) A schematic showing the concept of regularization to selectively learn relevant features, D) A schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](https://github.com/jaybee84/ml-in-rd/blob/draft-branch/content/images/figures/pdfs/statistical-techniques.pdf){#fig:2}
