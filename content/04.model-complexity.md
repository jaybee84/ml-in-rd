### Techniques and procedures must be implemented to manage model complexity without sacrificing the value of machine learning
Inherent challenges posed by low sample numbers in rare diseases are further aggravated by disease heterogeneity, poorly defined disease phenotypes, and often a lack of control (i.e. normal) data. 
Machine learning approaches must be carefully designed to address these challenges. 
We discuss how to implement methodological solutions like bootstrapping sample data, regularization methods for deep learning, and hyper-ensemble techniques to minimize misinterpretation of the data. 

#### Bootstrapping

Bootstrap or resampling computation is a powerful statistical technique that can be used for estimating population values from datasets of limited sample size by resampling the data to generate a distribution of the population statistic and minimize estimation error [@doi:10.1080/01621459.1997.10474007].
Bootstrap based techniques are used in conjunction with various learning methods to find the most informative models given a specific dataset (e.g. bootstrap aggregating or bagging used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
In addition to model selection, rare disease datasets also necessitate the use of bootstrap to form an informative dataset [@doi:10.3390/genes11020226].
In this study, bootstrapping the training sample without replacement simulated separate incomplete datasets that helped expose the learning models (in this case random forests) to the incompleteness of the data.
Thus additional bootstrapping of the training data helped create confidence intervals for the predictions and the important predictors originating from the ensemble models run on the incomplete training data.

#### Regularization

Incompleteness of data in rare disease has led to efforts for collating data points dispersed over sites or time points to produce a statistically powerful dataset. 
Given a limited dataset with strong pre-existing site-specific technical differences between groups of samples, machine learning models may optimize learning technical differences instead of real patterns leading to high prediction accuracy for training data (also termed low bias in the model or "overfit") [@doi:10.1073/pnas.1900654116].
Minimization of overfitting can be accomplished by cross-validation (to reduce variance in predictions) and regularization (to reduce low bias in models) methodologies. 
Regularization makes models less reliant on training data by adding a small penalty (determined by cross-validation), and can not only minimize overfitting but can additionally help in predicting outcomes using a limited number of samples. 

Regularization can be of three main types, each with their particular strengths and weaknesses. 
(1) Ridge regression aims to minimize the magnitude of the features, but in models that try to select the most important features for accurate prediction of sample labels, ridge regression shrinks all features equally, but cannot completely remove unimportant features. 
Thus in presence of many correlated parameters (e.g. gene expression networks), ridge regression may not be ideal in reducing the feature space. 
(2) LASSO or least absolute shrinkage and selection operator regression on the other hand works well for selecting few important features since its effect can minimize the magnitude of some features more than the others. 
Thus it helps in selecting most important features while the magnitude of irrelevant features are shrunk to 0 and eventually removed.
This selection attribute of LASSO (in a sample set of size "n", LASSO can select "n" features for the model) may be an advantage in reducing model complexity, but a disadvantage in cases where identification of all possible collinear features is important (e.g. all biomarkers correlating to a particular disease phenotype) [@doi:10.1038/nmeth.4014]. 
(3) Elastic-Net regression is a combination of LASSO and ridge regression[@doi:10.1111/j.1467-9868.2005.00503.x]. 
Both of the methodologies when applied together helps to select most useful features, specially where there are a lot of correlated features. 
In this setup, LASSO leads to selection of one of the correlated features and reduces the others to 0 (grouping of features), and the magnitude of the selected features are then minimized through ridge regression. 

A combination of above strategies implemented in rare variant discovery and immune cell signature discovery can serve as good case studies. 
Ridge regression is seldom utilized for feature selection, but adaptive ridge regression has been utilized to help combine rare variants into a single score analogous to feature engineering for increasing the signal of rare variants[@doi:10.1371/journal.pone.0044173]. 
LASSO has been preferred in rare variant discovery and has been implemented along with group penalties to identify rare variants/ low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Variations of LASSO include boosting the signal of rare variants by capturing linear combinations of absence or presence of rare variants (encoded in a 0-1 dummy variable), aggregating their occurrence by gene or chromosome location, and selecting combinations that are non-zero for at least 5% of subjects [@doi:10.1002/gepi.21746; @doi:10.1186/1753-6561-5-S9-S100; @doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-S9-S113; @doi:10.1186/1753-6561-5-S9-S100]. 
Another variation of LASSO included its integration with the probabilistic logistic bayesian approach to identify a protective rare variant in lung cancer[@doi:10.4137/CIN.S17290]. 
Xu et al. on the other hand combined the feature selection methods with a generalized pooling strategy, and evaluated the performance of these hybrid approaches for detection of rare genetic variants[@doi:10.1371/journal.pone.0041694]. 
Another interesting approach is the sparse-group LASSO approach which incorporates prior knowledge into the regularization[@doi:10.1080/10618600.2012.681250]. 
This approach works well for a scenario where only few genes in a pathway are true predictors of a phenotype, where it helps select the driving genes in a pathway of interest. 

Alternatively, Elastic-net regression (a combination of LASSO and ridge regression) has also been used to reduce the feature space in various types of cancer datasets and was found to outperform the other regression approaches [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of elastic-net, where a two-step regularized logistic regression was used to pre-select an optimal number of genes before implementing elastic-net regularization for gene selection, identified immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z].  
Thus robust regularizations methods like LASSO or elastic-net have been methods of choice where the the profiled feature space have outnumbered the number of samples or patients by a magnitude.

Still to add: 
techniques in deep learning e.g.
Deep and shallow architecture:
https://ieeexplore.ieee.org/document/7863293

#### Hyperensemble
