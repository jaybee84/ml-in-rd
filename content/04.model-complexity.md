### Techniques and procedures must be implemented to manage model complexity without sacrificing the value of machine learning

In additiopn to improving the input data, methodological considerations can also help address the aforementioned challenges in applying ML models to rare diseases. 
Below, we describe some examples, including bootstrapping sample data, regularization methods for deep learning, and hyper-ensemble techniques to minimize misinterpretation of the data. 

#### Bootstrapping

Bootstrapping is a powerful statistical technique that can be used to estimate population values from datasets of limited sample size by resampling the data [@doi:10.1080/01621459.1997.10474007].
Bootstrap based techniques are used in conjunction with various learning methods to find the most informative models given a specific dataset (e.g. bootstrap aggregating or bagging used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
Bootstrapping can be used to enhance the information content of rare disease datasets and generate confidence intervals for the model predictions as demonstrated by Banerjee et al [@doi:10.3390/genes11020226].
In this study, bootstrapping the training sample without replacement simulated generation of separate datasets that helped expose the learning models (random forests) to the incomplete nature of the data.
Such bootstrapping of the training data in addition to that included in the model (bagging) helped generate a distribution (and confidence intervals) of the importance scores of the predictive features selected by the model.

#### Regularization

Another common strategy for handling the paucity of data in rare disease is to aggregate data from multiple studies or time points to produce a more comprehensive dataset. 
Given a dataset with strong preexisting study-specific technical differences between groups of samples, ML methods may model dataset-specific features instead of true biology, leading to high prediction accuracy for training data but poor performance in new test data (an "overfit" model) [@doi:10.1073/pnas.1900654116].
Minimization of overfitting can be accomplished by cross-validation (to reduce variance in predictions) and regularization (to reduce low bias in models). 
Regularization makes models less reliant on training data by adding a small penalty (determined by cross-validation), and can minimize overfitting while improving performance. 

ML models can be regularized using 3 main methods, each with strengths and weaknesses. 
Ridge regression aims to minimize the magnitude of the features, but cannot remove unimportant features and thus may not be ideal for reducing the feature space. 
Another method, LASSO regression, works well for selecting few important features since it can minimize the magnitude of some features more than the others[@doi:10.1038/nmeth.4014]. 
Elastic-net regression is a combination of LASSO and ridge regression [@doi:10.1111/j.1467-9868.2005.00503.x], and helps to select the most useful features, especially in presence of large number of correlated features. 

While regression based regularization has not been used extensively in rare disease, examples in rare variant discovery and immune cell signature discovery provide insights into their possible application in rare disease.
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO have also been tested in rare variant discovery, including boosting the signal of rare variants by capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-S9-S113], integration with a probabilistic logistic Bayesian approach [@doi:10.4137/CIN.S17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250]. 

In immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of elastic-net, where a two-step regularized logistic regression was used to pre-select an optimal number of genes before implementing elastic-net regularization for gene selection, identified immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z].  
Thus, regularization methods like LASSO or elastic-net have been methods of choice where the profiled feature space is substantially larger than the number of samples; these methods should be explored while working with rare disease datasets.

#### Hyperensemble
