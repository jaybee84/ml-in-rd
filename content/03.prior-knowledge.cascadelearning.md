## Ensemble Learning {.page_break_before}

Implementing machine learning on data with low sample size and high label uncertainty can lead to unstable predictions. 
In such cases where single predictors fail, various machine learning methods together or _ensemble learning_ may help increase accuracy of prediction.
_Ensemble learning_ methods like random forests use bootstrap aggregation (or _bagging_) of independent decision trees that use similar parameters but different paths for the selection of features to form a consensus about the important predictive features[@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324, @doi:10.1214/aos/1031689014, @doi:10.1177/2045894019890549, @doi:10/btzfh6].
However, successful application of consensus based ensemble learning requires "gold standard" data where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "label-noise") leading to a _silver standard_ dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
In such cases, ensemble learning with multiple methods leveraging distinct underlying assumptions are used in tandem to capture stable patterns existing in the _silver standard_ data and reduce uncertainty.
Such _cascade learning_ classifiers have been widely used in image recognition where initially a small subset of image features are used to classify images (e.g. features identifying a face like eyes, nose, mouth). 
The initial classification is then augmented by more complex features and algorithms like AdaBoost ( _boosting_ ) that weight the various features implemented to detect the content of the image (e.g. features identifying a human face like relative distance between eyes etc.)[@doi:10.1109/CVPR.2001.990537, @doi:10.1007/978-3-540-75175-5_16, @doi:10.1109/ICPR.2004.1334680].

In rare diseases, a variant of _cascade learning_ that showed robustness in view of uncertainty in the data was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a different independent learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to resolve sample labels and reiterating step (2).
In this implementation the algorithm was able to identify rare disease patients due to the robustness conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
The classification step capitalized upon the information learned by the label prediction step preceding it and the prediction refinement step following it, and was able to perform better over other ensemble methods when implemented on silver standard data.

Most cascade classifiers follow _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But scenarios where the need for high prediction accuracy for one class outweighs other classes (e.g. malignant tumor-types, or severe psychiatric cases) require further modification of the cascade learning efforts.
An example of this was seen implemented for triaging psychiatric patients where the identification of one class of psychiatric patients ("severe") far outweighed the need for optimized overall classification accuracy[@pmid:30380082].
Due to the requirements of the problem, they developed a _one-class-at-a-time_ approach for cascade learning, where at each stage a binary classifier is used to predict a specific class against all others.
The final model implemented all models together each identifying one class sequentially and the final prediction was the union of the predictions at all the different models.
The cascade classifiers using the _one-class-at-a-time_ approach were found to perform better than multi-class ensemble classifiers in most cases.

Thus ensemble learning can be helpful in producing stable predictions from data that is limited in quality or quantity, where single algorithms would otherwise produce unstable predictions.
However, the choice of using _bagging_, _boosting_, independent algorithmic steps, or _one-class-at-a-time_ approach would strictly depend on the nature of the prediction problem.
In most cases involving rare disease data, it seems that _bagging_ has had limited success, which has necessitated various modifications of the approaches as discussed above.
