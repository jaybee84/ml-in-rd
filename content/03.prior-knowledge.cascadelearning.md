## Cascade Learning {.page_break_before}

Sometimes, due to paucity of data and its unstable predictive capabilities, one may feel the need to consult various machine learning methods and the predictions there-of to make an educated decision about the identified predictive patterns. 
This assembly of multiple machine learning methods is called ensemble learning. In an ensemble learning workflow, one may use various parameters to maximize the learned information. 
Sometimes a consensus approach is ideal to select predictive features from a group of independently running models, other times one may coordinate various learning methods in such a way that one model uses information from another model in augmenting its predictive capability. 
Random forest algorithms are a great example of ensemble learning where a consensus of the features selected through many different and independent decision trees are considered and ranked to make a final ranked list of important features that are predictive of a certain outcome (e.g. disease) [@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324].
However, successful application of consensus based ensemble learning algorithms requires what is known as "gold standard" data. These generally refer to well labeled cases where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "label-noise") leading to a "silver standard" dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
This kind of data begets modifications in ensemble learning methods where one method leverages findings from another method to capture stable patterns existing in the "silver standard" data and reduce uncertainty.
Such methods fall into the category of cascade learning.
An example of cascade learning that showed robustness in view of uncertainty in the data was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a different independent learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to de-noise sample labels and reiterating step (2).
In this implementation the algorithm was able to identify rare data due to the robustness conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
The classification step capitalized upon the information learned by the label prediction step preceding it, and helped the prediction refinement step following it through its enhanced classification accuracy.
