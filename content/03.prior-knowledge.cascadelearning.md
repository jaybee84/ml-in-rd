## Cascade Learning {.page_break_before}

Sometimes, due to paucity of data and its unstable predictive capabilities, one may feel the need to consult various machine learning methods and the predictions there-of to make an educated decision about the identified predictive patterns. 
This assembly of multiple machine learning methods is called ensemble learning. In an ensemble learning workflow, one may use various parameters to maximize the learned information. 
Sometimes a consensus approach is ideal to select predictive features from a group of independently running models, other times one may coordinate various learning methods in such a way that one model uses information from another model in augmenting its predictive capability. 
The latter method where one method leverages findings from another method is termed cascade learning.
Random forest algorithms are a great example of ensemble learning where a consensus of the features selected through many different decision trees is considered and ranked to make a final ranked list of important features that are predictive of a certain outcome (e.g. disease) [@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324].
However, successful application of ensemble learning algorithms prefer what is known as "gold standard" data. These generally refer to well labeled cases where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "noise") leading to a "silver standard" dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
An example of an ensemble method that developed robustness in view of such noisy data was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a specific learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to de-noise sample labels and reiterating step (2).
In this case the robustness of the algorithm to noisy and rare data was conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
