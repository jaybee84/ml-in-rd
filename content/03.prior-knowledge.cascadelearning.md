## Cascade Learning {.page_break_before}

Sometimes, due to paucity of data and its unstable predictive capabilities, one may feel the need to consult various machine learning methods and the predictions there-of to make an educated decision about the identified predictive patterns, collectively called _ensemble learning_. 
In an ensemble learning workflow, one may use various parameters to maximize the learned information. 
A consensus approach is often ideal to select predictive features from a group of independently running models, other times one may coordinate various learning methods in such a way that one model uses information from another model in augmenting its predictive capability. 
Random forest algorithms are a great example of ensemble learning where a consensus of the features selected through many different and independent decision trees are considered and ranked to make a final ranked list of important features that are predictive of a certain outcome (e.g. disease) [@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324].
For the ensemble of decision trees, e.g. random forest, bootstrap aggregation (or _bagging_) helps in forming the consensus[@doi:10.1214/aos/1031689014, @doi:10.1177/2045894019890549, @doi:10.1016/S0031-3203(02)00169-3].
However, successful application of consensus based ensemble learning algorithms requires what is known as "gold standard" data. 
These generally refer to well labeled cases where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "label-noise") leading to a _silver standard_ dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
This kind of data begets modifications in ensemble learning methods where one method leverages findings from another method to capture stable patterns existing in the _silver standard_ data and reduce uncertainty.
Such methods fall into the category of cascade learning.
Among classifiers, cascade learning has been widely used in image recognition where initially a small subset of image features are used to classify images (e.g. features identifying a face like eyes, nose, mouth). 
The initial classification is then augmented by more complex features and algorithms like Adaboost ( _boosting_ ) that weight the various features are implemented to detect the content of the image (e.g. features identifying a human face like relative distance between eyes etc.)[@doi:10.1109/CVPR.2001.990537, @doi:10.1007/978-3-540-75175-5_16, @doi:10.1109/ICPR.2004.1334680].

In rare diseases, a variant of cascade learning that showed robustness in view of uncertainty in the data was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a different independent learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to resolve sample labels and reiterating step (2).
In this implementation the algorithm was able to identify rare data due to the robustness conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
The classification step capitalized upon the information learned by the label prediction step preceding it and the prediction refinement step following it, and was able to perform better over other ensemble methods when implemented on silver standard data.

Most cascade classifiers follow _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But scenarios where the need for high prediction accuracy for one class outweighs other classes (e.g. malignant tumor-types, or severe psychiatric cases) require further modification of the cascade learning efforts.
An example of this was seen implemented for triaging psychiatric patients where the identification of one class of psychiatric patients ("severe") far outweighed the need for optimized overall classification accuracy[@pmid:30380082].
Due to the requirements of the problem, they developed a _one-class-at-a-time_ approach for cascade learning, where at each stage a binary classifier is used to predict a specific class against all others.
The final model implemented all models together each identifying one class sequentially and the final prediction was the union of the predictions at all the different models.
The cascade classifiers using the _one-class-at-a-time_ approach were found to perform better than multi-class ensemble classifiers in most cases.

Thus ensemble learning can be helpful in producing stable predictions from data that is limited in quality or quantity, where single algorithms would otherwise produce unstable predictions.
However, the choice of using _bagging_, _boosting_, independent algorithmic steps, or _one-class-at-a-time_ approach would strictly depend on the nature of the prediction problem.
In most cases involving rare disease data, it seems that _bagging_ has had limited success, which has necessitated various modifications of the approaches as discussed above.
