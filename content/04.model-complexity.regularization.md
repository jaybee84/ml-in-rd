## Regularization {.page_break_before}

Machine learning algorithms are optimized to find patterns among data points and prioritizes the strongest patterns that exist in a dataset.
Given a limited dataset with strong pre-existing technical differences between groups of samples, this optimization may lead to the model learning technical differences thus lowering its predictive accuracy [@doi:10.1073/pnas.1900654116].
For example, in a set of 1000 samples where 700 samples are from one healthcare site and 300 from another, it is likely that there will remain site-specific differences between them even after normalization of the samples. 
If the site-specific differences are more pronounced than the underlying patterns differentiating the samples, any machine learning model trained with these data will preferentially learn the site-specific differences to classify the samples, and rank them higher than the underlying patterns leading to a model showing high prediction accuracy of training data (termed low bias in model). 
When new test data points are introduced to the model, possibly coming from a third site, the model is unable to locate the earlier differences in the new data points and fails to classify them accurately causing a significant drop in accuracy of the model (termed high variance in model prediction).
Such a model is termed "overfit" to its training data.
Overfitting can lead to misinterpretation of the site-specific differences as true patterns in the limited data points and thus needs to be minimized. 
Minimization of overfitting can be accomplished by cross-validation and regularization methodologies. 
While cross-validation aims to reduce the variance in prediction, regularization adds a small amount of bias to the initial model to minimize its dependence and sensitivity to training data. 
Regularization makes models less reliant on training data by adding a penalty (determined by cross-validation), and then minimizes the error between the model's prediction and ground truth of the test data.
Regularization can not only minimize overfitting but can additionally help in predicting outcomes using a limited number of samples. 

Regularization can be of three main types, each with their particular strengths and weaknesses. 
(1) Ridge regression aims to minimize the magnitude of the features, but in models that try to select the most important features for accurate prediction of sample labels, ridge regression shrinks all features equally, but cannot completely remove unimportant features. 
Thus in presence of many correlated parameters (e.g. gene expression networks), ridge regression may not be ideal in reducing the feature space. 
(2) LASSO or least absolute shrinkage and selection operator regression on the other hand works well for selecting few important features since its effect can minimize the magnitude of some features more than the others. 
Thus it helps in selecting most important features while the magnitude of irrelevant features are shrunk to 0 and eventually removed.
This selection attribute of LASSO (in a sample set of size "n", LASSO can select "n" features for the model) may be an advantage in reducing model complexity, but a disadvantage in cases where identification of all possible collinear features is important (e.g. all biomarkers correlating to a particular disease phenotype) [@doi:10.1038/nmeth.4014]. 
(3) Elastic-Net regression is a combination of LASSO and ridge regression[@doi:10.1111/j.1467-9868.2005.00503.x]. 
Both of the methodologies when applied together helps to select most useful features, specially where there are a lot of correlated features. 
In this setup, LASSO leads to selection of one of the correlated features and reduces the others to 0 (grouping of features), and the magnitude of the selected features are then minimized through ridge regression. 

Any supervised learning implementation in rare disease would require robustness towards feature selection from a small number of samples, i.e. the features selected by a model as important should be stable in view of new data points added to a dataset, even though their relative importance may change due to additional evidence.
This robustness is mostly acquired through the combination of various regression strategies. 
Since machine learning applications in rare disease are infrequent, combination strategies used for rare variant discovery and immune cell signature discovery can serve as good case studies to examine. 
Many deleterious genomic variants can be extremely rare due to the constant selection pressure working against them. 
Since the frequency of a rare variant is so low (less than 1%) applying routine statistical procedures that were extensively developed for common variant association, to analyze a low minor allele frequency (MAF) seem inappropriate [@doi:10.1038/nrg2867]. 
For its feature selection attribute, LASSO has been widely applied in microarray and GWAS data for common variants. 
But since LASSO by itself is too stringent for rare variants, it has been employed along with group penalties to help identify rare variants/ low frequency predictors [@doi:10.1093/bioinformatics/btq448]. 
Variations of LASSO have also been implemented to aggregate or group the occurrence of rare variants together by gene or chromosome location [@doi:10.1002/gepi.21746; @doi:10.1186/1753-6561-5-S9-S100; @doi:10.1016/j.ajhg.2008.06.024]. 
In this strategy, a 0â€“1 dummy variable was created for each SNP based on the presence or absence of the rare variant. 
Then linear combinations of the selected dummy variables were considered by using the LASSO procedure. 
Even though most of the dummy variables were 0, their linear combination was more likely to be nonzero thus leading to increased signal to noise ratio for the rare variants. 
Only those linear combinations that were non-zero in at least 5% of the subjects were then included to ensure that the new markers were not rare [@doi:10.1186/1753-6561-5-S9-S113; @doi:10.1186/1753-6561-5-S9-S100]. 
While ridge regression is not usually utilized for feature selection, adaptive ridge regression has been utilized to help combine rare variants into a single score analogous to feature engineering for increasing the signal of rare variants[@doi:10.1371/journal.pone.0044173]. 
Another variation of LASSO included its integration with the probabilistic logistic bayesian approach to identify a protective rare variant in lung cancer[@doi:10.4137/CIN.S17290]. 
Xu et al. on the other hand combined the feature selection methods with a generalized pooling strategy, and evaluated the performance of these hybrid approaches for detection of rare genetic variants[@doi:10.1371/journal.pone.0041694]. 
Another interesting approach is the sparse-group LASSO approach which incorporates prior knowledge into the regularization[@doi:10.1080/10618600.2012.681250]. 
This approach works well for a scenario where only few genes in a pathway are true predictors of a phenotype, where it helps select the driving genes in a pathway of interest. 

Alternatively, Elastic-net regression (a combination of LASSO and ridge regression) has also been used to reduce the feature space in various types of cancer datasets [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198]. 
In cases where the number of features were far greater than the number of samples, elastic-net has usually been found to outperform the other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of the elastic-net regression was used for identifying immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z]. 
This two-step regularized logistic regression technique included a pre-filtering phase to select the optimal number of genes and then implemented elastic-net regularization for gene selection. 
The second step generated gene signatures for individual cell types using selected genes from first step and then implemented a binary regularized logistic regression for each cell type against all other samples.



Still to add: 
techniques in deep learning e.g.
Deep and shallow architecture:
https://ieeexplore.ieee.org/document/7863293
