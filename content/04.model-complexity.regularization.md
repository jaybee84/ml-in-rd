## Regularization {.page_break_before}

Machine learning algorithms are optimized to find patterns among data points. 
Given a large amount of data, this optimization works in favor of the algorithm trying to learn complex patterns in the data. 
However, when data is sparse, the same optimization leads to the model learning technical variances rather than actual patterns in the data. 
This results in low bias but high variance in the prediction results, i.e., overfitted models fit the training data very closely (low bias), but fail to predict new testing data with high accuracy (high variance) leading to low prediction accuracy of the model in view of new data. 
Such overfitting can lead to misinterpretation of technical variances as true patterns in the sparse data and thus needs to be minimized. 
Minimization of overfitting can be accomplished by crossvalidation and regularization methodologies. 
While crossvalidation aims to reduce the variance in prediction, regularization adds a small amount of bias to the initial model to minimize its dependance and sensitivity to training data. 
Regularization makes models less sensitive to variance in sample data by adding a penalty (lambda) to the sum of squared residuals (or sum of maximum likelihoods). 
While the value of lambda is determined by crossvalidation, the minimization of the sum of squared residuals (or error) leads to a model that is less sensitive to changes in sample data.

To learn from 'n' number of features, machine learning models ideally need at least 'n+1' samples. 
For eg. to predict any disease outcome from 20,000 genes we need at least 20,001 samples. 
However, in reality even the best sample set probably contains 500 samples, and in case of rare diseases that may be closer to 50 than 500. 
Regularization can not only minimize overfitting but can additionally help in predicting outcomes using fewer number of samples. 
Regularization can be of three main types, each with their particular strengths and weaknesses. 
(1) Ridge regression aims to minimize the magnitude of the features, but in models that try to select the most important features for accurate prediction of sample labels, ridge regression shrinks all features equally, but can never shrink features all the way to 0. 
Thus in presence of many correlated parameters (eg. gene expression networks), ridge regression may not be ideal in reducing the feature space. 
(2) LASSO regression on the other hand works well for selecting few important features since its effect can shrink some features more than the others. 
Thus it helps in selecting most important features while irrelevant features are lost and shrunk to 0. This selection attribute of LASSO (in a sample set of size "n", LASSO can only select "n" features for the model) may be an advantage in reducing model complexity, but a disadvantage in cases where identification of all possible collinear features is important (eg. all biomarkers correlating to a particular disease phenotype) [@doi:10.1038/nmeth.4014]. 
(3) Elastic net regression is a combination of LASSO and ridge regression[@doi:10.1111/j.1467-9868.2005.00503.x]. 
Both of the methodologies when applied together helps to select most useful features, specially where there are a lot of correlated features. 
In this setup, LASSO leads to selection of one of the correlated features and reduces the others to 0 (grouping of features). 
The ridge regression aspect of elastic-net then minimizes the magnitude of the features that were selected without shrinking any of them to 0 (minimization). 

Any supervised learning implementation in rare disease would require robustness towards feature selection from a small number of samples. 
This robustness is mostly acquired through the combination of various regression strategies. 
Since machine learning applications in rare disease are infrequent, combination strategies used for rare variant discovery and immune cell signature discovery can serve as good case studies to examine. 
Many deleterious genomic variants can be extremely rare due to the constant selection pressure working against them. 
Since the frequency of a rare variant is so low (less than 1%) applying routine statistical procedures that were extensively developed for common variant association, to analyze a low minor allele frequency (MAF) seem inappropriate [@doi:10.1038/nrg2867]. 
For its feature selection attribute, LASSO or least absolute shrinkage and selection operator has been widely applied in microarray and GWAS data for common variants. 
But since LASSO by itself is too stringent for rare variants, it has been employed along with group penalties to help identify rare variants/ low frequency predictors [@doi:10.1093/bioinformatics/btq448]. 
Variations of LASSO have also been implemented to aggregate or group the occurrance of rare variants together by gene or chromosome location [@doi:10.1002/gepi.21746; @doi:10.1186/1753-6561-5-S9-S100; @doi:10.1016/j.ajhg.2008.06.024]. 
In this strategy, a 0â€“1 dummy variable was created for each SNP based on the presence or absence of the rare variant. 
Then linear combinations of the selected dummy variables were considered by using the LASSO procedure. 
Even though most of the dummy variables were 0, their linear combination was more likely to be nonzero thus leading to increased signal to noise ratio for the rare variants. 
Only those linear combinations that were nonzero in at least 5% of the subjects were then included to ensure that the new markers were not rare [@doi:10.1186/1753-6561-5-S9-S113; @doi:10.1186/1753-6561-5-S9-S100]. 
While ridge regression is not usually utilized for feature selection, adaptive ridge regression has been utilized to help combine rare variants into a single score analogous to feature engineering for increasing the signal of rare variants[@doi:10.1371/journal.pone.0044173]. 
Another variation of LASSO included its integration with the probabilistic logistic bayesian approach to identify a protective rare variant in lung cancer[@doi:10.4137/CIN.S17290]. 
Xu et al. on the other hand combined the feature selection methods with a generalized pooling strategy, and evaluated the performance of these hybrid approaches for detection of rare genetic variants[@doi:10.1371/journal.pone.0041694]. 
Another interesting approach is the sparse-group LASSO approach which incorporates prior knowledge into the regularization[@doi:10.1080/10618600.2012.681250]. 
This approach works well for a scenario where only few genes in a pathway are true predictors of a phenotype, where it helps select the driving genes in a pathway of interest. 
Alternatively, a combination of LASSO and ridge regression, Elastic-net regression has also been used to reduce the feature space in various types of cancer datasets [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198]. 
In cases where the number of features were far greater than the number of samples, it has usually been found to outperform the other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of the elastic net regression was used for identifying immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z]. 
This two-step regularized logistic regression technique included a pre-filtering phase to select the optimal number of genes and then implemented elastic-net regularization for gene selection. 
The second step generated gene signatures for individual cell types using selected genes from first step and then implemented a binary regularized logistic regression for each cell type against all other samples.



Still to add: 
techniques in deep learning eg.
Deep and shallow architecture:
https://ieeexplore.ieee.org/document/7863293
