# Understanding experimental design of machine learning to inform requirements for data

Component of ML experiments
Machine learning algorithms identify patterns that explain or fit a given dataset.
So every machine learning algorithm (_model_) expects the following fundamental parts as input: 1. a _training dataset_ , 2. a _cross-validation dataset_ , 3. a _hold-out validation dataset_ .
A _training dataset_ is used to expose the model to underlying patterns present in the data of interest.
A _cross-validation dataset_ is a smaller dataset which is used during training of the model to help update its parameters (_model tuning_).
A _hold-out validation set_ is a new or unseen dataset that is used to test whether the patterns learned by the model can be generalized to new data points.
While the cross-validation dataset helps the model iteratively update its parameters to learn important patterns in the training data, the hold-out validation set helps test the generalizability of the model.
Generalizability of a model is its ability to recognize patterns that can help predict the class or an outcome for previously unseen data.
High generalizability of a model on previously unseen data suggest that the model has identified fundamental patterns in the data that may also inform our knowledge regarding the question of interest for which the experiment was designed.

Training and testing
The design of a machine learning experiment begins with splitting a single dataset of interest such that 70% of the data is used as the _training dataset_, 20% of the data is used as the _cross-validation dataset_, and remaining 10% of the data is used as the _hold-out validation dataset_.
This makes sure that all the datasets involved in training and testing a model maintain uniformity in the features.
In case of rare diseases where multiple datasets may be combined to make a large enough training dataset, special care is taken to standardize the features and the patterns therein.
The iterative training stage helps the model learn important patterns in the training dataset and then use the cross-validation dataset to test for errors in prediction and update its learning parameters (_hyperparameter tuning_ or _model tuning_).
Multiple approaches exist for cross-validation e.g. leave-p-out cross-validation, leave-one-out cross-validation, k-fold cross-validation, Monte-Carlo random subsampling cross-validation [@doi:10.1007/978-1-4614-6849-3].
In case of k-fold cross-validation, a given dataset is shuffled randomly and split into _k_ parts.
One of the _k_ parts is reserved as the _cross-validation dataset_ and the rest are used for training.
In the next iteration, a different part is used as the _cross-validation dataset_, while the rest are used for training.
In cases where training data is limited, the k-fold cross validation approach can help maximal utilization of the available data.

Hyperparameter tuning
TBD
