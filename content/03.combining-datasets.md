## Constructing machine learning-ready rare disease datasets

High-throughput ‘omic’ assays generate thousands of measurements in the case of transcriptomic sequencing to billions of measurements in the case of whole genome sequencing, resulting in high-dimensional datasets, regardless of the underlying disease or condition being assayed. 
A typical rare disease dataset consists of a small number of samples[@doi:10.1186/s13023-020-01424-6] leading to the “curse of dimensionality” (i.e., few samples but many features), which can lead to spurious results or models that do not generalize to new datasets [@doi:10.1038/nrc2294].
More features often mean increased missing observations (_sparsity_), more dissimilarity between samples (_variance_), and increased redundancy between individual features or combinations of features (_multicollinearity_) [@doi:10.1038/s41592-018-0019-x], all of which contribute to a challenges in ML implementation.

One of the important factors in machine learning is performance (e.g. the accuracy of a supervised model in identifying patterns relevant for the biological question of interest, or the reliability of an unsupervised model in identifying hypothetical biological patterns that are supported by post-hoc validation and research). 
When small sample sizes compromise an ML model’s performance, then two approaches can be taken to manage sparsity, variance, and multicollinearity: 1) increase the number of samples, 2) improve the quality of samples. 
In the first approach, appropriate training, evaluation, and held-out validation sets could be constructed by combining multiple rare disease cohorts (Figure [@fig:1]a, Box 1). 
When combining datasets, special attention should be directed towards data harmonization since data collection methods can differ from cohort to cohort. 
Without careful selection of aggregation methods, one may introduce variability into the combined dataset that can negatively impact the ML model’s ability to learn or detect meaningful signals. 
Steps such as reprocessing the data using a single pipeline, using batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalizing raw values appropriately without affecting the underlying variance in the data [@doi:10.1186/gb-2010-11-3-r25, @doi:10.1371/journal.pcbi.1003531, @doi:10.1186/s13059-014-0550-8] may be necessary to mitigate unwanted variability. (Figure [@fig:1]a)
Data harmonization may also entail the standardization of sample labels, for example, using biomedical ontologies to normalize how samples are annotated across multiple datasets.

Another way to improve the quality of a dataset is to improve the accuracy of metadata (both the description of technical variables and biologically relevant phenotypes) for each sample in the dataset. 
This may increase the effectiveness of ML models in extracting biologically relevant patterns from small datasets. 
The recognized need for improved labeling of, for instance, genomic data is highlighted by the recent introduction of the Phenopackets standard for sharing clinical phenotype data [@url:https://www.ga4gh.org/news/phenopackets-standardizing-and-exchanging-patient-phenotypic-data/; @url:https://phenopacket-schema.readthedocs.io/en/2.0.0/basics.html].
Collaboration with domain experts to boost the value of research datasets through careful annotation, and subsequent sharing of well-annotated datasets, is required to foster effective use of datasets in the future.

How does one know if a composite dataset has undergone proper harmonization and annotation? 
Ideally, the structure of the composite dataset reflects differences in variables of interest, such as phenotype labels. 
If the samples from the same cohort tend to group together regardless of phenotype, this suggests that the datasets used to generate the composite dataset need to be corrected to overcome differences in how the data were generated or collected. 
In the next section, we will discuss approaches that can aid in identifying and visualizing structure in datasets to determine whether composite rare disease datasets are appropriate for use in ML.

![Combining datasets to increase data for training machine learning models. a) Appropriate methods are required to combine smaller datasets into a larger composite dataset: The left panel shows multiple small rare disease datasets that need to be combined to form a dataset of higher sample size. The color of the samples suggests classes or groups present in the datasets. The shape represents the dataset of origin. The middle panel shows methods that may be used to combine the datasets while accounting for dataset-specific technical differences. The right panel shows Principal Component Analysis of the combined datasets to verify proper integration of samples in the larger dataset. b) Composite datasets can be used to make training, evaluation, and validation datasets for machine learning: Left panel shows the division of the composite dataset into training dataset and a held-out validation dataset (top). Shapes indicate the study of origin. The held-out validation set is a separate study that has not been seen by the model. The training set is further divided into training and evaluation datasets for k-fold cross-validation (in this example k=4), where each fold contains all samples from an individual study. This approach is termed study-wise cross validation, and supports the goal of training models that generalize to unseen cohorts.The panel on the right shows the class distribution of the training, evaluation, and held-out validation datasets.](images/figures/lower-res-figures/figure-1-combining-datasets.png){#fig:1}
