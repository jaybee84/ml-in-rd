## Constructing machine learning-ready rare disease datasets

High-throughput 'omic' data methods generate high-dimensional data (or data containing many features), regardless of the underlying disease or condition being assayed.
A typical rare disease dataset is comprised of a small number of samples [@doi:10.1186/s13023-020-01424-6].
A lack of samples gives rise to the “curse of dimensionality” (i.e., few samples but many features), which can contribute to the poor performance of models [@doi:10.1038/nrc2294].
More features often means increased missing observations (_sparsity_), more dissimilarity between samples (_variance_), and increased redundancy between individual features or combinations of features (_multicollinearity_) [@doi:10.1038/s41592-018-0019-x], all of which contribute to a challenges in ML implementation.

When small sample sizes compromise an ML model's performance, then two approaches can be taken to manage sparsity, variance, and multicollinearity: 1) increase the number of samples, 2) improve the quality of samples.
In the first approach, appropriate training, evaluation, and held-out validation sets could be constructed by combining multiple rare disease cohorts (Figure {@fig:1}a, Box 1).
When combining datasets, special attention should be directed towards _data harmonization_ since data collection methods can differ from cohort to cohort.
Without careful selection of aggregation methods, one may introduce variability into the combined dataset that can negatively impact the ML model's ability to learn or detect meaningful signal.
Steps such as reprocessing the data using a single pipeline, using batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalizing raw values [@doi:10.1186/gb-2010-11-3-r25] may be necessary to mitigate unwanted variability. (Figure {@fig:1}a)

In the second approach, small but thoughtfully generated datasets can enhance the performance of ML models.
One way to improve the quality of a dataset is to improve the accuracy of clinical annotations or phenotypes for each sample in the dataset, which may increase the effectiveness of ML models in extracting biologically relevant patterns. 
The recognized need for improved labeling of, for instance, genomic data is highlighted by the recent introduction of the Phenopackets standard for sharing clinical phenotype data [@url:https://www.ga4gh.org/news/phenopackets-standardizing-and-exchanging-patient-phenotypic-data/; @url:https://phenopacket-schema.readthedocs.io/en/2.0.0/basics.html].
Collaboration with domain experts to boost the value of research datasets through careful annotation, and subsequent sharing of well-annotated datasets, is required to foster effective use of datasets in the future.

How does one know if a composite dataset has undergone proper harmonization and annotation?
Ideally, the structure of the composite dataset is reflects differences in variables of interest, such as phenotype labels.
If the samples from the same cohort tend to group together regardless of phenotype, this suggests that the datasets used to generate the composite dataset need to be corrected to overcome differences in how the data were generated or collected.
In the next section, we will discuss approaches that can aid in identifying and visualizing structure in datasets to determine whether composite rare disease datasets are appropriate for use in ML.

![Combining datasets to increase data for training machine learning models. a) Appropriate methods are required to combine smaller datasets into a larger composite dataset: The left panel shows multiple small rare disease datasets that need to be combined to form a dataset of higher sample size. The color of the samples suggest classes or groups present in the datasets. The shape represents the dataset of origin. The middle panel shows methods that may be used to combine the datasets while accounting for dataset-specific technical differences. The right panel shows Principal Component Analysis of the combined datasets to verify proper integration of samples in the larger dataset. b) Composite datasets can be used to make training, evaluation, and validation datasets for machine learning: Left panel shows the division of the composite dataset into training dataset and a held-out validation dataset (top). The training set is further divided into training and evaluation datasets for k-fold cross-validation (in this example k=4). The panel on the right shows the class distribution of the training, evaluation, and held-out validation datasets](content/images/figures/pdfs/figure-1-combining-datasets-resized.png){#fig:1}

