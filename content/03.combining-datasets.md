## Constructing machine learning-ready rare disease datasets

High-throughput 'omic' data methods generate high-dimensional data (or data containing many features), regardless of the underlying disease or condition being assayed.
A typical rare disease dataset is comprised of a small number of samples [@doi:10.1186/s13023-020-01424-6].
A lack of samples gives rise to the “curse of dimensionality” (i.e., few samples but many features), which can contribute to the poor performance of models [@doi:10.1038/nrc2294].
More features often means increased missing observations (_sparsity_), more dissimilarity between samples (_variance_), and increased redundancy between individual features or combinations of features (_collinearity_) [@doi:10.1038/s41592-018-0019-x], all of which contribute to a challenges in ML implementation.

When small sample sizes compromise an ML model's performance, then two approaches can be taken to manage sparsity, variance, and collinearity: 1) increase the number of samples, 2) improve the quality of samples.
In the first approach, appropriate training, evaluation, and held-out validation sets could be constructed by combining multiple rare disease cohorts (Figure {@fig:1}a, Box 1).
When combining datasets, special attention should be directed towards _data harmonization_ since data collection methods can differ from cohort to cohort.
Without careful selection of aggregation methods, one may introduce variability into the combined dataset that can negatively impact the ML model's ability to learn or detect meaningful signal.
Steps such as reprocessing the data using a single pipeline, using batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalizing raw values [@doi:10.1186/gb-2010-11-3-r25] may be necessary to mitigate unwanted variability. (Figure {@fig:1}a)

In the second approach, small but thoughtfully generated datasets can enhance the performance of ML models.
One way to improve the quality of a dataset is to improve the accuracy of clinical annotations or phenotypes for each sample in the dataset. 
In our experience, collaboration with domain experts has proved to be essential in gaining insight into potential sources of variation in the datasets.
An anecdotal example from the authors' personal experience: conversations with a rare disease clinician revealed that samples in a particular tumor dataset were collected using vastly different surgical techniques (laser ablation and excision vs. standard excision). 
This information was not readily available to non-experts, but was obvious to the clinician. 
Addition of this kind of important metadata or labels to the samples can greatly help ML models become more effective in extracting biologically relevant patterns.
Such instances underline the fact that continuous collaboration with domain experts and the sharing of well-annotated data is needed to generate robust datasets in the future.

How does one know if a composite dataset has undergone proper harmonization and annotation?
Ideally, the structure of the composite dataset is reflects differences in variables of interest, such as phenotype labels.
If the samples from the same cohort tend to group together regardless of phenotype, this suggests that the datasets used to generate the composite dataset need to be corrected to overcome differences in how the data were generated or collected.
In the next section, we will discuss approaches that can aid in identifying and visualizing structure in datasets to determine whether composite rare disease datasets are appropriate for use in ML.

![Combining datasets to increase training data](content/images/figures/pdfs/figure-1-combining-datasets.png){#fig:1}