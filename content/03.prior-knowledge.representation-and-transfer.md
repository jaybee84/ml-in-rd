#### Representation learning

Representation learning, also called feature learning, is the process of learning features from raw data, where a feature is an individual variable or property.
An algorithm or approach will construct features as part of training and, in the case of supervised feature learning, use those features to predict labels on input data. 
Using an example from transcriptomics, an unsupervised method such as matrix factorization can be used to extract a low-dimensional representation of the gene-level data, learning features that are a combination of input genes' expression levels [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can then be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8]. 
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1016/j.zemedi.2018.11.002] and [@doi:10.1098/rsif.2017.0387]), which are trained to predict labels or classes, are also an example of representation learning.

Whether or not a learned representation or set of features is _useful_ depends on the task at hand.
In a supervised setting, it may be sufficient for a feature to distinguish between classes, but if we hope to use a feature for biological discovery, we may prioritize intepretability.
For example, learned features in the medical imaging domain may be a series of edges that constitute a blood vessel formation that discriminates between disease states or the learned features may not align with characteristics recognized as important by domain experts at all.
From transcriptomics data, learned features could be coordinated sets of genes involved in a biological process that are descriptive in some way [@doi:10.1038/s41467-020-14666-6], but do not necessarily distinguish cases from controls in our study.

In the rare disease domain, Dincer et al. leveraged publicly available acute myeloid leukemia (AML) gene expression data to improve the prediction of _in vitro_ drug responses [@doi:10.1101/278739]. 
The authors trained a variational autoencoder (VAE) on AML data that had been collected over time without the phenotypic information they were interested in–in this case, drug response. 
A VAE is an unsupervised neural network that learns a series of representations or encodings from data, where each learned attribute will have a probability distribution associated with it rather than a single value.
The authors used the learned attributes to encode a low-dimensional representation of held-out AML data with phenotype labels of interest, and used this low-dimensional representation as input to a classifier that predicted _in vitro_ drug response.

Representation learning tends to be data-intensive–many samples are required.
Though there were over 6500 publicly available AML samples from many different studies used as part of the training set in Dincer et al. [@doi:10.1101/278739], we expect that in other rare diseases considerably fewer samples will be available for training or, in the case of systemic diseases, studies may be from different tissues.
The study by Dincer and colleagues highlights another challenge: even if enough samples have been collected over time to support representation learning, these samples may not be associated with the deep phenotypic information that would maximize their scientific value.
In the next section, we will introduce methods or approaches that may be more broadly useful in rare diseases; representation learning underlies many of them.

#### Transfer, multitask, and few-shot learning

We focus on a series of approaches that are centered on the following concept: to realize the potential of machine learning for biological discovery in rare diseases, we often can not study an individual rare disease alone as samples are limited.
We may instead build on prior knowledge and large volumes of data that do not directly assay our disease of interest, but are closely-enough related to be valuable for discovery.
We can leverage shared features, whether they are biological patterns that are a normal part of development aberrant in a particular disease context or an imaging anomaly present in both rare and common diseases, for advancing our understanding.
We discuss transfer learning, multitask learning, and few-shot learning approaches below. 

##### Transfer learning

Transfer learning is an approach where a model trained for one task or domain (source domain) is applied to another, typically related task or domain (target domain).
Transfer learning can be supervised or unsupervised, where one or both of the source and target domains have labels or both domains are unlabeled, respectively.
Though there are multiple types of transfer learning, we will focus principally on feature-representation-transfer [@doi:10.1109/TKDE.2009.191] here. 
Feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain [@doi:10.1109/TKDE.2009.191].
This concept is embodied in Dincer et al., where features are learned from unlabeled AML data and then used to encode a low-dimensional representation of AML data with _in vitro_ drug response labels [@doi:10.1101/278739].
The authors then used this low-dimensional representation as input to predict drug response labels–a supervised example.

In an unsupervised case, Taroni et al. trained a Pathway-Level Information ExtractoR (PLIER) [@doi:10.1038/s41592-019-0456-1] on a large generic collection of human transcriptomic data (recount2 [@doi:10.1038/nbt.3838]) and used the latent variables learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003].
(Here "unseen" refers to the fact that no AAV or medulloblastoma data were included in the training set.)
PLIER is a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways and gene expression data as input; PLIER utilizes regularization such that some latent variables learned by the model will align with input gene sets and, for those latent variables that are aligned with gene sets, latent variables will be associated with a limited number of gene sets [@doi:10.1038/s41592-019-0456-1].
We demonstrated that training on larger collections of randomly selected samples produced models that captured a larger proportion of input gene sets and were more suitable for disentangling closely related signals (e.g., type I and type II interferon signaling), suggesting that larger training sets produced PLIER models that are more valuable for biological discovery [@doi:10.1016/j.cels.2019.04.003].

Though models trained on general collections of transcriptomic data had more appealing properties, that alone does not guarantee suitability for describing rare diseases.
To assess suitability, we must find ways to examine the relevance of learned features (latent variables) to the rare disease gene expression data.
In Taroni et al., we found that the expression of latent variables that could be matched between the MultiPLIER model and a model trained on a rare disease dataset alone were well-correlated, particularly in the case of latent variables that were biologically relevant (i.e., significantly associated with input gene sets) [@doi:10.1016/j.cels.2019.04.003].
Despite the absence of AAV from the training set, MultiPLIER was able to learn a latent variable where the genes with the highest contributions encode antigens that the antineutrophil cytoplasmic antibodies (ANCA) form against in AAV and with higher expression in a group of samples from patients with AAV that were reported to have more severe disease [@doi:10.1002/art.27398].
The utility of this approach stems from the fact that biological processes are often _shared_ between conditions–the same ANCA antigen genes are components of normal neutrophilic granule development that is likely captured or assayed in the collection of transcriptomic data used for training.
MultiPLIER has additional attributes that make it practical for studying rare diseases: not all latent variables learned by a PLIER model are associated with input gene sets, and therefore may capture technical noise separately from biological signal, and we can use a single model to describe datasets from multiple tissues or cohorts that are not obviously directly comparable instead of attempting to reconcile results from multiple models (see _05.heterogeneity.md_).

Taken together, the DeepProfile [@doi:10.1101/278739] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] results suggest transfer learning can be beneficial for studying rare diseases. 
In the natural images field, researchers have demonstrated that the transferability of features depends on relatedness of tasks [@arxiv:1411.1792].
The limits of transfer learning for and the concept of relatedness in high-dimensional biomedical data assaying rare diseases are open research questions.
In the authors' opinion, selecting an appropriate model for a given task (e.g., using PLIER for biological discovery) and evaluation strategies that are well-aligned with a research goal are crucial for successful application of these approaches in rare diseases.
