## Learning representations from rare disease data

Dimensionality reduction methods help explore and visualize underlying structure in the data (e.g., [@doi:10.1038/s41467-019-13056-x]), to define sample subgroups (e.g., [@doi:10.1038/s41467-020-15351-4]), or for feature selection and extraction during application of specific machine learning models [@doi:10.1007/978-3-030-03243-2_299-1] (Figure [@fig:2]c).
Unsupervised methods, in finding low-dimensional patterns in data, can ‘compress’ information from a large number of features into a smaller number of features [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://www.jmlr.org/papers/v9/vandermaaten08a.html; @https://arxiv.org/abs/1802.03426] (Figure [@fig:2]).
A method commonly used for dimensionality reduction is principal components analysis (PCA).
PCA identifies higher order features, termed _principal components_ (PCs), that are combinations of original features.
The PCs are calculated in a way that maximizes the amount of information (variance) they contain and ensures that each PC is uncorrelated with the other PCs. [@doi:10.1098/rsta.2015.0202]
In practice, researchers often use the first few PCs to reduce the dimensionality without removing what may be important biological variability in the data.
Nguyen and Holmes highlight the use of "elbow method" to select the number of appropriate dimensions. [@doi:10.1371/journal.pcbi.1006907]
Multidimensional scaling (MDS), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) are other popular dimension reduction methods, often used for low-dimensional visualization and interpretation of data [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002]
Testing multiple dimensionality reduction methods may be necessary to obtain a more comprehensive portrait of the data. [@doi:10.1186/s13059-020-02021-3]
Other unsupervised learning approaches such as k-means or hierarchical clustering are used to characterize structure in genomic and imaging data. [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766]
Dimensionality reduction methods are a subset of a type of ML called representation learning.
Representation learning methods have been used to extract features from transcriptomics datasets made of combinations of gene expression values [@doi:10.1038/s41467-020-14666-6; @doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3], predict rare pathologies from images [@doi:10.1016/j.media.2020.101660] (Box 1a) or detect cell populations associated with rare diseases [@doi:10.1038/ncomms14825].

When applied to complex systems, representation learning generally requires many samples and therefore may appear to aggravate the curse of dimensionality.
However, it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets.
In later sections, we discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer learning.
Once the dimensions of the training dataset have been reduced, model training can proceed using the experimental design as outlined in Box 2.
