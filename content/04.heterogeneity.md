## Learning representations from rare disease data

Dimensionality reduction methods can help explore and visualize underlying structure in the data (e.g., [@doi:10.1038/s41467-019-13056-x]), to define sample subgroups (e.g., [@doi:10.1038/s41467-020-15351-4]), or for feature selection and extraction during application of specific machine learning models [@doi:10.1007/978-3-030-03243-2_299-1] (Figure [@fig:2]c).
These methods ‘compress’ information from a large number of features into a smaller number of features in an unsupervised manner [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://www.jmlr.org/papers/v9/vandermaaten08a.html; @https://arxiv.org/abs/1802.03426; @doi:10.1016/j.media.2020.101660; @doi:10.1038/ncomms14825] (Figure [@fig:2], Box 1a).
An example of a method that is commonly used for dimensionality reduction is principal components analysis (PCA). 
PCA identifies new features or dimensions, termed _principal components_ (PCs), that are combinations of original features. 
The PCs are calculated in a way that maximizes the amount of information (variance) they contain and ensures that each PC is uncorrelated with the other PCs. [@doi:10.1098/rsta.2015.0202]
In practice, researchers often use the first few PCs to reduce the dimensionality without removing what may be important or informative variability in the data. 
Other methods like multidimensional scaling (MDS), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) can also help identify useful patterns in the data, though t-SNE and UMAP require adjusting hyperparameters to get results that are not misleading or  not reproducible. [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002]
Testing multiple dimensionality reduction methods, rather than a single method, may be necessary to obtain a more comprehensive portrait of the data. [@doi:10.1186/s13059-020-02021-3]
Nguyen and Holmes discuss additional important considerations for using dimensionality reduction methods such as selection criteria and interpretation of results. [@doi:10.1371/journal.pcbi.1006907]
Beyond dimensionality reduction, other unsupervised learning approaches such as k-means clustering or hierarchical clustering have been used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766]

Representation learning approaches (which include dimensionality reduction) learn low-dimensional representations (composite features) from the raw data. 
For example, representation learning through matrix factorization methods can extract features from transcriptomics datasets that are made of combinations of gene expression values. [@doi:10.1038/s41467-020-14666-6; @doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3]

When applied to complex biological systems, representation learning generally requires many samples and therefore may appear to aggravate the curse of dimensionality. 
However, it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In later sections, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer learning. 
Once the dimensions of the training dataset have been reduced, model training can proceed using the experimental design as outlined in Box 2.

