## Reducing misinterpretation of model output with statistical techniques 

Machine learning methods generally work well on data that meets a few critical assumptions. 
First, the dataset contains equal number of samples for all categories (no "class imbalance").
Second, the dataset is complete; all samples have measurements for all variables in the dataset (i.e., the dataset is not "sparse", meaning that it is not missing data for some of the samples).
Third, there is no ambiguity about the labels for the samples in the dataset (i.e., no "label-noise"). 

Rare disease datasets, however, violate many of these assumptions.
There is generally a high _class imbalance_ due to small number of samples for specific classes (e.g., only a few patients with a particular rare disease in a health records dataset), the data are often _sparse_, and there may be abundant _label-noise_ due to incomplete understanding of the disease.
All of these contribute to low signal to noise ratio in rare disease datasets.
Thus, applying ML to rare disease data without addressing the aforementioned shortcomings may lead to models that have low reproducibility or are hard to interpret.

Class imbalance in datasets can be addressed using decision tree-based ensemble learning methods (e.g., random forests) (Figure[@fig:3]a).
Random forests use resampling (with replacement) based techniques to form a consensus about the important predictive features identified by the decision trees [@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6]. 
Additional approaches like combining random forests with resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
Resampling approaches are most helpful in constructing confidence intervals for algorithms that generate the same outcome every time they are run (i.e., deterministic models).
For decision trees that choose features at random for selecting a path to the outcome (i.e., are non-deterministic), resampling approaches can be helpful in estimating the reproducibility of the model. 

In situations where decision tree-based ensemble methods fail when they are applied to rare disease datasets, cascade learning has been a viable alternative [@doi:10.1007/s11634-019-00354-x; @pmid:30815073].
In cascade learning, multiple methods leveraging distinct underlying assumptions are used in tandem to capture stable patterns existing in the dataset [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
For example, a cascade learning approach for identifying rare disease patients from electronic health record data incorporated independent steps for feature extraction (word2vec [@arxiv:1301.3781]), preliminary prediction with ensembled decision trees, and then prediction refinement using data similarity metrics [@pmid:30815073]. 
Combining these three methods resulted in better overall prediction when implemented on a silver standard dataset, as compared to using the methods in isolation.
In addition to cascade learning, other approaches that can better represent rare classes like inverse class weighting and oversampling [doi:10.1613/jair.953] may also help mitigate limitations due to class imbalance. 

The presence of label-noise and sparsity in the data can lead to overfitting of models to the training data, meaning that the models show high prediction accuracy on the training data but low prediction accuracy (and large prediction errors) on new evaluation data. 
Overfit models tend to rely on patterns that are unique to the training data (for example, the calibration of the instrument that was used to generate the training data), and not generalizable to new data (e.g., data generated on the same instrument that has been recalibrated). [@doi:10.1073/pnas.1900654116]
In such cases, regularization can not only protect ML models as well as learned representations from poor generalizability caused by overfitting, but also reduce model complexity by reducing the feature space available for training [@doi:10.1371/journal.pgen.1004754, @doi:10.1002/sim.6782]. (Figure[@fig:3]a)
Regularization is an approach by which a penalty is added to the model to avoid making large prediction errors. 
Some examples of ML methods with regularization include ridge regression, LASSO regression, and elastic net regression, among others.
Regularization is often used in rare variant discovery and immune cell signature discovery studies; much like rare disease, these examples need to accommodate sparsity in data.
For example, LASSO has been used to capturing combinations of rare and common variants associated with specific traits have proven beneficial [@doi:10.1186/1753-6561-5-s9-s113].
In this example, applying LASSO regularization reduced the number of common variants included as features in the final analysis generating a simpler model while reducing error in the association of common and rare variants with a specific trait. 
In the context of rare immune cell signature discovery, variations of elastic-net regression were found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/s12859-019-2994-z]. 
Thus, regularization methods like LASSO or elastic-net are beneficial in ML with rare observations, and are worth exploring in the context of rare diseases.[@doi:10.1371/journal.pgen.1004754] 
Other examples of regularization that have been successfully applied to rare disease ML include Kullbackâ€“Leibler divergence (KL divergence) and dropout regularization.
In a study using a variational auto encoder (VAE) for dimensionality reduction in gene expression data from AML samples, the KL divergence between the input data and its low dimensional representation provided the regularizing penalty for the model.[@doi:10.1101/278739; @doi:10.48550/arXiv.1312.6114]
In a study using a convolutional neural network (CNN) to identify tubers in MRI images from TS patients, overfitting was minimized using the dropout regularization method which removed randomly chosen network nodes in each iteration of the CNN model generating simpler models in each iteration.[@doi:10.1371/journal.pone.0232376]
Thus, depending on the learning method that is used, regularization approaches should be incorporated into data analysis when working with rare disease datasets. 

![OLD FIGURE (new figure still WIP) Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) A schematic showing the concept of bootstrap, B) A schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) A schematic showing the concept of regularization to selectively learn relevant features, D) A schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](images/figures/pdfs/statistical-techniques.png){#fig:3}
