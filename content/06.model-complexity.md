### Augmenting machine learning models with statistical techniques to reduce misinterpretation


Machine learning methods are generally accompanied by a few critical assumptions. 
First, ML methods often work best on datasets that contain equal number of samples for all categories (no "class imbalance").
Second, the dataset is complete; all samples have measurements for all variables in the dataset (i.e., the dataset is not "sparse", meaning that it is not missing data for some samples).
Third, there is no ambiguity about the labels for the samples in the dataset (no "label-noise", meaning that, for example, we are confident in the disease of each patient in a dataset).  
Rare disease datasets, however, violate many of these assumptions.
There is generally a high _class imbalance_ due to small number of samples for specific classes (e.g., only a few patients with a particular rare disease in a health records dataset), the data are often _sparse,_ and there may be abundant _label-noise_ due to incomplete understanding of the disease.
All of these contribute to low signal to noise ratio in rare disease datasets.
Thus, applying ML to rare disease data without any accommodations for the above shortcomings may lead to models that lack stability required for reproducibility or simplicity required for interpretation.
In this section, we highlight a few ML techniques that can help manage the challenges in applying ML models applied to rare disease data.

Accommodating class imbalance in datasets and increasing the stability of ML predictions can be achieved through decision tree-based ensemble learning methods (Figure[@fig:2]A-B).
Random forests use resampling (with replacement) based techniques to form a consensus about the important predictive features identified by the decision trees[@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6]. 
Additional approaches like combining random forests with resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
Resampling approaches are most helpful in constructing confidence intervals for algorithms that generate the same outcome every time they are run (i.e. deterministic models).
For non-deterministic models like decision trees that choose features at random for selecting a path to the outcome, resampling approaches can be helpful in determining robustness or stability of the model. 

Recent studies suggest that there are limitations to decision tree-based ensemble methods when applied to rare disease datasets, leading to adoption of cascade learning, a variant of ensemble learning [@doi:10.1007/s11634-019-00354-x; @pmid:30815073].
In cascade learning, multiple methods leveraging distinct underlying assumptions are used in tandem to capture stable patterns existing in the dataset [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
For example, a cascade learning approach for identifying rare disease patients from electronic health record data utilized independent steps for feature extraction (word2vec [@arxiv:1301.3781]), preliminary prediction with ensembled decision trees, and prediction refinement using data similarity metrics [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on a silver standard dataset in isolation.
Other approaches like inverse class weighting, oversampling (with methods like Synthetic Minority Oversampling Technique, abbreviated SMOTE), or methods that adequately represent the rarer classes are also beneficial in increasing stability of predictions while accommodating for class imbalance.

The presence of label-noise and sparsity in the data can also lead to overfitting of models to the training data, meaning that the models show high prediction accuracy on the training data but low prediction accuracy on new data. 
Features identified as predictive by overfit models are often not representative of the data making the model not generalizable to new data. [@doi:10.1073/pnas.1900654116]
In such cases, regularization can not only protect ML models from poor generalizability caused by overfitting, but also reduce model complexity by reducing the feature space available for training. (Figure[@fig:2]C)
Regularization is often used in rare variant discovery and immune cell signature discovery studies which are useful examples that also need to accommodate sparsity of data like rare diseases.
For example, among various regularization methods (ridge regression, LASSO regression, and elastic-net)[@doi:10.1111/j.1467-9868.2005.00503.x], hybrid applications of LASSO for capturing combinations of rare variants have proven beneficial [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113] .
In the context of rare immune cell signature discovery, elastic-net regression was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1186/s12859-019-2994-z]. 
Thus regularization methods like LASSO or elastic-net have been methods of choice while working with rare observations; use of these regularization approaches should be considered while working with rare disease datasets. 

![Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) A schematic showing the concept of bootstrap, B) A schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) A schematic showing the concept of regularization to selectively learn relevant features, D) A schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](images/figures/pdfs/statistical-techniques.png){#fig:2}
