---
author-meta:
- Jineta Banerjee [1]
- Jaclyn N Taroni [1]
- Robert J Allaway
- Deepashree Venkatesh Prasad
- Justin Guinney *
- Casey Greene *
bibliography:
- content/manual-references.json
date-meta: '2021-03-02'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Machine learning in rare disease\" />\n<meta name=\"citation_title\" content=\"Machine learning in rare disease\" />\n<meta property=\"og:title\" content=\"Machine learning in rare disease\" />\n<meta property=\"twitter:title\" content=\"Machine learning in rare disease\" />\n<meta name=\"dc.date\" content=\"2021-03-02\" />\n<meta name=\"citation_publication_date\" content=\"2021-03-02\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Jineta Banerjee [1]\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-1775-3645\" />\n<meta name=\"citation_author\" content=\"Jaclyn N Taroni [1]\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-4734-4508\" />\n<meta name=\"citation_author\" content=\"Robert J Allaway\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-3573-3565\" />\n<meta name=\"twitter:creator\" content=\"@allawayr\" />\n<meta name=\"citation_author\" content=\"Deepashree Venkatesh Prasad\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-5756-4083\" />\n<meta name=\"citation_author\" content=\"Justin Guinney *\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-1477-1888\" />\n<meta name=\"citation_author\" content=\"Casey Greene *\" />\n<meta name=\"citation_author_institution\" content=\"Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-8713-9213\" />\n<link rel=\"canonical\" href=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"og:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"twitter:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_pdf_url\" content=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://jaybee84.github.io/ml-in-rd/v/39fbddeed912687ca8044075b0d011da9e60d69f/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/39fbddeed912687ca8044075b0d011da9e60d69f/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/39fbddeed912687ca8044075b0d011da9e60d69f/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- rare disease
- machine learning
- transfer learning
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Machine learning in rare disease
...






<small><em>
This manuscript
([permalink](https://jaybee84.github.io/ml-in-rd/v/39fbddeed912687ca8044075b0d011da9e60d69f/))
was automatically generated
from [jaybee84/ml-in-rd@39fbdde](https://github.com/jaybee84/ml-in-rd/tree/39fbddeed912687ca8044075b0d011da9e60d69f)
on March 2, 2021.
</em></small>

## Authors



+ **Jineta Banerjee [1]**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1775-3645](https://orcid.org/0000-0002-1775-3645)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaybee84](https://github.com/jaybee84)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Jaclyn N Taroni [1]**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4734-4508](https://orcid.org/0000-0003-4734-4508)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaclyn-taroni](https://github.com/jaclyn-taroni)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Robert J Allaway**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3573-3565](https://orcid.org/0000-0003-3573-3565)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [allaway](https://github.com/allaway)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [allawayr](https://twitter.com/allawayr)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Deepashree Venkatesh Prasad**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-5756-4083](https://orcid.org/0000-0001-5756-4083)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [dvenprasad](https://github.com/dvenprasad)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Justin Guinney ***<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1477-1888](https://orcid.org/0000-0003-1477-1888)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jguinney](https://github.com/jguinney)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Casey Greene ***<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania; Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>



## Synopsis {.page_break_before}

(Instructions: Describe the background, basic structure of the article, list material to be covered indicating depth of coverage, how they are logically arranged, include recent pubs in the area, 300-500 words)

The advent of high-throughput profiling methods such as genomics, transcriptomics, and other technologies has accelerated basic research and made deep characterization of patient samples routine.
These approaches provide a rich portrait of genes, cellular pathways, and cell types involved in complex phenotypes.
Machine learning is often a perfect fit for extracting disease-relevant patterns from these high dimensional datasets.
Often, machine learning methods require many samples to identify recurrent and biologically meaningful patterns.
With rare diseases, biological specimens, and consequently data, are limited due to the rarity of the condition.
In this perspective, we outline the challenges and emerging solutions for using machine learning in rare disease settings.
We also note that precision medicine presents a similar challenge, in which a common disease is partitioned into small subsets of patients with shared etiologies and treatment strategies. 
Advances from rare disease research are likely to be highly informative for other applications as well, and we propose that the methods community should prioritize the development of machine learning techniques for rare disease research.


## Introduction {.page_break_before}

Rare disease research, as with many other biomedical domains, is increasingly using high-throughput profiling methods to better understand the mechanisms of the disease. 
These profiling methods, including RNA sequencing (RNA-seq, whole genome sequencing, imaging data, electronic health record data, among others, generate large and complex data. 
The analysis of such complex data from rare disease will require machine learning (ML)-based methodologies to assist in the modeling and interpretation of this data. 
Indeed, a systematic review of application of ML in rare disease in the last 10 years uncovered 211 human data studies in 74 different rare diseases employing ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) [@doi:10.1186/s13023-020-01424-6]. 
While this review points to the increasing popularity of ML methods in rare disease, there are various hurdles that are inherent to such datasets.
ML based methods benefit from large sample sizes, rare disease datasets typically contain fewer than one hundred samples [@doi:10.1186/s13023-020-01424-6].
Small datasets lead to a lack of statistical power and magnify the susceptibility of ML methods to misinterpretation and unstable performance.
Additionally, successful training of ML models require training datasets made of “gold standard” data where the diagnosis or label of a data point has very little uncertainty (or “label-noise”) associated with it [@doi:10.1093/jamia/ocw028]. 
Due to limited understanding of the biology of rare diseases, the symptoms or disease labels often come with significant label-noise (a _silver standard_ dataset) [@doi:10.1109/tnnls.2013.2292894]. 
Thus, specialized computational methods that can learn patterns from small datasets and can generalize to newly acquired data are required for rare disease applications [@doi:10.1016/j.ebiom.2019.08.027]. 
In this perspective, we first highlight ML approaches that address or better tolerate the limitations of rare disease data, and then discuss the future of ML applications in rare disease. 


### Manage complex high-dimensional rare disease data

In rare diseases, the high throughput ‘omic’ methods generate high dimensional data – data with many features, such as all of the mRNA transcripts in a sample – from a small number of samples.
A lack of samples gives rise to the “curse of dimensionality” (i.e., few samples but many features), which is an impediment in analyzing feature-rich data in sample-deficient contexts such as rare disease [@doi:10.1038/nrc2294] (Figure {@fig:1}A-B).
In particular, increasing the number of features can result in increased sparsity (missing observations), more dissimilarity between samples, and increased redundancy between individual features or combinations of features [@doi:10.1038/s41592-018-0019-x], all of which combine to create a challenging prediction problem. 
Furthermore, rare disease data collection and aggregation methods can add to these challenges by introducing technical variability into the data at hand.
In this section, we will discuss strategies for reducing the feature space and addressing technical artifacts through dimensionality reduction.

Dimensionality reduction methods like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) can help ‘compress’ information from a large number of features into a smaller number of features in an unsupervised manner [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://www.jmlr.org/papers/v9/vandermaaten08a.html; @https://arxiv.org/abs/1802.03426] (Figure {@fig:1}C).
These methods not only help in reducing the number of features in various types of data [@doi:10.1016/j.media.2020.101660; @doi:10.1038/ncomms14825], but can also be used to visualize structure or artifacts in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), to define sample subgroups (e.g. [@doi:10.1038/s41467-020-15351-4], or for feature selection and extraction during application of specific machine learning models [@doi:10.1007/978-3-030-03243-2_299-1] (Figure {@fig:1}D).

Rare disease datasets are often combined from multiple small studies leading to the confounding of biological characteristics with technical variables such as batch, sample preparation methodology, or sequencing platform [@doi:10.23915/distill.00002]. 
Methods like PCA, MDS, t-SNE, and UMAP can successfully identify the effect of these variables on the original data, though t-SNE and UMAP may require tuning of hyperparameters that may effect the output [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002].
Furthermore, testing multiple dimensionality reduction methods, rather than a single method, may be necessary to obtain a more comprehensive portrait of the data [@doi:10.1186/s13059-020-02021-3]. 
Nguyen and Holmes discuss additional important considerations for using dimensionality reduction methods such as selection criteria and interpretation of results [@doi:10.1371/journal.pcbi.1006907].
Beyond dimensionality reduction, other unsupervised learning approaches such as k-means clustering or hierarchical clustering have been used to characterize the structure present in genomic or imaging data [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766].
Other approaches like reprocessing the data using a single pipeline (when data are obtained from multiple sources), using batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalizing raw values [@doi:10.1186/gb-2010-11-3-r25] may be necessary to obtain meaningful insights from the data. 

Dimensionality reduction, or more fundamentally, representation learning, learns low-dimensional representations (composite features) from the raw data. 
For example, representation learning through matrix factorization can extract features from transcriptomics datasets that are made of combinations of gene expression values found in the training data [@doi:10.1038/s41467-020-14666-6], and use them to interpret test data [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
To ensure that the learned representations are generalizable to other data, the features learned by the model can be constrained through methods like regularization [@doi:10.1371/journal.pgen.1004754, @doi:10.1002/sim.6782]. 
Representation learning generally requires many samples when applied to complex biological systems and therefore may appear to aggravate the curse of dimensionality. 
However, it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In later sections, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer learning. 

![Dimension reduction can help manage the curse of dimensionality in rare disease data. A) Multiple datasets (shapes) with multiple phenotypes (purple, green) are combined for an analysis. The data (e.g., transcriptomic data) are highly dimensional, having thousands of features (f1-f100000). B) Evaluating the features, it appears that a combination of features (e.g., expressed genes) partition the purple samples from the green samples. C) Applying a dimensionality reduction method (e.g., PCA) condenses these features into new features (e.g., New Feature 1, a combination of f1, f2 .... f100000, and New Feature 2, a different combination of f1, f2 .... f100000). New Feature 1 describes the difference in input dataset (shapes) while New Feature 2 describes the difference in phenotype (color). D) New features (F1-F1000) can be used to interrogate the biology of the input samples, develop classification models, or use other analytical techniques that would have been more difficult with the original dataset dimensions.](images/figures/pdfs/dimensionality-reduction.png){#fig:1}


### Manage model complexity while preserving the value of machine learning

Translating machine learning findings into testable hypotheses requires the ML models to be both stable – the same predicted features should surface from the data if the model is run multiple times – and simple, as simple models guard against misinterpretation, while still being performant. 
Meeting these requirements is challenging in rare disease datasets where label-noise is abundant. 
In this section we highlight a few common ML techniques that can help improve the stability and simplicity of ML models applied to rare disease data.

Techniques like resampling and combining various ML methods together (ensemble learning) can help achieve stability in predictions (Figure[@fig:2]A-B). 
Resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
Alternatively, resampling with replacement (bootstrapping) helps estimate population values from datasets of limited size, and is also commonly used to find robust models when multiple models are combined into an ensemble ([@doi:10.1080/01621459.1997.10474007; @https://doi.org/10.1023/A:1010933404324; @doi:10.1198/0003130043277; @doi:10/c8xpqz; @doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
Ensemble learning methods like random forests use _bagging_ (bootstrap aggregation) of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@https://doi.org/10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6]. 
But recent studies suggest that there are limitations to decision tree-based ensemble methods when applied to rare disease datasets with substantial class imbalance and label-noise [@doi:10.1007/s11634-019-00354-x; @pmid:30815073]. 
This has led to the adoption of cascade learning, a variant of ensemble learning, where multiple methods leveraging distinct underlying assumptions are used in tandem; and augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
For example, a cascade learning approach for identifying rare disease patients from electronic health record data utilized independent steps for feature extraction (word2vec [@arxiv:1301.3781]), preliminary prediction with ensembled decision trees, and prediction refinement using data similarity metrics [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets also decreases the available data points per class. 
In such cases, a one-class-at-a-time cascade learning approach (where at each stage a binary classifier predicts a specific class against all others) has been found to produce simpler models that perform better compared to multi-class ensemble classifiers [@doi:10.1093/jamia/ocy109]. (Figure[@fig:2]D)

Regularization simplifies models by making the feature space proportionate with the sample space. (Figure[@fig:2]C)
Regularization can not only protect ML models from poor generalizability caused by overfitting (where the model performs well on held-out training data but poorly on new test data) [@doi:10.1073/pnas.1900654116], but also be used to constrain model complexity and reduce feature space. 
Three popular regularized methods, ridge regression, LASSO regression, and elastic-net regression, differ predominantly in how they modify the inclusion and weighting of features of the input data. 
Ridge regression can minimize the magnitude of the features, but cannot entirely remove features. 
LASSO regression, on the other hand, works well for selecting a few important features since it can minimize the magnitude of some features more than the others [@doi:10.1038/nmeth.4014]. 
A combination of LASSO and ridge, elastic-net regression [@doi:10.1111/j.1467-9868.2005.00503.x] selects the most useful features, especially in presence of a large number of correlated features.

Rare variant discovery and immune cell signature discovery studies, like rare diseases, face challenges of the sparsity of observations, and may be useful models for examining the utility of regularization in scenarios with limited signal.
For example, ridge regression has been used to combine rare variants into a single score to increase the signal of these variants [@doi:10.1371/journal.pone.0044173], while LASSO has been implemented along with group penalties to identify gene variants [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO in rare variant discovery studies like capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113], integrating with a probabilistic logistic Bayesian approach [@doi:10.4137/cin.s17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250] have also proven beneficial.
On the other hand, in the context of rare immune cell signature discovery, elastic-net regression was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1186/s12859-019-2994-z]. 
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space in data with rare observations; use of these regularization approaches should be considered while working with rare disease datasets. 

![Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) A schematic showing the concept of bootstrap, B) A schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) A schematic showing the concept of regularization to selectively learn relevant features, D) A schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](images/figures/pdfs/statistical-techniques.png){#fig:2}


### Build upon prior knowledge and indirectly related data {.page_break_before}

Rare diseases often lack large, normalized datasets, limiting our ability to study key attributes of these diseases. 
One strategy to overcome this is to integrate and explore rare disease information alongside other knowledge by combining a variety of different data types. By using several data modalities, it may be possible to gain a better understanding of rare diseases (e.g., identifying novel genotype-phenotype relationships or opportunities for drug repurposing).
Knowledge graphs (KGs) which integrate related-but-different data types, create a rich multimodal data source (e.g. Monarch Graph Database [@doi:10.1093/nar/gkw1128], hetionet [@doi:10.7554/elife.26726], PheKnowLator [@doi:10.1101/2020.04.30.071407], and the Global Network of Biomedical Relationships [@doi:10.1093/bioinformatics/bty114], Orphanet [@http://www.orpha.net]). 
These graphs connect genetic, functional, chemical, clinical, and ontological data to enable the exploration of relationships of data with disease phenotypes through manual review [@doi:10.1093/database/baaa015] or computational methods [@doi:10.1101/727925; @doi:10.1186/s12911-019-0938-1].(Figure[@fig:3]a)
KGs may include links or nodes that are specific to the rare disease of interest (e.g., an FDA approved treatment  would be a specific disease-compound link in the KG) as well as links that are more generalized (e.g., gene-gene interactions noted in the literature for a different disease). 

Rare disease researchers can leverage the entities and relationships in a knowledge graph outside of the specific disease-context [@doi:10.1101/727925]. 
Such approaches have been used in rare disease research in areas such as drug repurposing [@doi:10.1101/727925] and disease classification [@doi:10.1186/s12911-019-0938-1]. 
Identifying KG encoding methods that can provide actionable insights for a specific rare disease application is an active area of research.

Other approaches that build upon prior knowledge and large volumes of related data include transfer learning, multitask learning, and few-shot learning approaches. 
These approaches leverage shared features, e.g., normal developmental processes that are aberrant in disease or an imaging anomaly present in both rare and common diseases, to advance our understanding of rare diseases. 
Transfer learning, where a model trained for one task or domain (source domain) is applied to another related task or domain (target domain), can be supervised or unsupervised. 
Among various types of transfer learning, feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain [@doi:10.1109/tkde.2009.191] (Figure[@fig:3]b).
For example, low-dimensional representations can be learned from tumor transcriptomic data and transferred to describe patterns associated with genetic alterations in cell line data [@doi:10.1186/s13059-020-02021-3].
Alternatively, multitask and few-shot learning are forms of supervised learning that often rely on deep neural networks. 

While multitask learning classifiers use shared representations to learn multiple related but individual predictions (tasks) simultaneously [@https://www.doi.org/10.1023/a:1007379606734], few-shot learning generalizes a model trained on related tasks to a new task with limited labeled data (e.g., the detection of a patient with a rare disease from a low number of examples of that rare disease) [@https://arxiv.org/abs/1706.05098; @https://arxiv.org/abs/1707.08114v2; @https://arxiv.org/abs/1904.05046v3] (Figure[@fig:3]c-d).
Smaller datasets tended to benefit from multitask learning (due to task relatedness, _multitask effect_) [@https://arxiv.org/abs/1606.08793], and the performance gains were generally context-dependent, i.e., multitask neural networks outperformed single-task networks for predicting complex rare phenotypes from EHR data or predicting drug sensitivity in rare cancer cell lines [@https://arxiv.org/abs/1808.03331; @doi:10.1101/2020.12.21.423514]. 
In contrast, one-shot or few-shot learning used prior knowledge to generalize a distance metric learned from input data to compare with a low number of new examples for prediction [@https://arxiv.org/abs/1904.05046v3, @doi:10.1021/acscentsci.6b00367; @doi:10.1021/acscentsci.6b00367; @doi:10.1038/s43018-020-00169-2]. 
In another study, a few-shot learning approach had a performance advantage over multitask learning, since predicting common conditions simultaneously resulted in a loss of performance for the multitask learner [@doi:10.1016/j.media.2020.101660]. 
Thus, transfer, multi-task, and few-shot learning are appealing approaches for rare disease applications, but their limits and potential utility are still open research questions. 

![Strategies that build upon prior knowledge help ML models learn patterns in rare disease datasets. A) Knowledge graphs integrate different data types and may allow models to learn from connections that are rare disease-specific or happen in many biomedical contexts. B) Transfer learning is when a model trained in for one task or domain is applied to another, related task. C) Multitask learning uses models that learn and leverage shared representations to predict multiple, related tasks. D) Few-shot learning generalizes a previously trained model to predict a new, related task with a limited number of samples.](images/figures/pdfs/prior-knowledge.png){#fig:3}


### Using composite approaches can be a powerful strategy

We have described multiple approaches for maximizing the success of ML applications in rare disease, but it is rarely sufficient to use any of these techniques in isolation. 
Below, we highlight two recent works in the rare disease domain that draw on concepts of feature-representation-transfer, use of prior data, and regularization.

A large public dataset of acute myeloid leukemia (AML) patient samples with no drug response data and a small _in vitro_ experiment with drug response data form the basis of our first example [@doi:10.1038/s41467-017-02465-5].
Training an ML model on the small _in vitro_ dataset alone faced the _curse of dimensionality_ and the dataset size prohibited representation learning.
Dincer et al. trained a variational autoencoder on the large AML patient dataset (VAE; see [definitions]) to learn meaningful representations in an approach termed DeepProfile [@doi:10.1101/278739] (Figure[@fig:4]a).
The representations or _encodings_ learned by the VAE were then _transferred_ to the small _in vitro_ dataset reducing it's number of features from thousands to eight, and improving the performance of the final LASSO linear regression model.
In addition to improvement in performance, the _encodings_ learned by the VAE captured more biological pathways than PCA, which may be attributable to the constraints on the encodings imposed during the training process (see [definitions]).
Similar results were observed for prediction of histopathology in another rare cancer dataset [@doi:10.1101/278739].

While DeepProfile was centered on training on an individual disease and tissue combination, some rare diseases affect multiple tissues that a researcher may be interested in studying together for the purpose of biological discovery. 
Studying multiple tissues poses significant challenges and a cross-tissue analysis may require comparing representations from multiple models.
Models trained on a low number of samples may learn representations that "lump together" multiple biological signals, reducing the interpretability of the results.
To address these challenges, Taroni et al. trained a Pathway-Level Information ExtractoR (PLIER) (a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways) on a large generic collection of human transcriptomic data [@doi:10.1038/s41592-019-0456-1]. 
PLIER used constraints (regularization) that learned _latent variables_ aligned with a small number of input gene sets, making it suitable for biological discovery or description of rare disease data. 
The authors _transferred_ the representations or _latent variables_ learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003]. (Figure[@fig:4]b)
MultiPLIER used one model to describe multiple datasets instead of reconciling output from multiple models, thus making it possible to identify commonalities among disease manifestations or affected tissues. 

DeepProfile [@doi:10.1101/278739] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] exemplify modeling approaches that can incorporate prior knowledge – thereby constraining the model space according to plausible or expected biology – or that can share information across datasets.
These two methods capitalize on the fact that similar biological processes are observed across different biological contexts and that the methods underlying the approaches can effectively learn about those processes. 

![Combining multiple strategies strengthens the performance of ML models in rare disease. A) The authors of DeepProfile trained a variational autoencoder (VAE) to learn a representation from acute myeloid leukemia data without phenotype labels, transferred those representations to a small dataset with phenotype labels, and found that it improved prediction performance [@doi:10.1101/278739]. B) The authors of MultiPLIER trained a Pathway-Level Information ExtractoR (PLIER) model on a large, heterogeneous collection of expression data and transferred the representations to multiple datasets from unseen rare diseases [@doi:10.1038/s41592-019-0456-1].](images/figures/pdfs/multiplier-DeepProfile.png){#fig:4}


### Outlook

Throughout this perspective, we highlighted various challenges in applying ML methods to rare disease data as well as examples of approaches that address these challenges.
Small sample size, while significant, is not the only roadblock towards application of ML in rare disease data.
The high dimensionality of modern data requires creative approaches, such as learning new representations of the data, to manage the curse of dimensionality.
Leveraging prior knowledge and transfer learning methods to appropriately interpret data is also required.
Furthermore, we posit that researchers applying machine learning methods on rare disease data should use techniques that increase confidence (i.e., bootstrapping) and penalize complexity of the resultant models (i.e., regularization) to enhance the generalizability of their work. 

All of the approaches highlighted in this perspective come with weaknesses that may undermine investigators' confidence in using these techniques for rare disease research.
We believe that the challenges in applying ML to rare disease are opportunities for data generation and method development going forward.
In particular, we identify the following two areas as important for the field to explore to increase the utility of machine learning in rare disease.

_Emphasis on not just "more n" but "more meaningful n"_

Mindful addition of data is key for powering the next generation of analysis in rare disease data.
While there are many techniques to collate rare data from different sources, low-quality data may hurt the end goal even if it adds to the size of the dataset.
In our experience, collaboration with domain experts has proved to be critical in gaining insight into potential sources of variation in the datasets.
An anecdotal example from the authors' personal experience: conversations with a rare disease clinician revealed that samples in a particular tumor dataset were collected using vastly different surgical techniques (laser ablation and excision vs standard excision). 
This information that was not readily available to non-experts, but was obvious the clinician. 
Such instances underline the fact that continuous collaboration with domain experts and the sharing of well-annotated data is needed to generate robust datasets in the future.

In addition to sample scarcity, there is a dearth of comprehensive phenotypic-genotypic databases in rare disease.
While rare disease studies that collect genomic and phenotypic data are becoming more common [@doi:10.1038/nrg3555; @doi:10.1038/nrg.2017.116; @doi:10.1056/NEJMra1711801], an important next step is to develop comprehensive genomics-based genotype-phenotype databases that prioritize clinical and genomics data standards in order to fuel interpretation of features extracted using ML methods.
Finally, mindful sharing of data with proper metadata and attribution to enable prompt data reuse is of utmost important in building datasets that can be of great value in rare disease [@https://www.nature.com/articles/s41576-020-0257-5].

_Development of methods that reliably support mechanistic interrogation of specific rare diseases_

The majority of ML methods for rare disease that we have investigated are applied to classification tasks. 
Conversely, we've found few examples of methodologies that interrogate biological mechanisms of rare diseases. 
This is likely a consequence of a dearth of methods that can tolerate the constraints imposed by rare disease research such as phenotypic heterogeneity and limited data.
An intentional push towards developing methods or analytical workflows that address this will be critical to apply machine learning approaches to rare disease data.

Method development with rare disease applications in mind requires the developers to bear the responsibility of ensuring that the resulting model is trustworthy.
The field of natural language processing has a few examples of how this can be achieved [@doi:10.18653/v1/N16-3020, @doi:10.18653/v1/P19-1073].
One way to increase trust in a developed model is by helping users understand the behavior of the developed model through providing explanations regarding why a certain model made certain predictions [@doi:10.18653/v1/N16-3020].
Another approach is to provide robust _error analysis_ for newly developed models to help users understand the strengths and weaknesses of a model [@doi:10.18653/v1/P19-1073; @https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00072; @doi:10.1093/bioinformatics/bth060].
Adoption of these approaches into biomedical ML is quickly becoming necessary as machine learning approaches become mainstream in research and clinical settings.

Finally, methods that can reliably integrate disparate datasets will likely always remain a need in rare disease research. 
To facilitate such analyses in rare disease, methods that rely on finding structural correspondences between datasets ("anchors") may be able to transform the status-quo of using machine learning methods in rare disease [@https://www.aclweb.org/anthology/W06-1615; @https://dl.acm.org/doi/10.5555/2283516.2283652; @doi:10.1016/j.cell.2019.05.031].
We speculate that this an important and burgeoning area of research, and we are optimistic about the future of applying machine learning approaches to rare diseases.


## Definitions {.page_break_before}

### Unsupervised learning: 
Machine learning algorithms which can learn features from unlabeled training data (e.g. datasets where the samples do not have disease or phenotype labels) to predict the class or phenotype of new or unseen test data are part of unsupervised learning. Examples of unsupervised learning include principal component analyses, multidimensional scaling, UMAP, t-SNE, and k-means clustering [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://www.jmlr.org/papers/v9/vandermaaten08a.html; @https://arxiv.org/abs/1802.03426, @https://projecteuclid.org/euclid.bsmsp/1200512992].

### Supervised learning: 
Machine learning algorithms that require training data with specific phenotype labels are part of supervised learning. 
Such algorithms learn correlations of features with the phenotype labels and use the learned correlations to predict the phenotype labels of unseen or new test data.

### VAE: 
Variational Autoencoders or VAEs are unsupervised neural networks that use hidden layers to learn or encode representations from available data while mapping the input data to the output data. 
VAEs are distinct from other autoencoders since the distribution of the encodings are regularized such that they are close to a normal distribution, which may contribute to learning more biologically relevant signals [@doi:10.1186/s13059-020-02021-3].


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
