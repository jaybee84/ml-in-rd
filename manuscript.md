---
author-meta:
- Jineta Banerjee
- Robert J Allaway
- Jaclyn N Taroni
- Casey Greene
- Justin Guinney
bibliography:
- content/manual-references.json
date-meta: '2021-02-03'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"citation_title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"og:title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"twitter:title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"dc.date\" content=\"2021-02-03\" />\n<meta name=\"citation_publication_date\" content=\"2021-02-03\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Jineta Banerjee\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-1775-3645\" />\n<meta name=\"citation_author\" content=\"Robert J Allaway\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-3573-3565\" />\n<meta name=\"twitter:creator\" content=\"@allawayr\" />\n<meta name=\"citation_author\" content=\"Jaclyn N Taroni\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-4734-4508\" />\n<meta name=\"citation_author\" content=\"Casey Greene\" />\n<meta name=\"citation_author_institution\" content=\"Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-8713-9213\" />\n<meta name=\"citation_author\" content=\"Justin Guinney\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-1477-1888\" />\n<link rel=\"canonical\" href=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"og:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"twitter:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_pdf_url\" content=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://jaybee84.github.io/ml-in-rd/v/4c1396e7b2456a2df64c43d1e76141f8e616ee69/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/4c1396e7b2456a2df64c43d1e76141f8e616ee69/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/4c1396e7b2456a2df64c43d1e76141f8e616ee69/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- rare disease
- machine learning
- transfer learning
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Machine learning methods for rare diseases
...






<small><em>
This manuscript
([permalink](https://jaybee84.github.io/ml-in-rd/v/4c1396e7b2456a2df64c43d1e76141f8e616ee69/))
was automatically generated
from [jaybee84/ml-in-rd@4c1396e](https://github.com/jaybee84/ml-in-rd/tree/4c1396e7b2456a2df64c43d1e76141f8e616ee69)
on February 3, 2021.
</em></small>

## Authors



+ **Jineta Banerjee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1775-3645](https://orcid.org/0000-0002-1775-3645)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaybee84](https://github.com/jaybee84)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Robert J Allaway**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3573-3565](https://orcid.org/0000-0003-3573-3565)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [allaway](https://github.com/allaway)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [allawayr](https://twitter.com/allawayr)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Jaclyn N Taroni**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4734-4508](https://orcid.org/0000-0003-4734-4508)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaclyn-taroni](https://github.com/jaclyn-taroni)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Casey Greene**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania; Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Justin Guinney**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1477-1888](https://orcid.org/0000-0003-1477-1888)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jguinney](https://github.com/jguinney)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>



## Synopsis {.page_break_before}

(Instructions: Describe the background, basic structure of the article, list material to be covered indicating depth of coverage, how they are logically arranged, include recent pubs in the area, 300-500 words)

Substantial technological advances have accelerated basic research and made deep characterization of patient samples routine [TODO: what do we mean by "technological advances" specifically?].
These technologies provide a rich portrait of genes, cellular pathways, and cell types involved in complex phenotypes.
Machine learning is often a perfect fit for extracting disease-relevant patterns from these high dimensional datasets.
Often, machine learning methods require many samples to identify reproducible and biologically meaningful patterns.
With rare diseases, biological specimens, and consequently data, are limited due to the rarity of the condition.
In this perspective, we outline the challenges and emerging solutions for using machine learning in rare disease settings.
We aim to spur the development of powerful machine learning techniques for rare diseases.
We also note that precision medicine presents a similar challenge, in which a common disease is partitioned into small subsets of patients with shared etiologies and treatment strategies.
Advances from rare disease research are likely to be highly informative for other applications as well.


## Introduction {.page_break_before}

Rare disease research is increasingly dependent on high-throughput profiling of samples and would greatly benefit from machine learning (ML) analytics. 
A systematic review of application of ML in rare disease in the last 10 years uncovered 211 human data studies in 74 different rare diseases employing ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) [@doi:10.1186/s13023-020-01424-6]. 
While the review points to the increasing popularity of using ML methods in rare disease, there are various hurdles that are inherent to such datasets.
ML based methods benefit from using large datasets, but analyzing high dimensional data from rare diseases datasets that typically contain 20 to 99 samples is challenging [@https://www.fda.gov/media/99546/download; @doi:10.1186/s13023-020-01424-6].
Small datasets lead to a lack of statistical power and magnify the susceptibility of ML methods to misinterpretation and unstable performance.
Additionally, successful training of ML models require training datasets made of “gold standard” data where the diagnosis or label of a data point has very little uncertainty (or “label-noise”) associated with it [@doi:10.1093/jamia/ocw028]. 
Due to limited understanding of the biology of rare diseases, the symptoms or disease labels often come with significant label-noise (a _silver standard_ dataset) [@doi:10.1109/tnnls.2013.2292894]. [TODO: we use "label-noise" and "high label-uncertainty" somewhat interchangeably, or drop "label-noise" altogether when we get to the model complexity section]
Thus, specialized computational methods that can learn patterns from small datasets and can generalize to newly acquired data are required for rare disease applications [@doi:10.1016/j.ebiom.2019.08.027]. 
In this perspective, we first highlight ML approaches that address or better tolerate the limitations of rare disease data, and then discuss the future of ML applications in rare disease.


### Manage complex high-dimensional rare disease data

In rare diseases, the high throughput ‘omic’ methods generate highly dimensional data – data with many features such as all of the mRNA transcripts in a sample – from a vanishingly small number of samples.
A lack of samples gives rise to the “curse of dimensionality” (i.e., few samples but many features), which is an impediment in analyzing feature-rich data in sample-deficient contexts such as rare disease.[@doi:10.1038/nrc2294] 
In particular, increased numbers of features results in increased sparsity (missing observations), more dissimilarity between samples, and increased redundancy between individual features or combinations of features [@doi:10.1038/s41592-018-0019-x], which creates a challenging prediction problem. 
Furthermore, rare disease data collection and aggregation methods can add to these challenges by introducing technical variability into the data at hand.
In this section, we will discuss strategies for reducing the feature space and addressing technical artifacts through dimensionality reduction.

Dimensionality reduction methods like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) can help ‘compress’ information from a large number of features into a smaller number of features  in an unsupervised manner [@doi:10.1007/978-3-540-33037-0; @doi:10.1098/rsta.2015.0202, @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @https://arxiv.org/abs/1802.03426] . 
These methods not only help in reducing the number of features in various types of data [@doi:10.1016/j.media.2020.101660; @doi:10.1038/ncomms14825], but can also be used to visualize structure or artifacts in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), define sample subgroups (e.g. [@doi:10.1038/s41467-020-15351-4], or for feature selection or extraction during application of specific machine learning models.[@doi:10.1007/978-3-030-03243-2_299-1] (Figure {@fig:1})

Rare disease datasets are often combined from multiple small studies leading to the confounding of biological characteristics with technical variables such as batch, sample preparation methodology, or sequencing platform [@doi:10.23915/distill.00002]. 
Methods like PCA, MDS, t-SNE, and UMAP can successfully identify the effect of these variables on the original data, though t-SNE and UMAP may require tuning of hyperparameters that may effect the output [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002].
Furthermore, testing multiple dimensionality reduction methods, rather than a single method, may be desirable to obtain a more comprehensive portrait of the data [@doi:10.1186/s13059-020-02021-3]. 
Nguyen and Holmes discuss additional important considerations for using dimensionality reduction methods such as criteria for selecting a dimensionality reduction method and interpretation of results in detail [@doi:10.1371/journal.pcbi.1006907].
Beyond dimensionality reduction, unsupervised learning approaches such as k-means clustering or hierarchical clustering have been used to characterize the structure present in genomic or imaging data [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766].
Other approaches like reprocessing the raw data using a single analysis pipeline (if the data are obtained from different sources), application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values [@doi:10.1186/gb-2010-11-3-r25] may be required to obtain value from these datasets. 

Dimensionality reduction, or more fundamentally representation learning, learns low-dimensional representations or composites of features from the raw data. 
For example, representation learning through matrix factorization can extract composite features from transcriptomics datasets that are made of combinations of gene expression levels found in the training data[@doi:10.1038/s41467-020-14666-6], and then use them to interpret test input data [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
To ensure that the learned representations are generalizable, the features learned by the model can be constrained through methods like regularization [@doi:10.1371/journal.pgen.1004754, @doi:10.1002/sim.6782]. 
Representation learning generally requires many samples in complex biological systems and therefore may appear to aggravate the curse of dimensionality. 
However, it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In the later sections of this perspective, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer. 

![Dimension reduction can help manage the curse of dimensionality in rare disease data](https://github.com/jaybee84/ml-in-rd/blob/draft-branch/content/images/figures/pdfs/dimensionality-reduction.pdf){#fig:1}


### Manage model complexity while preserving the value of machine learning

Translating machine learning findings into testable hypotheses requires the applied models to be a) stable – the same predicted features should surface from the data if the model is run multiple times – and b) simple, as simple models guard against misinterpretation. 
Meeting these requirements is challenging in rare disease datasets where there is high label-uncertainty. 
In this section we highlight a few common ML techniques that can help improve the stability and simplicity of ML models applied to rare disease data.

Techniques like resampling, as well as by combining various ML methods together (ensemble learning) can help achieve stability in predictions (Figure[@fig:2]A-B). 
Resampling without replacement can generate confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets, mimicking real world cases where most rare disease datasets are incomplete [@doi:10.3390/genes11020226].
Resampling with replacement, or bootstrapping, helps estimate population values from datasets of limited size, and is also commonly used to find robust models when multiple models are combined into an ensemble ([@doi:10.1080/01621459.1997.10474007; @doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277; @doi:10.1016/s0925-2312(01)00650-6; @doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
Ensemble learning methods like random forests use _bagging_ (bootstrap aggregation) of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@doi:10.1023/A:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10.1016/s0031-3203(02)00169-3]. 
Regular ensemble learning has shown limited success in rare disease datasets with substantial label-noise. [TODO: citation?] 
This has led to the adoption of cascade learning, a variant of ensemble learning, where multiple methods leveraging distinct underlying assumptions are used in tandem; and augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in the silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
A cascade learning approach for identifying rare disease patients from electronic health records from the general population utilized independent steps for feature extraction (using natural language processing based word2vec [@https://arxiv.org/abs/1301.3781v343]), preliminary prediction using an ensemble of decision trees, and prediction refinement using similarity of data points to resolve sample labels [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets further decreases the available data points per class. 
In such cases, a one-class-at-a-time cascade learning approach (where at each stage a binary classifier predicts a specific class against all others) has been found to produce simpler models that perform better compared to multi-class ensemble classifiers [@doi:10.1093/jamia/ocy109]. (Figure[@fig:2]D)

Regularization simplifies models by making the feature space proportionate with the sample space. (Figure[@fig:2]C)
Regularization can not only protect ML models from poor generalizability that results from overfitting (where the model performs well for the training data but poorly for new test data) [@doi:10.1073/pnas.1900654116], but also help penalize model complexity and reduce the feature space to build simpler models using limited datasets. 
Among the three popular regularized methods, ridge regression can minimize the magnitude of the features, but cannot remove unimportant features. 
LASSO regression, on the other hand, works well for selecting few important features since it can minimize the magnitude of some features more than the others [@doi:10.1038/nmeth.4014]. 
A combination of LASSO and ridge, elastic-net regression [@doi:10.1111/j.1467-9868.2005.00503.x] selects the most useful features, especially in presence of a large number of correlated features.

Rare variant discovery and immune cell signature discovery studies, like rare diseases, face challenges of the sparsity of observations (e.g. rare variants, or rare immune cells).
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants or low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO in rare variant discovery studies like capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113], integrating with a probabilistic logistic Bayesian approach [@doi:10.4137/cin.s17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250] have proven beneficial.
On the other hand, in immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1186/s12859-019-2994-z]. 
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space; these methods should be explored while working with rare disease datasets. 

Thus by employing bootstrapping, ensemble learning, and regularization methods, researchers may be able to better generate stable, simple models that identify reliable biological phenomena underlying rare diseases .

![Strategies to simplify models and stabilize predictions preserve the value of machine learning in rare disease. A-B) Strategies to build confidence in model predictions; A) schematic showing the concept of bootstrap, B) schematic showing the concept of ensemble learning to converge on reliable models; C-D) Strategies to simplify models by penalizing complexity in ML models; C) schematic showing the concept of regularization to selectively learn relevant features, D)schematic showing the concept of one-class-at-a-time learning to select few features at a time. Horizontal bars represent health of a model, models are represented as a network of nodes (features) and edges (relationships), nodes with solid edges represent real patterns, nodes with broken edges represent spurious patterns](https://github.com/jaybee84/ml-in-rd/blob/draft-branch/content/images/figures/pdfs/statistical-techniques.pdf){#fig:2}


### Build upon prior knowledge and indirectly related data {.page_break_before}

Rare diseases often lack large, normalized datasets, limiting our ability to study key attributes of these diseases. 
Evaluating genotype-phenotype relationships or repurposing drugs using knowledge graphs can greatly benefit rare disease. 
Knowledge graphs (KGs) integrate related-but-different data types, creating a rich data source (e.g. Monarch Graph Database[@doi:10.1093/nar/gkw1128], hetionet[@doi:10.7554/elife.26726], PheKnowLator[@doi:10.1101/2020.04.30.071407], and the Global Network of Biomedical Relationships[@doi:10.1093/bioinformatics/bty114], Orphanet[@http://www.orpha.net]). 
These graphs connect genetic, functional, chemical, clinical, and ontological data to enable the exploration of relationships of data with disease phenotypes through manual review[@doi:10.1093/database/baaa015] or computational methods[@doi:10.1101/727925; @doi:10.1186/s12911-019-0938-1].(Figure[@fig:3]a)
KGs may include links or nodes that are specific to the rare disease of interest (e.g., an FDA approved treatment  would be a specific disease-compound link in the KG ) as well as links that are more generalized (e.g., gene-gene interactions noted in the literature for a different disease). 

Rare disease researchers can leverage the entities and relationships in a knowledge graph outside of the specific disease-context [@doi:10.1101/727925]. 
Such approaches have been used in rare disease research in areas such as drug repurposing[@doi:10.1101/727925] and disease classification[@doi:10.1186/s12911-019-0938-1]. 
Identifying KG encoding methods that can provide actionable insights for a specific rare disease application is an active area of research. 

Other approaches that build upon prior knowledge and large volumes of related data include transfer learning, multitask learning, and few-shot learning approaches. 
These approaches leverage shared features, e.g., normal developmental processes that are aberrant in disease or an imaging anomaly present in rare and common diseases, for advancing our understanding of rare diseases. 
Transfer learning, where a model trained for one task or domain (source domain) is applied to another related task or domain (target domain), can be supervised or unsupervised. 
Among various types of transfer learning, feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain [@doi:10.1109/tkde.2009.191] (Figure[@fig:3]b).
For example, low-dimensional representations can be learned from tumor transcriptomic data and transferred to describe patterns associated with genetic alterations in cell line data [@doi:10.1186/s13059-020-02021-3].
Alternatively, multitask and few-shot learning are forms of supervised learning that often rely on deep neural networks. 

While multitask learning classifiers use shared representations to learn multiple related but individual predictions (tasks) simultaneously [@doi:10.1023/a:1007379606734], few-shot learning generalizes a model trained on related tasks to a new task with limited labeled data (e.g., the detection of a patient with a rare disease from a low number of examples of that rare disease) [@https://arxiv.org/abs/1706.05098; @https://arxiv.org/abs/1707.08114v2; @https://arxiv.org/abs/1904.05046v3] (Figure[@fig3]c-d).
Smaller datasets tended to benefit from multitask learning (due to task relatedness, _multitask effect_) [@https://arxiv.org/abs/1606.08793], and the performance gains were generally context-dependent, i.e., multitask neural networks outperformed single-task networks for predicting complex rare phenotypes from EHR data or predicting drug sensitivity in rare cancer cell lines [@https://arxiv.org/abs/1808.03331; TODO: CTD-squared Chemogenomic DREAM Challenge citation]. 
In contrast, one-shot or few-shot learning used prior knowledge to generalize a distance metric learned from input data to compare with a low number of new examples for prediction [@https://arxiv.org/abs/1904.05046v3, @doi:10.1021/acscentsci.6b00367; @doi:10.1021/acscentsci.6b00367]. 
In another study, a few-shot learning approach had a performance advantage over multitask learning, since predicting common conditions simultaneously resulted in a loss of performance for the multitask learner [@doi:10.1016/j.media.2020.101660]. 
Thus, transfer, multi-task, and few-shot learning are appealing approaches for rare disease applications, but their limits and potential utility are still open research questions. 

![Strategies that build upon prior knowledge help ML models learn patterns in rare disease datasets. A) Knowledge graphs integrate different data types and may allow models to learn from connections that are rare disease-specific or happen in many biomedical contexts. B) Transfer learning is when a model trained in for one task or domain is applied to another, related task. C) Multitask learning uses models that learn and leverage shared representations to predict multiple, related tasks. D) Few-shot learning generalizes a previously trained model to predict a new,  related task with a limited number of samples.](https://github.com/jaybee84/ml-in-rd/blob/draft-branch/content/images/figures/pdfs/prior-knowledge.pdf){#fig:3}


### Using composite approaches can be a powerful strategy

We have described multiple approaches for maximizing the success of ML applications in rare disease, but it is rarely sufficient to use any of these techniques in isolation. 
Below, we highlight two recent works in the rare disease domain that draw on concepts of feature-representation-transfer, use of prior data, and regularization.

A large public dataset of acute myeloid leukemia (AML) patient samples with no drug response data and a small _in vitro_ experiment with drug response data form the basis of our first example [@doi:10.1038/s41467-017-02465-5].
Training an ML model on the small _in vitro_ dataset alone faced the _curse of dimensionality_ and the dataset size prohibited representation learning.
Dincer et al. trained a variational autoencoder on the large AML patient dataset (VAE; see [definitions]) to learn meaningful representations in an approach termed DeepProfile [@https://www.biorxiv.org/content/10.1101/278739v2] (Figure[@fig:4]a).
The representations or _encodings_ learned by the VAE were then _transferred_ to the small _in vitro_ dataset reducing it's number of features from thousands to eight, and improving the performance of the final LASSO linear regression model.
In addition to improvement in performance, the _encodings_ learned by the VAE captured more biological pathways than PCA, which may be attributable to the constraints on the encodings imposed during the training process [definitions].
Similar results were observed for prediction of histopathology in another rare cancer dataset [@https://www.biorxiv.org/content/10.1101/278739v2].

While DeepProfile was centered on training on an individual disease and tissue combination, some rare diseases affect multiple tissues that a researcher may be interested in studying together for the purpose of biological discovery. 
Studying multiple tissues poses significant challenges and a cross-tissue analysis may require an analyst to compare representations from multiple models.
Models trained on a low number of samples may learn representations that "lump together" multiple biological signals, reducing the interpretability of the results.
To address these challenges, Taroni et al. trained a Pathway-Level Information ExtractoR (PLIER) (a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways) on a large generic collection of human transcriptomic data [@doi:10.1038/s41592-019-0456-1]. 
PLIER used constraints (regularization) that learned _latent variables_ aligned with a small number of input gene sets, making it suitable for biological discovery or description of rare disease data. 
The authors _transferred_ the representations or _latent variables_ learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003]. (Figure[@fig:4]b)
MultiPLIER used one model to describe multiple datasets instead of reconciling output from multiple models, thus making it possible to identify commonalities among disease manifestations or affected tissues. 

Taken together, DeepProfile [@doi:10.1101/278739v2] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] suggest a combination of the techniques discussed throughout this article can be capitalized on for rare disease research. 
In cases where we have a few samples from our disease of interest with the required phenotypic labels, we can leverage prior knowledge if we select the models with the right attributes. 
DeepProfile and MultiPLIER capitalizes on the fact that biological processes can be shared between biological contexts and that the methods underlying the approaches can effectively learn about those processes. 

![Combining multiple strategies strengthens the performance of ML models in rare disease. A) The authors of DeepProfile trained a variational autoencoder (VAE) to learn a representation from acute myeloid leukemia data without phenotype labels, transferred those representations to a small dataset with phenotype labels, and found that it improved prediction performance [@https://www.biorxiv.org/content/10.1101/278739v2]. B) The authors of MultiPLIER trained a Pathway-Level Information ExtractoR (PLIER) model on a large, heterogeneous collection of expression data and transferred the representations to multiple datasets from unseen rare diseases [@doi:10.1038/s41592-019-0456-1].](https://github.com/jaybee84/ml-in-rd/blob/draft-branch/content/images/figures/pdfs/multiplier-DeepProfile.pdf){#fig:4}


### Outlook

Throughout this perspective, we have highlighted various challenges in applying ML methods to rare disease data as well as examples of approaches that address these challenges.
Small sample size, while significant, is not the only roadblock towards application of ML in rare disease data.
The high dimensionality of modern data requires creative approaches, such as learning new representations of the data, to manage the curse of dimensionality.
Leveraging prior knowledge and transfer learning methods to appropriately interpret data is also required.
Anyone applying machine learning methods on rare disease data should use techniques that increase confidence (i.e., bootstrapping) and penalize complexity of the resultant models (i.e., regularization) to enhance the generalizability of their work. 

All of the approaches highlighted in this perspective come with certain challenges or weaknesses that may undermine investigators' confidence in using these powerful techniques for rare disease research.
We believe that the same challenges that are currently considered major pitfalls in applying ML to rare disease can be great opportunities for data generation and method development going forward.
During our journey through the various challenges, we identified two major areas where mindful strategies can immeasurably enhance the power of machine learning in rare disease and move the field forward.

_Emphasis on not just "more n" but "more meaningful n"_

Mindful addition of data is key for powering the next generation of analysis in rare disease data.
While there are many techniques to collate rare data from different sources, incorrect data generation may hurt the end goal even if it adds to the size of the dataset.
In our experience, collaboration with domain experts have proved to be critical in gaining insight into potential sources of variation in the datasets.
As an example, an neurofibromatosis type 1 (NF1) dataset was found to contain samples collected using vastly different surgical techniques (laser ablation and excision vs standard excision). [@doi:10.3390/genes11020226] 
While the integrative analysis in the study using transfer learning techniques was able to minimize technique related signals [@doi:10.1016/j.cels.2019.04.003], a more traditional analysis may have resulted in surfacing of substantial biological differences that are a consequence of process (e.g. activation of heat shock protein related pathways), not disease related biology. 
Such instances underline the fact that continuous collaboration with domain experts is needed to generate robust datasets in the future.
A few such collaborations are beginning to show promise in generating valuable datasets for future use.[@doi:10.1038/s41597-020-0508-5]

In addition to sample scarcity, there is a dearth of comprehensive phenotypic-genotypic databases in rare disease.
With the ubiquity of sequencing platforms, genomic data has been, relatively speaking, easy to gather for rare disease patients.[@doi:10.1038/nrg3555; @doi:10.1038/nrg.2017.116; @doi:10.1056/NEJMra1711801]
An important next step is to develop comprehensive comprehensive genomics-driven genotype-phenotype databases that can fuel interpretation of features extracted using ML methods.
Finally, mindful sharing of data with proper metadata and attribution to enable prompt data reuse is of utmost important in building datasets that can be of great value in rare disease. [@https://www.nature.com/articles/s41576-020-0257-5]

_Development of methods that reliably support mechanistic interrogation of specific rare diseases_

The majority of ML methods for rare disease that we have investigated are applied to classification tasks. 
Conversely, we've found few examples of methodologies that interrogate biological mechanisms of rare diseases. 
This is likely a consequence of a dearth of methods that can tolerate the constraints imposed by rare disease research such as phenotypic heterogeneity and limited data.
An intentional push towards developing methods or analytical workflows that address this will be critical to apply machine learning approaches to rare disease data.

Method development with rare disease applications in mind requires the developers to bear the responsibility of ensuring that the resulting model is _trustworthy_.
The field of natural language processing has a few examples of how this can be achieved.[@https://www.aclweb.org/anthology/N16-3020.pdf, @https://www.aclweb.org/anthology/P19-1073.pdf]
One way to increase trust in a developed model is by helping users understand the behavior of the developed model through providing explanations regarding why a certain model made certain predictions.[@https://www.aclweb.org/anthology/N16-3020.pdf]
Another approach is to provide robust _error analysis_ for newly developed models to help users understand the strengths and weaknesses of a model.[@https://www.aclweb.org/anthology/P19-1073.pdf; @https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00072; @doi:10.1093/bioinformatics/bth060]
Adoption of these kind of approaches into biological data and analysis is still limited but is quickly becoming necessary as machine learning approaches become mainstream in biomedicine.

Finally, methods that can reliably integrate disparate datasets will always remain a need in rare disease research.
Moreover, combining data that originated from diverse modalities to create a complete picture of the disease related biology is increasingly becoming common.
To facilitate such analyses in rare disease, methods that rely on finding structural correspondences between datasets ("anchors") may be able to transform the status-quo of using machine learning methods in rare disease.[@https://www.aclweb.org/anthology/W06-1615.pdf; @https://people.cs.umass.edu/~mahadeva/papers/IJCAI2011-DA.pdf; https://www.cell.com/cell/fulltext/S0092-8674(19)30559-8]
Overall, we speculate that this an important and burgeoning area of research, and we are optimistic about the future of applying machine learning approaches to rare diseases.


## Definitions {.page_break_before}

### Unsupervised learning: 
Machine learning algorithms which can learn features from unlabeled training data (e.g. datasets where the samples do not have disease or phenotype labels) to predict the class or phenotype of new or unseen test data are part of unsupervised learning. Examples of unsupervised learning include principal component analyses, multidimensional scaling, UMAP, t-SNE, k-means clustering etc [TODO - add REFs].

### Supervised learning: 
Machine learning algorithms that require training data with specific phenotype labels are part of supervised learning. 
Such algorithms learn correlations of features with the phenotype labels and use the learned correlations to predict the phenotype labels of unseen or new test data.

### VAE: 
Variational Autoencoders or VAEs are unsupervised neural networks that use hidden layers to learn or encode representations from available data while mapping the input data to the output data. 
VAEs are distinct from other autoencoders since the distribution of the encodings are regularized such that they are close to a normal distribution, which may contribute to learning more biologically relevant signals [@doi:10.1186/s13059-020-02021-3].


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
