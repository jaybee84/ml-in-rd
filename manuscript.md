---
author-meta:
- Jineta Banerjee
- Robert J Allaway
- Jaclyn N Taroni
- Casey Greene
- Justin Guinney
bibliography:
- content/manual-references.json
date-meta: '2020-11-02'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"citation_title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"og:title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"twitter:title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"dc.date\" content=\"2020-11-02\" />\n<meta name=\"citation_publication_date\" content=\"2020-11-02\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Jineta Banerjee\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-1775-3645\" />\n<meta name=\"citation_author\" content=\"Robert J Allaway\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-3573-3565\" />\n<meta name=\"twitter:creator\" content=\"@allawayr\" />\n<meta name=\"citation_author\" content=\"Jaclyn N Taroni\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-4734-4508\" />\n<meta name=\"citation_author\" content=\"Casey Greene\" />\n<meta name=\"citation_author_institution\" content=\"Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-8713-9213\" />\n<meta name=\"citation_author\" content=\"Justin Guinney\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-1477-1888\" />\n<link rel=\"canonical\" href=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"og:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"twitter:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_pdf_url\" content=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://jaybee84.github.io/ml-in-rd/v/d07c85fbeb595560a7c2f42b2ff4184cb21e7f65/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/d07c85fbeb595560a7c2f42b2ff4184cb21e7f65/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/d07c85fbeb595560a7c2f42b2ff4184cb21e7f65/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- rare disease
- machine learning
- transfer learning
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Machine learning methods for rare diseases
...






<small><em>
This manuscript
([permalink](https://jaybee84.github.io/ml-in-rd/v/d07c85fbeb595560a7c2f42b2ff4184cb21e7f65/))
was automatically generated
from [jaybee84/ml-in-rd@d07c85f](https://github.com/jaybee84/ml-in-rd/tree/d07c85fbeb595560a7c2f42b2ff4184cb21e7f65)
on November 2, 2020.
</em></small>

## Authors



+ **Jineta Banerjee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1775-3645](https://orcid.org/0000-0002-1775-3645)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaybee84](https://github.com/jaybee84)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Robert J Allaway**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3573-3565](https://orcid.org/0000-0003-3573-3565)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [allaway](https://github.com/allaway)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [allawayr](https://twitter.com/allawayr)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Jaclyn N Taroni**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4734-4508](https://orcid.org/0000-0003-4734-4508)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaclyn-taroni](https://github.com/jaclyn-taroni)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Casey Greene**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania; Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Justin Guinney**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1477-1888](https://orcid.org/0000-0003-1477-1888)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jguinney](https://github.com/jguinney)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>



## Synopsis {.page_break_before}

(Instructions: Describe the background, basic structure of the article, list material to be covered indicating depth of coverage, how they are logically arranged, include recent pubs in the area, 300-500 words)

Substantial technological advances have dramatically changed biomedicine by making deep characterization of patient samples routine.
<!-- TODO: What about basic research? We probably need to address that somewhere too. -->
These technologies provide a rich portrait of genes, cellular pathways, and cell types involved in complex phenotypes.
Machine learning is often a perfect fit to extract disease-relevant patterns from these high dimensional datasets.
Often, these methods require many samples to identify reproducible and biologically meaningful patterns.
With rare diseases, biological specimens and consequently data, are limited due to the rarity of the condition.
In this perspective, we outline the challenges and emerging solutions for using machine learning in these settings.
We aim to spur the development of powerful machine learning techniques for rare diseases.
We also note that precision medicine presents a similar challenge, in which a common disease is partitioned into small subsets of patients with shared etiologies and treatment strategies.
Advances from rare disease research are likely to be highly informative for other applications as well.


## Introduction {.page_break_before}

<!--TODO: There are no refs in the first paragraph. -->
Data collection is increasingly high-throughput, and it can now be more feasible to perform genome-wide profiling than targeted profiling in some settings.
Machine learning (ML) is gaining momentum in biomedical data analysis as a means of analyzing and identifying relevant factors from broad profiling experiments.
Application of ML to any dataset requires careful execution, but the application to biomedical data and subsequent interpretation requires depth of knowledge not only in the biomedical domain but also a clear understanding of the methods and their underlying assumptions.
Application of ML to any kind of data consists of the following major steps: (1) data evaluation and question formulation, (2) selection of normalization/dimension reduction to mitigate technical differences, (3) selection of appropriate algorithms which select features to answer the formulated question, (4) evaluation of the answers generated by the algorithm.
Each of these steps require the practitioner to choose from a variety of methodologies to apply.
The selection of the methodologies at each of these steps need to be based upon robust reasoning to ensure stability of the results.

A promising yet challenging application of machine learning is in the study of rare diseases - those with fewer than 200,000 cases in the United States [@https://www.fda.gov/media/99546/download].
Rare disease research has substantial constraints to consider when using ML methods, including lack of statistical power in dataset size, heterogeneity in available data, and sensitivity of ML methods to misinterpretation in view of small datasets.
For example, successful training of ML models require training datasets made of "gold standard" data where the diagnosis or label of a data point has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028].
In rare disease the symptoms as well as any underlying biology often come with a reasonable amount label-noise leading to a _silver standard_ dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
Moreover, in the context of rare disease, special considerations need to be made to safeguard against misinterpretation of results.
Rare disease datasets are often limited in size and/or constructed at multiple institutions from different types of specimens.
Such considerations include incorporation of methods that can mitigate technical disparities in the data and that are resilient to challenges posed by small datasets.
Often, this will require techniques that build upon prior domain-specific knowledge and data.
In this perspective, we discuss techniques for understanding the nature of rare disease data, including those that address or better tolerate the limitations of these data.


### Manage complex high-dimensional rare disease data

The great irony of studying rare diseases with genomics, transcriptomics, or other similar methods that the ability to get an enormous number of measurements from a vanishingly small number of samples, is both the upside and the downfall of these methods.
These 'omic' methods generate highly dimensional data - that is, data with many features - such as all of the mRNA transcripts in a sample. 
However, to make sense of these many features (in other words, to be able to statistically interrogate these measurements), we must have many samples, or observations, which is often not the case in rare disease. 
This is known as the "curse of dimensionality," and can be a major impediment in analyzing feature-rich data in sample-deficient contexts [@doi:10.1038/nrc2294].
Additionally, simply untangling the many correlated features and interpreting how they relate to the biological question at hand can make the use of highly dimensional rare disease data for discovery daunting. 
Furthermore, rare disease data collection and aggregation methods can further complicate these challenges by introducing technical variability into the data at hand. 
In this section, we'll discuss strategies to consider that can help mitigate these challenges such as simplifying data by reducing the dimensionality of high-dimensional datasets as well as detecting and correcting technical artifacts. 


#### Dimensionality reduction and representation learning

Dimensionality reduction simply describes any method that 'compresses' information from many features into a smaller number of features.
Methods such as multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) - which are, themselves, unsupervised machine learning methods - can be used to compress data from many dimensions to a few. [@doi:10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202; @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @arXiv:1802.03426]
Of note, these techniques are not restricted to 'omic' data - they can also be used in rare disease applications to characterize imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], and others.
The output of these can then be used to visualize structure in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), used to define subgroups within data (e.g. [@doi:10.1038/s41467-020-15351-4], or can be used for feature selection/extraction during machine learning[@doi:10.1007/978-3-030-03243-2_299-1]. 

In fact, dimensionality reduction is the core concept underpinning a class of ML called representation learning (or feature learning).
Representation learning is the process of learning features from raw data, where a feature is an individual variable.
An algorithm or approach will construct features as part of training and, in a supervised application, use those features to predict labels on input data.
Using an example from transcriptomics, an unsupervised method such as matrix factorization can be used to extract a low-dimensional representation of the gene-level data, learning features that are a combination of input genes' expression levels [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can then be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8].
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1098/rsif.2017.0387]), which are trained to predict labels or classes, are also an example of representation learning.
Learned features in the medical imaging domain may be a series of edges representing a blood vessel formation that discriminates between disease states.
Features learned from transcriptomic data could be coordinated sets of genes involved in a biological process that are descriptive in some way [@doi:10.1038/s41467-020-14666-6].
Representation learning tends to be data-intensive (i.e. many samples are required) and thus may seem to aggravate the curse of dimensionality.
But representation learning when applied to learn low dimensional patterns from large datasets and then applying those patterns to smaller but related datasets can be a powerful tool for dimensionality reduction.
In the later sections of this perspective, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as _feature-representation-transfer_ .
In later sections, we will introduce methods that can leverage data that do not directly assay a rare disease of interest; representation learning underlies many of time.

#### Visualization and correction of technical effects

Another application of dimensionality reduction methods is to assess the presence or absence of unwanted signal in the data. 
There is sometimes structure in our data that is not related to what we want to study; for example, structure related to batch, sample preparation methodology, or sequencing platform. 
Structure introduced by these variables can obscure the biologically-important information that we seek. 
However, we can leverage dimensionality reduction methods such as PCA, MDS, t-SNE, and UMAP to identify the effect of these variables on the data. 
All of these methods can be used to identify batch effects and other structure in the data, though some (like t-SNE and UMAP) require parameters that can affect the output [@doi:10.23915/distill.00002;@arXiv:1802.03426].
Therefore, obtaining a clear interpretation from these methods requires understanding the underlying approach and parameters.
Another important consideration is discussed by Way, et. al. [@doi:10.1186/s13059-020-02021-3]: a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; testing multiple methods may result in a more comprehensive portrait of the data.

Rare disease data often suffers from artifacts introduced by non-biological phenomena such as batch or assay platform [@doi:10.1016/j.cels.2019.04.003, @doi:10.1186/s13023-020-01376-x, @doi:10.1038/s41591-019-0457-8].
The consequences of these artifacts are amplified when there are few samples and heterogeneous phenotypes.
Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables.
Collaboration with domain experts may result in unexpected insight into potential sources of variation.
As an example, consider a study of neurofibromatosis type 1 (NF1) datasets.[@doi:10.3390/genes11020226]
These datasets were, unbeknownst to the computational biologists, generated from samples obtained with vastly different surgical techniques (laser ablation and excision vs standard excision), resulting in substantial biological differences that are a consequence of process, not reality. One might expect, in this example, that this technical decision would result in profound changes in the underlying biology, such as the activation of heat shock protein related pathways, unfolded protein responses, and so on. 
Consequently, careful assessment of and accounting for confounding factors is critical to identifying meaningful features within a dataset.

Assessment of confounding factors and heterogeneity is perhaps most easily performed using unsupervised learning approaches.
K-means clustering or hierarchical clustering can be used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497;@doi:10.1109/JBHI.2013.2276766] <!-- TODO: Make reference to the dimensionality reduction section above -->

Once the nature of the non-biological heterogeneity has been established, different techniques can be used to correct the differences.
Common approaches include reprocessing the raw data using a single analysis pipeline if the data are obtained from different sources, application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25].
It is also important to be realistic when working with rare disease data.
For various reasons including ethical constraints, funding, and limited biospecimens, experimental design and the resulting data will often be less-than-ideal.
In these cases, it may be prudent to take a step back, re-evaluate the data, and identify methods that can operate within the constraints of the data, rather than expecting the data to conform to a method of choice.


### Manage model complexity without sacrificing the value of machine learning

Machine learning is a useful tool to capture complex patterns that underlie a dataset.
However, for fruitful translation of patterns extracted using machine learning into testable hypotheses, there are a few pre-requisites: the models need to be a) stable i.e. the same predicted features should surface from the data if the model is run multiple times and, b) simple to improve interpretability and avoid misinterpretation due to technical artifacts.
Fulfilling these pre-requisites become even more important in case of rare disease dataset where there is high label-uncertainty (i.e. where the label given to a data point may not be correct due to imperfect understanding of the disease).
In this section we highlight few techniques which can help improve the stability and simplicity of ML models.

Techniques like bootstrapping and ensemble learning can increase stability in machine learning predictions.

#### Bootstrapping

Bootstrapping is a powerful statistical technique where resampling the data with replacements can help estimate population values from datasets of limited sample size [@doi:10.1080/01621459.1997.10474007].
Such resampling with replacement is used in various learning methods to find the most informative models (e.g. bootstrap aggregating or _bagging_ used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).
A variation where resampling of a rare disease dataset was done _without replacement_, generated confidence intervals for the model predictions as they were exposed to incomplete datasets (mimicking the real life case where most rare disease datasets are incomplete) [@doi:10.3390/genes11020226].

#### Ensemble learning

Stability in predictions can also be achieved by combining various machine learning methods together (_ensemble learning_).
Ensemble learning methods like random forests use bagging of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@doi:10.1186/1472-6947-13-134; @doi:10.1023/A:1010933404324; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549; @doi:10/btzfh6].
But such methods have shown limited success in rare disease datasets where the label-uncertainty can be high due to imperfect understanding of the disease (i.e.silver standard datasets).
This has led to the adoption of cascade learning, where multiple methods leveraging distinct underlying assumptions are used in tandem. 
The methods are augmented with algorithms like AdaBoost (_boosting_) to capture stable patterns existing in the silver standard data [@doi:10.1109/CVPR.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/ICPR.2004.1334680].
A variation of cascade learning implemented to identify rare disease patients from electronic health records from the general population utilized independent steps for feature extraction (using word2vec [@arXiv:1301.3781v3]), preliminary prediction (ensemble of decision trees with penalization for excessive tree-depth), and prediction refinement (using similarity of data points to resolve sample labels) [@pmid:30815073].
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.

Techniques like regularization and binary predictions can help simplify models by making the feature space proportionate with the sample space.

#### Regularization

Regularization can not only protect ML models from _overfitting_ (where the model performs well for the training data but poorly for new test data) [@doi:10.1073/pnas.1900654116], but also help reduce the feature space to help build simpler models using limited datasets.
The three main methods of regularization include ridge regression, LASSO, and elastic-net.
While ridge regression can minimize the magnitude of the features, it cannot remove unimportant features.
LASSO regression, on the other hand, works well for selecting few important features since it can minimize the magnitude of some features more than the others[@doi:10.1038/nmeth.4014].
A combination of LASSO and ridge, elastic-net regression[@doi:10.1111/j.1467-9868.2005.00503.x] efficiently selects the most useful features, especially in presence of large number of correlated features.

While regularization has not been used extensively in rare disease yet, examples in rare variant discovery and immune cell signature discovery can provide insights into their possible application in rare disease.
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448].
Hybrid applications of LASSO have also been tested in rare variant discovery, including boosting the signal of rare variants by capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-S9-S113], integration with a probabilistic logistic Bayesian approach [@doi:10.4137/CIN.S17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250].
In immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198; @doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1186/s12859-019-2994-z].
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space; these methods should be explored while working with rare disease datasets.

#### One-class-at-a-time classification

In rare diseases like neurofibromatosis, the presence of more than one phenotype (or class) further decreases the number of data-points per class and introduces additional label-uncertainty due to related phenotypes.
In datasets with multiple classes, the classical ensemble or cascade classifiers approach follows a _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But instances where the need for high prediction accuracy for one class outweighs other classes, modification of the cascade learning method into a _one-class-at-a-time_ approach (where at each stage a binary classifier predicts a specific class against all others) has proved to be beneficial [@pmid:30380082].
In this instance, the final model implemented all models together each identifying one class sequentially and then reporting the union of the predictions of all the different models as the final prediction.
The cascade classifiers using the one-class-at-a-time approach were found to perform better than multi-class ensemble classifiers in most cases.

By employing bootstrapping, ensemble learning, and regularization methods, researchers may be able to better generate stable, simple models that identify reliable biological phenomena in rare diseases.


### Techniques that build on prior knowledge and indirectly related data are necessary for many rare disease applications {.page_break_before}

#### Knowledge graphs
Rare diseases lack large, normalized datasets, limiting our ability to study key attributes of these diseases.
A potentially powerful strategy for evaluating genotype-phenotype relationships or repurposing drugs when large datasets are scarce is to use knowledge graphs.
Knowledge graphs integrate related-but-different data types, creating a rich data source.
Examples of public biomedical knowledge graphs and frameworks that could be useful in rare disease include the Monarch Graph Database[@doi:10.1093/nar/gkw1128], hetionet[@doi:10.7554/eLife.26726], PheKnowLator[@doi:10.1101/2020.04.30.071407], and the Global Network of Biomedical Relationships[@doi:10.1093/bioinformatics/bty114].
These graphs connect information like genetic, functional, chemical, clinical, and ontological data to enable the exploration of relationships of data with disease phenotypes through manual review[@doi:10.1093/database/baaa015] or computational methods[@doi:10.1101/727925; @doi:10.1186/s12911-019-0938-1].

In the academic rare disease space, there a few pioneering examples of ML-based mining of knowledge graphs to repurpose drugs[@doi:10.1101/727925] and classify rare diseases[@doi:10.1186/s12911-019-0938-1].
These studies make it clear that there are challenges in using machine learning based on knowledge graphs in rare disease.
For example, these projects rely on a gold standard dataset to validate the performance of the models; often, there are not robust gold standard datasets available for individual rare diseases.
They also evaluate rare diseases in an unbiased manner, rather than interrogating a specific disease of interest.
Consequently, it is not yet clear how effective these approaches, and knowledge graphs in general, are in studying a specific disease of interest; more work needs to be done to identify methods that can provide actionable insights for a specific rare disease application. <!-- TODO: Is this a point to note that may need to be combined with statistical techniques described earlier? -->

Beyond the aforementioned studies, there are not many examples of studies in the public domain that leverage knowledge graphs to characterize rare diseases.
Private entities (e.g. healx, Boehringer Ingelheim), however, are performing an undisclosed amount of work to create proprietary rare disease knowledge graphs for ML-based drug discovery applications.
The existence of private companies pursuing this idea, as well as the availability of public biomedical knowledge graphs, suggests that this may be a fruitful untapped area of rare disease research in the public arena.
More work needs to be done to assess 1) which graphs and graph features capture the salient information about rare diseases, 2) the utility of ML methods to obtain actionable insights about rare diseases and 3) which problems - like drug discovery, identification of novel rare diseases, or assessment of genotype-phenotype relationships - can be interrogated using ML of knowledge graphs.

<!-- TODO: Is this header level right? I can't tell if this is supposed to be a subsection or its own section. The strategies discussed here might either be considered bringing both statistical + prior knowledge & data together or another part of using prior knowledge + data. I think I favor the former, though it would require putting a brief transfer and multi-task blurb in the prior/related data section. -->

#### Transfer, multitask, and few-shot learning

To realize the potential of machine learning for biological discovery in rare diseases, we often cannot study an individual rare disease alone as samples are limited.
Instead, we can build on prior knowledge and large volumes of data that do not directly assay our disease of interest, but are similar enough to be valuable for discovery.
We can leverage shared features, whether they are normal developmental processes that are aberrant in disease or an imaging anomaly present in rare and common diseases, for advancing our understanding.
Methods that leverage shared features include transfer learning, multitask learning, and few-shot learning approaches.

Transfer learning is an approach where a model trained for one task or domain (source domain) is applied to another, typically related task or domain (target domain).
Transfer learning can be supervised (one or both of the source and target domains have labels), or unsupervised (both domains are unlabeled).
Though there are multiple types of transfer learning, in a later section we will focus in-depth on feature-representation-transfer.
Feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain [@doi:10.1109/TKDE.2009.191].
For example, low-dimensional representations can be learned from tumor transcriptomic data and transferred to describe patterns associated with genetic alterations in cell line data [@doi:10.1186/s13059-020-02021-3].  
<!--TODO: anything need to be added/revised here once the dimensionality reduction section is rewritten to be more general? -->

Whereas transfer learning can be supervised or unsupervised, the related approaches multitask and few-shot learning are forms of supervised learning that generally rely on deep neural networks.
Multitask learning is an approach where classifiers are learned for _related individual predictions_ (tasks) at the same time using a shared representation [@doi:10.1023/A:1007379606734].
Few-shot learning is the generalization of a model trained on related tasks to a new task with limited labeled data (e.g., the detection of a patient with a rare disease from a low number of examples of that rare disease).
Multitask and few-shot learning are comprised of a variety of approaches and architectures that are beyond this scope of this work (see [@arxiv:1706.05098; @arxiv:1707.08114v2] and [@arxiv:1904.05046v3] for an overview).
We include brief descriptions of selected studies to illustrate potential uses and limitations of these approaches below.

Multitask neural networks (which predict multiple tasks simultaneously) are thought to improve performance over single task models by learning a shared representation, effectively being exposed to more training data than single task models [@doi:10.1023/A:1007379606734; @arxiv:1606.08793].
Kearnes, Goldman, and Pande set out to examine the effects of dataset size and task relatedness on multitask learning performance improvements ("multitask effect") in drug discovery–an area that also suffers from insufficient data [@arxiv:1606.08793].
The authors found that the multitask performance gains were highly dataset-specific: smaller datasets tended to benefit most from multitask learning and the addition of more training data did not guarantee improved performance for multitask models.
Ding et al. also demonstrated that performance gains were context-dependent; multitask neural networks outperformed single-task networks for predicting complex rare phenotypes from EHR data, but not common phenotypes [@arxiv:1808.03331].

In contrast, one-shot or few-shot learning relies on using prior knowledge to generalize to new prediction tasks where there are a low number of examples [@arxiv:1904.05046v3], where a distance metric is learned from input data and used to compare new examples for prediction [@doi:10.1021/acscentsci.6b00367].
Altae-Tran et al. developed a method for predicting small molecule activity that learned a meaningful distance metric over the properties of various compounds [@doi:10.1021/acscentsci.6b00367].
The authors' results suggested that when structural similarity could not be exploited, one-shot learning methods underperform relative to baseline random forest models, and that models trained on very different contexts from the target task did not generalize well.
Quellec et al. presented a few-shot learning approach for detecting rare pathologies in fundus photographs that outperformed multitask learning, suggesting that few-shot learning provides an advantage in contexts where predicting common conditions simultaneously results in a loss of performance [@arxiv:1907.09449v3].  

Transfer, multi-task, and few-shot learning are appealing for the study of rare diseases, conditions, or phenotypes, but their limits and potential utility are open research questions.
We expect that the benefit will be highly dependent on the dataset and research question at hand as was the case in the studies outlined above.
In the authors' opinion, selecting an appropriate model for a given task and evaluations that are well-aligned with a research goal are crucial for applying these approaches in rare diseases.


#### Multiple approaches are required 

<!-- aka "Putting it all together" -->

<!-- TODO: This needs to explicitly hit regularization, other data, and prior knowledge points!--> 

<!-- From representation learning -->

In the rare disease domain, Dincer et al. leveraged publicly available acute myeloid leukemia (AML) gene expression data to improve the prediction of _in vitro_ drug responses [@doi:10.1101/278739].
The authors trained a variational autoencoder (an unsupervised neural network that learns a series of representations from data), or VAE, on AML data that had been collected over time without the desired phenotypic information (drug response).
The authors used the learned attributes to encode a low-dimensional representation of held-out AML data with phenotype labels of interest, and used this representation as input to a classifier that predicted _in vitro_ drug response.


Though there were over 6500 AML samples from many different studies used as part of the training set in Dincer et al. [@doi:10.1101/278739], we expect that in other rare diseases considerably fewer samples will be available or may be from different tissues in systemic diseases.
The study by Dincer and colleagues highlights another challenge: samples collected as part of multiple studies may not be associated with the deep phenotypic information that would maximize their scientific value.


<!-- From transfer learning -->

Feature-representation-transfer is embodied in Dincer et al., where features are learned from unlabeled AML data and then used to encode a low-dimensional representation of AML data with in vitro drug response labels [@doi:10.1101/278739]. 
The authors then used this low-dimensional representation as input to predict drug response labels–a supervised example.

In an unsupervised case, Taroni et al. trained Pathway-Level Information ExtractoR (PLIER) [@doi:10.1038/s41592-019-0456-1] on a large generic collection of human transcriptomic data (recount2 [@doi:10.1038/nbt.3838]) and used the latent variables learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003]. 
(Here "unseen" refers to the fact that these diseases were not in the training set.) 
PLIER is a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways and gene expression data as input; some latent variables learned by the model will align with input gene sets [@doi:10.1038/s41592-019-0456-1]. 
Training on larger collections of randomly selected samples produced models that captured a larger proportion of input gene sets and better distinguished closely related signals, which suggests that larger training sets produced models that are more suitable for biological discovery [@doi:10.1016/j.cels.2019.04.003].

Though models trained on generic compendia had appealing properties, we need to also examine the relevance of learned features to the disease under study. 
In Taroni et al., we found that the expression of latent variables that could be matched between the MultiPLIER model and a dataset-specific model were well-correlated, particularly when latent variables were associated with input gene sets [@doi:10.1016/j.cels.2019.04.003]. Despite the absence of AAV from the training set, MultiPLIER was able to learn a latent variable where the genes with the highest contributions encode antigens that the ANCA form against in AAV and with higher expression in more severe disease [@doi:10.1002/art.27398]. 
The utility of this approach stems from the fact that biological processes are often shared between conditions–the same ANCA antigen genes are components of neutrophilic granule development that is likely captured or assayed in the collection of transcriptomic data used for training. 
MultiPLIER has additional attributes that make it practical for studying rare diseases: latent variables that are not associated with input gene sets may capture technical noise separately from biological signal and we can use one model to describe multiple datasets instead of reconciling output from multiple models (see 03.heterogeneity.md).

Taken together, DeepProfile [@doi:10.1101/278739] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] suggest transfer learning can be beneficial for studying rare diseases. 
In the natural images field, researchers have demonstrated that the transferability of features depends on relatedness of tasks [@arxiv:1411.1792]. 
The limits of transfer learning for and the concept of relatedness in high-dimensional biomedical data assaying rare diseases are open research questions. 
In the authors' opinion, selecting an appropriate model for a given task and evaluations that are well-aligned with a research goal are crucial for applying these approaches in rare diseases.


### Conclusions
> We will conclude by discussing the potential of the above-mentioned approaches in rare diseases and other biomedical areas where data is scarce.


### Outlook


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
