---
author-meta:
- Jineta Banerjee
- Robert J Allaway
- Jaclyn N Taroni
- Casey Greene
- Justin Guinney
bibliography:
- content/manual-references.json
date-meta: '2020-08-10'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"citation_title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"og:title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"twitter:title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"dc.date\" content=\"2020-08-10\" />\n<meta name=\"citation_publication_date\" content=\"2020-08-10\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Jineta Banerjee\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-1775-3645\" />\n<meta name=\"citation_author\" content=\"Robert J Allaway\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-3573-3565\" />\n<meta name=\"twitter:creator\" content=\"@allawayr\" />\n<meta name=\"citation_author\" content=\"Jaclyn N Taroni\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-4734-4508\" />\n<meta name=\"citation_author\" content=\"Casey Greene\" />\n<meta name=\"citation_author_institution\" content=\"Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-8713-9213\" />\n<meta name=\"citation_author\" content=\"Justin Guinney\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-1477-1888\" />\n<link rel=\"canonical\" href=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"og:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"twitter:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_pdf_url\" content=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://jaybee84.github.io/ml-in-rd/v/de7a952b2d7c9d43834c68d7d5f032012164c32f/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/de7a952b2d7c9d43834c68d7d5f032012164c32f/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/de7a952b2d7c9d43834c68d7d5f032012164c32f/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- rare disease
- machine learning
- transfer learning
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Machine learning methods for rare diseases
...






<small><em>
This manuscript
([permalink](https://jaybee84.github.io/ml-in-rd/v/de7a952b2d7c9d43834c68d7d5f032012164c32f/))
was automatically generated
from [jaybee84/ml-in-rd@de7a952](https://github.com/jaybee84/ml-in-rd/tree/de7a952b2d7c9d43834c68d7d5f032012164c32f)
on August 10, 2020.
</em></small>

## Authors



+ **Jineta Banerjee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1775-3645](https://orcid.org/0000-0002-1775-3645)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaybee84](https://github.com/jaybee84)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Robert J Allaway**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3573-3565](https://orcid.org/0000-0003-3573-3565)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [allaway](https://github.com/allaway)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [allawayr](https://twitter.com/allawayr)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Jaclyn N Taroni**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4734-4508](https://orcid.org/0000-0003-4734-4508)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaclyn-taroni](https://github.com/jaclyn-taroni)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Casey Greene**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania; Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Justin Guinney**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1477-1888](https://orcid.org/0000-0003-1477-1888)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jguinney](https://github.com/jguinney)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>



## Synopsis {.page_break_before}

(Instructions: Describe the background, basic structure of the article, list material to be covered indicating depth of coverage, how they are logically arranged, include recent pubs in the area, 300-500 words)

Substantial technological advances have dramatically changed biomedicine by making deep characterization of patient samples routine. 
These technologies provide a rich portrait of genes, cellular pathways, and cell types involved in complex phenotypes. 
Machine learning is often a perfect fit for the types of data now being generated, and Nature Methods routinely has reports of machine learning methods that extract disease-relevant patterns from these high dimensional datasets. 
Often, these methods require a large number of samples to identify reproducible and biologically meaningful patterns. 
With rare diseases, biological specimens and consequently data, are limited due to the rarity of the condition. 
In this perspective, we outline the challenges and emerging solutions for using machine learning in these settings. 
We aim to spur the development of powerful machine learning techniques for rare diseases. 
We also note that precision medicine presents a similar challenge, in which a common disease is partitioned into small subsets of patients with shared etiologies and treatment strategies. 
Advances from rare disease research are likely to be highly informative for other applications as well.


## Introduction {.page_break_before}

Machine learning is gaining momentum in biomedical data analysis as data collection is increasingly high-throughput and algorithms or approaches become more transparent and interpretable. 

Application of machine learning to any dataset poses challenges, but the application to biomedical data and subsequent interpretation requires depth of knowledge not only in the biomedical domain but also a clear understanding of the methods and their underlying assumptions.

Rare disease research has not yet significantly benefited from machine learning applications for various reasons, including lack of statistical power in dataset size, heterogeneity in available data, and sensitivity of machine learning methods to misinterpretation in view of small datasets. 
We anticipate higher occurrence of such applications in the near future and aim to highlight the current state-of-art in this perspective.
However recent advances in the methodologies to accommodate rarity of samples and increased transparency in model outputs have encouraged application of machine learning in rare disease.


Application of machine learning to any kind of data consists of the following major steps: (1) data evaluation and question formulation, (2) selection of normalization/dimension reduction to mitigate technical differences, (3) selection of appropriate algorithms which select features to answer the formulated question, (4) evaluation of the answers generated by the algorithm. 
Each of these steps require the practitioner to choose from a variety of methodologies to apply. 
The selection of the methodologies at each of these steps need to be based upon robust reasoning to ensure stability of the results. 
Moreover, in the context of rare diseases, special considerations need to be made at each of the above mentioned steps to safeguard against misinterpretation of data.
Such considerations include incorporation of techniques that build upon prior domain-specific knowledge, methods that are resilient to challenges posed by small datasets, and methods that can mitigate technical disparities in the data.


### Techniques that build on prior knowledge and indirectly related data are necessary for many rare disease applications {.page_break_before}
This section will highlight promising approaches for analyzing rare disease data to extract biological insights. 
We will discuss techniques like transfer learning, representation learning, cascade learning, integrative analysis, and knowledge-graph creation and use that leverage other knowledge and data sources to construct testable hypotheses from rare diseases datasets with limited sample sizes.

#### Ensemble Learning 

Implementing machine learning on data with low sample size and high label uncertainty can lead to unstable predictions. 
In such cases where single predictors fail, various machine learning methods together or _ensemble learning_ may help increase accuracy of prediction.
_Ensemble learning_ methods like random forests use bootstrap aggregation (or _bagging_) of independent decision trees that use similar parameters but different paths for the selection of features to form a consensus about the important predictive features[@doi:10.1186/1472-6947-13-134, @doi:10.1023/A:1010933404324, @doi:10.1214/aos/1031689014, @doi:10.1177/2045894019890549, @doi:10/btzfh6].
However, successful application of consensus based ensemble learning requires "gold standard" data where the diagnosis or label of a data point in the training dataset has very little uncertainty (or "label-noise") associated with it [@doi:10.1093/jamia/ocw028]. 
In most cases of rare disease, due to the inherent nature of being less defined, the symptoms as well as any underlying biology comes with a reasonable amount of uncertainty (or "label-noise") leading to a _silver standard_ dataset[@doi:10.1109/TNNLS.2013.2292894, @doi:10.1093/jamia/ocw028].
In such cases, ensemble learning with multiple methods leveraging distinct underlying assumptions are used in tandem to capture stable patterns existing in the _silver standard_ data and reduce uncertainty.
Such _cascade learning_ classifiers have been widely used in image recognition where initially a small subset of image features are used to classify images (e.g. features identifying a face like eyes, nose, mouth). 
The initial classification is then augmented by more complex features and algorithms like AdaBoost ( _boosting_ ) that weight the various features implemented to detect the content of the image (e.g. features identifying a human face like relative distance between eyes etc.)[@doi:10.1109/CVPR.2001.990537, @doi:10.1007/978-3-540-75175-5_16, @doi:10.1109/ICPR.2004.1334680].

In rare diseases, a variant of _cascade learning_ that showed robustness in view of uncertainty in the data was implemented to identify rare disease patients from electronic health records from the general population [@pmid:30815073].
This implementation consisted of three steps each employing a different independent learning algorithm: (1) feature extraction to assign text words (from Pubmed literature) to diagnosis using word2vec [@arXiv:1301.3781v3], (2) preliminary prediction using an ensemble of decision trees with penalization for excessive tree-depth, (3) prediction refinement using similarity of data points to resolve sample labels and reiterating step (2).
In this implementation the algorithm was able to identify rare disease patients due to the robustness conferred by the independence of the feature extraction step and the prediction refinement step from the preliminary classification of the labeled dataset.
The classification step capitalized upon the information learned by the label prediction step preceding it and the prediction refinement step following it, and was able to perform better over other ensemble methods when implemented on silver standard data.

Most cascade classifiers follow _one-classifier-at-a-time_ approach where algorithms at each level predict all classes involved.
But scenarios where the need for high prediction accuracy for one class outweighs other classes (e.g. malignant tumor-types, or severe psychiatric cases) require further modification of the cascade learning efforts.
An example of this was seen implemented for triaging psychiatric patients where the identification of one class of psychiatric patients ("severe") far outweighed the need for optimized overall classification accuracy[@pmid:30380082].
Due to the requirements of the problem, they developed a _one-class-at-a-time_ approach for cascade learning, where at each stage a binary classifier is used to predict a specific class against all others.
The final model implemented all models together each identifying one class sequentially and the final prediction was the union of the predictions at all the different models.
The cascade classifiers using the _one-class-at-a-time_ approach were found to perform better than multi-class ensemble classifiers in most cases.

Thus ensemble learning can be helpful in producing stable predictions from data that is limited in quality or quantity, where single algorithms would otherwise produce unstable predictions.
However, the choice of using _bagging_, _boosting_, independent algorithmic steps, or _one-class-at-a-time_ approach would strictly depend on the nature of the prediction problem.
In most cases involving rare disease data, it seems that _bagging_ has had limited success, which has necessitated various modifications of the approaches as discussed above.


### Knowledge graphs for rare disease

An intrinsic constraint in the study of rare disease is the availability of large, normalized datasets. 
This limits our ability to study genotype-phenotype relationships or other key attributes of rare diseases. 
A potentially powerful strategy for evaluating genotype-phenotype relationships or repurposing drugs when large datasets are scarce or nonexistent is to develop and use knowledge graphs.
Knowledge graphs integrate related-but-different data types, creating a rich and complex data source. 
Examples of well-known public biomedical knowledge graphs and graph frameworks that could be useful in the rare disease context include the Monarch Graph Database[@doi:10.1093/nar/gkw1128], hetionet[@doi:10.7554/eLife.26726], PheKnowLator[@doi:10.1101/2020.04.30.071407], and the Global Network of Biomedical Relationships[@doi:10.1093/bioinformatics/bty114]. 
These graphs connect information like genetic, functional, chemical, clinical, and ontological data to enable the end user to explore many types of data and their relationships with disease phenotypes whether through manual review[@doi:10.1093/database/baaa015] or computational methods[@doi:10.1101/727925; @doi:10.1186/s12911-019-0938-1]. 
In the academic rare disease space, there a few pioneering examples of machine learning-based mining of knowledge graphs to repurpose drugs[@doi:10.1101/727925] and classify rare diseases[@doi:10.1186/s12911-019-0938-1].
These studies make it clear that there are some challenges in using machine learning using graph databases in rare disease.
For example, these papers rely on a gold standard dataset to validate the performance of the models; often, there are not robust gold standard datasets available for individual rare diseases.
These methods also evaluate a broad swath of rare diseases in a relatively unguided manner, rather than interrogating a pre-defined disease of interest.
Consequently, it is not yet clear how effective these approaches, and knowledge graphs in general, are in studying a specific disease of interest; more work needs to be done to identify methods that can provide actionable insights for a specific rare disease application. 
Beyond the aforementioned studies, there are very few examples of studies in the public domain that leverage knowledge graphs to characterize rare disease. 
However, private entities (e.g. healx, Boehringer Ingelheim, DrugBankPlus) have established partnerships and are performing an undisclosed amount of work to create and explore proprietary rare disease knowledge graphs for machine learning-based drug discovery applications.  
The formation of private companies pursuing this idea, as well as the availability of several public knowledge graphs with relevance to rare disease, suggests to us that this is a likely fruitful but generally untapped area of rare disease research in the public sphere. 
More work needs to be done to assess 1) which graph networks and network features best capture the salient information about rare diseases, 2) the utility of an array of knowledge graph analysis methodologies (such as graph neural nets, reinforcement learning approaches, and adversarial learning approaches)[doi:10.1109/TKDE.2020.2981333] to obtain actionable insights about rare diseases and 3) which problems - like drug discovery, identification of novel rare diseases, or assessment of genotype-phenotype relationships - can be meaningfully interrogated using machine learning of knowledge graphs.


#### Representation learning

Representation learning, also called feature learning, is the process of learning features from raw data, where a feature is an individual variable or property.
An algorithm or approach will construct features as part of training and, in the case of supervised feature learning, use those features to predict labels on input data. 
Using an example from transcriptomics, an unsupervised method such as matrix factorization can be used to extract a low-dimensional representation of the gene-level data, learning features that are a combination of input genes' expression levels [@doi:10.1093/bioinformatics/btq503; @doi:10.1186/s13059-020-02021-3].
Low-dimensional representations trained on a collection of transcriptomic data can then be used as input to supervised machine learning methods [@doi:10.1186/s12859-020-3427-8]. 
Supervised neural networks used in medical imaging studies [@doi:10.1016/j.procs.2016.07.014] (reviewed in [@doi:10.1016/j.zemedi.2018.11.002] and [@doi:10.1098/rsif.2017.0387]), which are trained to predict labels or classes, are also an example of representation learning.

Whether or not a learned representation or set of features is _useful_ depends on the task at hand.
In a supervised setting, it may be sufficient for a feature to distinguish between classes, but if we hope to use a feature for biological discovery, we may prioritize intepretability.
For example, learned features in the medical imaging domain may be a series of edges that constitute a blood vessel formation that discriminates between disease states or the learned features may not align with characteristics recognized as important by domain experts at all.
From transcriptomics data, learned features could be coordinated sets of genes involved in a biological process that are descriptive in some way [@doi:10.1038/s41467-020-14666-6], but do not necessarily distinguish cases from controls in our study.

In the rare disease domain, Dincer et al. leveraged publicly available acute myeloid leukemia (AML) gene expression data to improve the prediction of _in vitro_ drug responses [@doi:10.1101/278739]. 
The authors trained a variational autoencoder (VAE) on AML data that had been collected over time without the phenotypic information they were interested in–in this case, drug response. 
A VAE is an unsupervised neural network that learns a series of representations or encodings from data, where each learned attribute will have a probability distribution associated with it rather than a single value.
The authors used the learned attributes to encode a low-dimensional representation of held-out AML data with phenotype labels of interest, and used this low-dimensional representation as input to a classifier that predicted _in vitro_ drug response.

Representation learning tends to be data-intensive; many samples are required.
Though there were over 6500 publicly available AML samples from many different studies used as part of the training set in Dincer et al. [@doi:10.1101/278739], we expect that in other rare diseases considerably fewer samples will be available for training or, in the case of systemic diseases, studies may be from different tissues.
The study by Dincer and colleagues highlights another challenge: even if enough samples have been collected over time to support representation learning, these samples may not be associated with the deep phenotypic information that would maximize their scientific value.
In the next section, we will introduce methods or approaches that may be more broadly useful in rare diseases; representation learning underlies many of them.

#### Transfer, multitask, and few-shot learning

We focus on a series of approaches that are centered on the following concept: to realize the potential of machine learning for biological discovery in rare diseases, we often cannot study an individual rare disease alone as samples are limited.
Instead, we can build on prior knowledge and large volumes of data that do not directly assay our disease of interest, but are similar enough to be valuable for discovery.
We can leverage shared features, whether they are biological patterns that are a normal part of development aberrant in a particular disease context or an imaging anomaly present in both rare and common diseases, for advancing our understanding.
Methods that leverage shared features include transfer learning, multitask learning, and few-shot learning approaches. 

##### Transfer learning

Transfer learning is an approach where a model trained for one task or domain (source domain) is applied to another, typically related task or domain (target domain).
Transfer learning can be supervised (one or both of the source and target domains have labels), or unsupervised (both domains are unlabeled).
Though there are multiple types of transfer learning, we will focus principally on feature-representation-transfer [@doi:10.1109/TKDE.2009.191] here. 
Feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain [@doi:10.1109/TKDE.2009.191].
This concept is embodied in Dincer et al., where features are learned from unlabeled AML data and then used to encode a low-dimensional representation of AML data with _in vitro_ drug response labels [@doi:10.1101/278739].
The authors then used this low-dimensional representation as input to predict drug response labels–a supervised example.

In an unsupervised case, Taroni et al. trained a Pathway-Level Information ExtractoR (PLIER) [@doi:10.1038/s41592-019-0456-1] on a large generic collection of human transcriptomic data (recount2 [@doi:10.1038/nbt.3838]) and used the latent variables learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003].
(Here "unseen" refers to the fact that no AAV or medulloblastoma data were included in the training set.)
PLIER is a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways and gene expression data as input; PLIER utilizes regularization such that some latent variables learned by the model will align with input gene sets and, for those latent variables that are aligned with gene sets, latent variables will be associated with a limited number of gene sets [@doi:10.1038/s41592-019-0456-1].
We demonstrated that training on larger collections of randomly selected samples produced models that captured a larger proportion of input gene sets and were more suitable for disentangling closely related signals (e.g., type I and type II interferon signaling), suggesting that larger training sets produced PLIER models that are more valuable for biological discovery [@doi:10.1016/j.cels.2019.04.003].

Though models trained on general collections of transcriptomic data had more appealing properties, that alone does not guarantee suitability for describing rare diseases.
To assess suitability, we must find ways to examine the relevance of learned features (latent variables) to the rare disease gene expression data.
In Taroni et al., we found that the expression of latent variables that could be matched between the MultiPLIER model and a model trained on a rare disease dataset alone were well-correlated, particularly in the case of latent variables that were biologically relevant (i.e., significantly associated with input gene sets) [@doi:10.1016/j.cels.2019.04.003].
Despite the absence of AAV from the training set, MultiPLIER was able to learn a latent variable where the genes with the highest contributions encode antigens that the antineutrophil cytoplasmic antibodies (ANCA) form against in AAV and with higher expression in a group of samples from patients with AAV that were reported to have more severe disease [@doi:10.1002/art.27398].
The utility of this approach stems from the fact that biological processes are often _shared_ between conditions–the same ANCA antigen genes are components of normal neutrophilic granule development that is likely captured or assayed in the collection of transcriptomic data used for training.
MultiPLIER has additional attributes that make it practical for studying rare diseases: not all latent variables learned by a PLIER model are associated with input gene sets, and therefore may capture technical noise separately from biological signal, and we can use a single model to describe datasets from multiple tissues or cohorts that are not obviously directly comparable instead of attempting to reconcile results from multiple models (see _05.heterogeneity.md_).

Taken together, the DeepProfile [@doi:10.1101/278739] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] results suggest transfer learning can be beneficial for studying rare diseases. 
In the natural images field, researchers have demonstrated that the transferability of features depends on relatedness of tasks [@arxiv:1411.1792].
The limits of transfer learning for and the concept of relatedness in high-dimensional biomedical data assaying rare diseases are open research questions.
In the authors' opinion, selecting an appropriate model for a given task (e.g., using PLIER for biological discovery) and evaluation strategies that are well-aligned with a research goal are crucial for successful application of these approaches in rare diseases.


### Techniques and procedures must be implemented to manage model complexity without sacrificing the value of machine learning
Inherent challenges posed by low sample numbers in rare diseases are further aggravated by disease heterogeneity, poorly defined disease phenotypes, and often a lack of control (i.e. normal) data. 
Machine learning approaches must be carefully designed to address these challenges. 
We discuss how to implement methodological solutions like bootstrapping sample data, regularization methods for deep learning, and hyper-ensemble techniques to minimize misinterpretation of the data. 


#### Bootstrapping

Bootstrap or resampling computation is a powerful statistical technique that can be used for estimating population values from datasets of limited sample size [@doi:10.1080/01621459.1997.10474007].
The technique utilizes random sampling of data points from a dataset of limited sample size with replacement to approximate a larger population and estimate various population statistics (e.g. mean).
Subsequent iterations of resampling generates a distribution of the statistical value (mean) which minimizes the error of the estimate.
Bootstrap based techniques are used in conjunction with various learning methods to find the most informative models given a specific dataset (e.g. bootstrap aggregating or bagging used in random forests [@doi:10.1023/A:1010933404324; @doi:10.1198/0003130043277], bootstrap in neural networks [@doi:10/c8xpqz], or regression models [@doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]).

While most datasets in practice are of finite sample size and can benefit from bootstrapping, rare disease datasets with limited number of samples necessitate the use of bootstrap to form an informative dataset in addition to model selection [@doi:10.3390/genes11020226].
In this study, bootstrapping the training sample without replacement simulated formation of different incomplete datasets that helped expose the learning models (in this case random forests) to the incompleteness of the data.
Such additional bootstrapping of the training data helped create confidence intervals for the predictions and the important predictors originating from unstable ensemble models run on the incomplete training data


#### Regularization

Machine learning algorithms are optimized to find patterns among data points and prioritizes the strongest patterns that exist in a dataset.
Given a limited dataset with strong pre-existing technical differences between groups of samples, this optimization may lead to the model learning technical differences thus lowering its predictive accuracy [@doi:10.1073/pnas.1900654116].
For example, in a set of 1000 samples where 700 samples are from one healthcare site and 300 from another, it is likely that there will remain site-specific differences between them even after normalization of the samples. 
If the site-specific differences are more pronounced than the underlying patterns differentiating the samples, any machine learning model trained with these data will preferentially learn the site-specific differences to classify the samples, and rank them higher than the underlying patterns leading to a model showing high prediction accuracy of training data (termed low bias in model). 
When new test data points are introduced to the model, possibly coming from a third site, the model is unable to locate the earlier differences in the new data points and fails to classify them accurately causing a significant drop in accuracy of the model (termed high variance in model prediction).
Such a model is termed "overfit" to its training data.
Overfitting can lead to misinterpretation of the site-specific differences as true patterns in the limited data points and thus needs to be minimized. 
Minimization of overfitting can be accomplished by cross-validation and regularization methodologies. 

While cross-validation aims to reduce the variance in prediction, regularization adds a small amount of bias to the initial model to minimize its dependence and sensitivity to training data. 
Regularization makes models less reliant on training data by adding a penalty (determined by cross-validation), and then minimizes the error between the model's prediction and ground truth of the test data.
Regularization can not only minimize overfitting but can additionally help in predicting outcomes using a limited number of samples. 

Regularization can be of three main types, each with their particular strengths and weaknesses. 
(1) Ridge regression aims to minimize the magnitude of the features, but in models that try to select the most important features for accurate prediction of sample labels, ridge regression shrinks all features equally, but cannot completely remove unimportant features. 
Thus in presence of many correlated parameters (e.g. gene expression networks), ridge regression may not be ideal in reducing the feature space. 
(2) LASSO or least absolute shrinkage and selection operator regression on the other hand works well for selecting few important features since its effect can minimize the magnitude of some features more than the others. 
Thus it helps in selecting most important features while the magnitude of irrelevant features are shrunk to 0 and eventually removed.
This selection attribute of LASSO (in a sample set of size "n", LASSO can select "n" features for the model) may be an advantage in reducing model complexity, but a disadvantage in cases where identification of all possible collinear features is important (e.g. all biomarkers correlating to a particular disease phenotype) [@doi:10.1038/nmeth.4014]. 
(3) Elastic-Net regression is a combination of LASSO and ridge regression[@doi:10.1111/j.1467-9868.2005.00503.x]. 
Both of the methodologies when applied together helps to select most useful features, specially where there are a lot of correlated features. 
In this setup, LASSO leads to selection of one of the correlated features and reduces the others to 0 (grouping of features), and the magnitude of the selected features are then minimized through ridge regression. 

Any supervised learning implementation in rare disease would require robustness towards feature selection from a small number of samples, i.e. the features selected by a model as important should be stable in view of new data points added to a dataset, even though their relative importance may change due to additional evidence.
This robustness is mostly acquired through the combination of various regression strategies. 
Since machine learning applications in rare disease are infrequent, combination strategies used for rare variant discovery and immune cell signature discovery can serve as good case studies to examine. 
Many deleterious genomic variants can be extremely rare due to the constant selection pressure working against them. 
Since the frequency of a rare variant is so low (less than 1%) applying routine statistical procedures that were extensively developed for common variant association, to analyze a low minor allele frequency (MAF) seem inappropriate [@doi:10.1038/nrg2867]. 
For its feature selection attribute, LASSO has been widely applied in microarray and GWAS data for common variants. 
But since LASSO by itself is too stringent for rare variants, it has been employed along with group penalties to help identify rare variants/ low frequency predictors [@doi:10.1093/bioinformatics/btq448]. 
Variations of LASSO have also been implemented to aggregate or group the occurrence of rare variants together by gene or chromosome location [@doi:10.1002/gepi.21746; @doi:10.1186/1753-6561-5-S9-S100; @doi:10.1016/j.ajhg.2008.06.024]. 
In this strategy, a 0–1 dummy variable was created for each SNP based on the presence or absence of the rare variant. 
Then linear combinations of the selected dummy variables were considered by using the LASSO procedure. 
Even though most of the dummy variables were 0, their linear combination was more likely to be nonzero thus leading to increased signal to noise ratio for the rare variants. 
Only those linear combinations that were non-zero in at least 5% of the subjects were then included to ensure that the new markers were not rare [@doi:10.1186/1753-6561-5-S9-S113; @doi:10.1186/1753-6561-5-S9-S100]. 
While ridge regression is not usually utilized for feature selection, adaptive ridge regression has been utilized to help combine rare variants into a single score analogous to feature engineering for increasing the signal of rare variants[@doi:10.1371/journal.pone.0044173]. 
Another variation of LASSO included its integration with the probabilistic logistic bayesian approach to identify a protective rare variant in lung cancer[@doi:10.4137/CIN.S17290]. 
Xu et al. on the other hand combined the feature selection methods with a generalized pooling strategy, and evaluated the performance of these hybrid approaches for detection of rare genetic variants[@doi:10.1371/journal.pone.0041694]. 
Another interesting approach is the sparse-group LASSO approach which incorporates prior knowledge into the regularization[@doi:10.1080/10618600.2012.681250]. 
This approach works well for a scenario where only few genes in a pathway are true predictors of a phenotype, where it helps select the driving genes in a pathway of interest. 

Alternatively, Elastic-net regression (a combination of LASSO and ridge regression) has also been used to reduce the feature space in various types of cancer datasets [@doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198]. 
In cases where the number of features were far greater than the number of samples, elastic-net has usually been found to outperform the other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x]. 
A variation of the elastic-net regression was used for identifying immune cell signatures in an RNA-seq dataset where the number of cells sampled were far fewer than number of genes profiled [@doi: 10.1186/s12859-019-2994-z]. 
This two-step regularized logistic regression technique included a pre-filtering phase to select the optimal number of genes and then implemented elastic-net regularization for gene selection. 
The second step generated gene signatures for individual cell types using selected genes from first step and then implemented a binary regularized logistic regression for each cell type against all other samples



Still to add: 
techniques in deep learning e.g.
Deep and shallow architecture:
https://ieeexplore.ieee.org/document/7863293


### Techniques to manage disparities in data generation are required to power robust analyses in rare diseases

As with common diseases, genomic and transcriptomic data from rare diseases can suffer from artifacts introduced by batch, processing methodology, sequencing platform, specimen/data quality or other non-biological phenomena. 
The consequences of these non-biological artifacts are amplified in rare diseases which often have few samples and heterogeneous phenotypes.
Furthermore, because datasets are many times pieced together from multiple small studies, in which disease phenotypes or other important biological characteristics are often confounded by the previously mentioned "batch" factors. 
A key consideration here is, if possible, active dialogue with the data generators or experts in the field who may have unexpected insights into potential sources of variation. 
One example of the value of this, experienced by the authors, occurred when studying tumors associated with the disease neurofibromatosis type 1. 
These datasets were, unbeknownst to the computational biologists, generated from samples obtained with vastly different surgical techniques (laser ablation and excision vs standard excision), resulting in substantial biological differences that are a consequence of process, not reality. 
One might expect, in this example, that this technical decision would result in profound changes in the underlying biology, such as the activation of heat shock protein related pathways, unfolded protein responses, and so on. 
Consequently, careful assessment of confounding factors and implementation of normalization methods is important to identifying biologically meaningful features within a dataset. 
Assessment of confounding factors and heterogeneity in rare disease datasets is perhaps most easily performed using unsupervised learning approaches such as clustering and dimensionality reduction. 
Clustering methods like k-means clustering or hierarchical clustering can be used to characterize the structure present in many different types of data such as genomic or imaging data. [@doi:10.1186/1471-2105-9-497;@doi:10.1109/JBHI.2013.2276766]. 
Similarly, a variety of dimensionality reduction methods are can be used to visualize sample heterogeneity and potential confounding variables, including multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP), among many others. [@doi:10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202; @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @arXiv:1802.03426]
All of these methods can be effectively used to identify batch effects and other structure in the data, though some, like t-SNE and UMAP, have parameters, such as perplexity (number of nearest neighbors), that can substantially affect the output, and thus the interpretation, of the analysis [@doi:10.23915/distill.00002;@arXiv:1802.03426]. 
Therefore, successful application of these methods requires a sufficient understanding of the underlying method and parameter sweeping to get a clear picture of the structure of the underlying data. 
Another important consideration is that, as discussed by Way, et. al. [@doi:10.1186/s13059-020-02021-3], a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; testing multiple methods may result in a more comprehensive portrait of the data 
Dimensionality reduction techniques are not restricted to 'omic' data - they can also be used in rare disease applications to characterize the structure and heterogeneity of imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], and others.
Once the nature of the non-biological heterogeneity has been established, different techniques can be used to correct the differences. 
Common approaches to ameliorate non-biological effects include the assessment of data quality using robust metrics, reprocessing the raw data using a single analysis pipeline if the data are obtained from different sources, application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], normalization of raw values (e.g. z-scores, trimmed mean of M-values [@doi:10.1186/gb-2010-11-3-r25]).
It can also be helpful to be fatalistic, in some sense, when working with rare disease data. 
For various reasons including ethical considerations, limited funding, and limited biospecimen availability, experimental design and the resulting data will be less-than-ideal - for example - when batch variables and biological variables are confounded. 
In these cases, it may be prudent to take a step back, re-evaluate the data, and identify methods that can operate within these constraints. 


### Conclusions
>We will conclude by discussing the potential of the above-mentioned approaches in rare diseases and other biomedical areas where data is scarce.


### draft
>this is a test file to differentiate draft-branch from master


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
