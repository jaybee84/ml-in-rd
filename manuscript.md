---
author-meta:
- Jineta Banerjee
- Robert J Allaway
- Jaclyn N Taroni
- Casey Greene
- Justin Guinney
bibliography:
- content/manual-references.json
date-meta: '2020-12-09'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"citation_title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"og:title\" content=\"Machine learning methods for rare diseases\" />\n<meta property=\"twitter:title\" content=\"Machine learning methods for rare diseases\" />\n<meta name=\"dc.date\" content=\"2020-12-09\" />\n<meta name=\"citation_publication_date\" content=\"2020-12-09\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Jineta Banerjee\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-1775-3645\" />\n<meta name=\"citation_author\" content=\"Robert J Allaway\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-3573-3565\" />\n<meta name=\"twitter:creator\" content=\"@allawayr\" />\n<meta name=\"citation_author\" content=\"Jaclyn N Taroni\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-4734-4508\" />\n<meta name=\"citation_author\" content=\"Casey Greene\" />\n<meta name=\"citation_author_institution\" content=\"Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania\" />\n<meta name=\"citation_author_institution\" content=\"Childhood Cancer Data Lab, Alex\u2019s Lemonade Stand Foundation\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-8713-9213\" />\n<meta name=\"citation_author\" content=\"Justin Guinney\" />\n<meta name=\"citation_author_institution\" content=\"Sage Bionetworks\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0003-1477-1888\" />\n<link rel=\"canonical\" href=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"og:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta property=\"twitter:url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://jaybee84.github.io/ml-in-rd/\" />\n<meta name=\"citation_pdf_url\" content=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://jaybee84.github.io/ml-in-rd/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://jaybee84.github.io/ml-in-rd/v/3399e73a30cd6065fb621a502bc35fd7559b858c/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/3399e73a30cd6065fb621a502bc35fd7559b858c/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://jaybee84.github.io/ml-in-rd/v/3399e73a30cd6065fb621a502bc35fd7559b858c/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- rare disease
- machine learning
- transfer learning
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Machine learning methods for rare diseases
...






<small><em>
This manuscript
([permalink](https://jaybee84.github.io/ml-in-rd/v/3399e73a30cd6065fb621a502bc35fd7559b858c/))
was automatically generated
from [jaybee84/ml-in-rd@3399e73](https://github.com/jaybee84/ml-in-rd/tree/3399e73a30cd6065fb621a502bc35fd7559b858c)
on December 9, 2020.
</em></small>

## Authors



+ **Jineta Banerjee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1775-3645](https://orcid.org/0000-0002-1775-3645)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaybee84](https://github.com/jaybee84)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Robert J Allaway**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3573-3565](https://orcid.org/0000-0003-3573-3565)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [allaway](https://github.com/allaway)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [allawayr](https://twitter.com/allawayr)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>

+ **Jaclyn N Taroni**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4734-4508](https://orcid.org/0000-0003-4734-4508)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jaclyn-taroni](https://github.com/jaclyn-taroni)<br>
  <small>
     Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Casey Greene**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania; Childhood Cancer Data Lab, Alex’s Lemonade Stand Foundation
  </small>

+ **Justin Guinney**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1477-1888](https://orcid.org/0000-0003-1477-1888)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [jguinney](https://github.com/jguinney)<br>
  <small>
     Sage Bionetworks
     · Funded by Neurofibromatosis Therapeutic Acceleration Program; Children's Tumor Foundation
  </small>



## Synopsis {.page_break_before}

(Instructions: Describe the background, basic structure of the article, list material to be covered indicating depth of coverage, how they are logically arranged, include recent pubs in the area, 300-500 words)

Substantial technological advances have dramatically changed biomedicine by making deep characterization of patient samples routine.
<!-- TODO: What about basic research? We probably need to address that somewhere too. -->
These technologies provide a rich portrait of genes, cellular pathways, and cell types involved in complex phenotypes.
Machine learning is often a perfect fit to extract disease-relevant patterns from these high dimensional datasets.
Often, these methods require many samples to identify reproducible and biologically meaningful patterns.
With rare diseases, biological specimens and consequently data, are limited due to the rarity of the condition.
In this perspective, we outline the challenges and emerging solutions for using machine learning in these settings.
We aim to spur the development of powerful machine learning techniques for rare diseases.
We also note that precision medicine presents a similar challenge, in which a common disease is partitioned into small subsets of patients with shared etiologies and treatment strategies.
Advances from rare disease research are likely to be highly informative for other applications as well.


## Introduction {.page_break_before}

Rare disease research is increasingly becoming dependent on high-throughput profiling of samples and would greatly benefit from applications of machine learning (ML) in their analysis. 
Analyzing such high dimensional data from rare samples (fewer than 200,000 cases in the United States [@https://www.fda.gov/media/99546/download]) is challenging. 
It requires specialized computational methods that can learn from sparse yet rich data such that the learned patterns can be generalized to newly acquired data [@doi:10.1016/j.ebiom.2019.08.027]. 
Rare disease research has substantial constraints to consider when using ML methods, including lack of statistical power due to small dataset size (typically ranging from 20 to 99 for a rare disease [@doi: 10.1186/s13023-020-01424-6]), heterogeneity in available data, and sensitivity of ML methods to misinterpretation and unstable performance in view of small datasets. 
For example, successful training of ML models require training datasets made of “gold standard” data where the diagnosis or label of a data point has very little uncertainty (or “label-noise”) associated with it [@doi:10.1093/jamia/ocw028]. 
Due to the limited understanding of the biology of  rare diseases, the symptoms or disease labels often come with a reasonable amount label-noise leading to a silver standard dataset[@doi:10.1109/tnnls.2013.2292894]. 
A systematic review of application of ML in rare disease in the last 10 years unveiled 211 human data studies in 74 different rare diseases employing ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%). 
The review also showed that most studies used ML for diagnosis (40.8%) or prognosis (38.4%), but studies aiming to improve treatment were infrequent (4.7%) [@doi:10.1186/s13023-020-01424-6]. 
Moreover, in the context of rare disease, special considerations need to be made to safeguard against misinterpretation of results. 
Rare disease datasets are often limited in size and/or assembled from combining data from multiple institutions  from differently processed specimens collected with geographical and chronological disparity. 
 Consequently, data analysis techniques must be resilient to challenges posed by small sample sizes, as well as technical artifacts present in aggregated data. In this perspective, we discuss techniques for understanding the nature of rare disease data, including those that address or better tolerate the limitations of these data.


### Manage complex high-dimensional rare disease data

In rare diseases, the ability to get an enormous number of measurements from a vanishingly small number of samples using high-throughput methods is both the upside and the downfall of these methods. These ‘omic’ methods generate highly dimensional data - that is, data with many features (e.g. all of the mRNA transcripts in a sample). 
However, statistical interrogation of  this large number of measurements requires an abundance of samples or observations, which is often not the case in rare disease. The sparsity of samples in rare diseases gives rise to the “curse of dimensionality” (i.e. few samples but many features), which can be a major impediment in analyzing feature-rich data in sample-deficient contexts [@doi:10.1038/nrc2294] [TODO: Clarify further why this can be an impediment]. 
This problem is further aggravated by the presence of highly correlated features that relate to disease relevant information. Furthermore, rare disease data collection and aggregation methods can add to these challenges by introducing technical variability into the data at hand. 
In this section, we will discuss strategies like simplifying data and addressing technical artifacts through dimension reduction  which can help mitigate these challenges. 
Dimensionality reduction methods including unsupervised approaches like multidimensional scaling (MDS), principal components analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP) [@doi: 10.1007/978-3-540-33037-0_14; @doi:10.1098/rsta.2015.0202, @https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf; @https://arxiv.org/abs/1802.03426] can help ‘compress’ information from a large number of features into a smaller number of features. 
These  techniques can be applied to characterize imaging data [@doi:10.1016/j.media.2020.101660], mass cytometry data [@doi:10.1038/ncomms14825], ‘omics data, and others. These methods not only help in reducing the number of features, but can also be used to visualize structure or artifacts in the data (e.g. [@doi:10.1038/s41467-019-13056-x]), define subgroups within data (e.g. [@doi:10.1038/s41467-020-15351-4], or for feature selection/extraction during application of specific machine learning models[@doi:10.1007/978-3-030-03243-2_299-1].

Rare disease datasets, like many other scenarios, can contain structure unrelated to the biology of the disease; e.g. structure related to batch, sample preparation methodology, or sequencing platform [@doi:10.23915/distill.00002]. 
The consequences of these artifacts are amplified when samples are rare and the cohort contains several phenotypes. Furthermore, datasets are often combined from multiple small studies where biological characteristics are confounded by technical variables. 
We can leverage dimensionality reduction methods like PCA, MDS, t-SNE, and UMAP to identify the effect of these variables on the data. All of these methods can be used to identify batch effects and other structures in the data, though some (like t-SNE and UMAP) may require tuning of hyperparameters that affect the output [@https://arxiv.org/abs/1802.03426; @doi:10.23915/distill.00002]. 
Way, et. al. [@doi:10.1186/s13059-020-02021-3] further suggests that a single dimensionality reduction method alone may not be sufficient to reveal all of the technical or biological heterogeneity; thus testing multiple methods may result in a more comprehensive portrait of the data. 
Additional important considerations for using dimensionality reduction methods such as criteria for selecting a dimensionality reduction method and interpretation of results are discussed in detail by Nguyen and Holmes.[@doi:10.1371/journal.pcbi.1006907] 

Beyond dimensionality reduction, unsupervised learning approaches such as k-means clustering or hierarchical clustering have also been used to characterize the structure present in genomic or imaging data. [@doi:10.1186/1471-2105-9-497; @doi:10.1109/jbhi.2013.2276766] 
If non-biological heterogeneity is detectable, common approaches like  reprocessing the raw data using a single analysis pipeline (if the data are obtained from different sources), application of batch correction methods [@doi:10.1093/biostatistics/kxj037; @doi:10.1093/nar/gku864], and normalization of raw values[@doi:10.1186/gb-2010-11-3-r25] may be required to obtain value from these datasets. 

Dimensionality reduction is, in fact, a type of representation learning (or feature learning), a process of learning low-dimensional representations or composites of features from raw data. 
Each learned composite feature becomes a new variable - representing a combination of original features - thereby reducing the dimension of the dataset. 
Representation learning through matrix factorization can extract composite features from transcriptomics datasets made of combinations of gene expression levels found in the training data that are descriptive in some way [@doi: 10.1038/s41467-020-14666-6], and use them to interpret test input data [@doi:10.1093/bioinformatics/btq503 ; @doi:10.1186/s13059-020-02021-3]. 
Putting constraints on the features learned by the model, through regularization, can help ensure that the learned representations are biologically relevant [TODO: REF]. 
Low-dimensional representations trained on a collection of transcriptomic data can also be used as input to supervised machine learning methods[@doi:10.1186/s12859-020-3427-8; @doi:10.1016/j.procs.2016.07.014; @doi:10.1098/rsif.2017.0387]

Representation learning generally requires many samples  in complex biological systems and thus may appear to aggravate the curse of dimensionality. 
But it can be a powerful tool to learn low-dimensional patterns from large datasets and then find those patterns in smaller, related datasets. 
In the later sections of this perspective, we will discuss this method of leveraging large datasets to reduce dimensionality in smaller datasets, also known as feature-representation-transfer. 


### Manage model complexity while preserving the value of machine learning

Fruitful translation of patterns extracted from a rare disease dataset using machine learning into testable hypotheses requires the applied models to be a) stable i.e. the same predicted features should surface from the data if the model is run multiple times and, b) simple to improve interpretability and avoid misinterpretation due to technical challenges. Fulfilling these prerequisites is challenging in rare disease datasets where there is high label-uncertainty (i.e. where the label given to a data point may not be correct due to imperfect understanding of the disease). 
In this section we highlight a few common ML techniques that can help improve the stability and simplicity of ML models applied to rare disease data.
Techniques like bootstrapping and ensemble learning can increase stability in machine learning predictions. Bootstrapping is a powerful statistical technique where resampling the data with replacements can help estimate population values from datasets of limited sample size [@doi:10.1080/01621459.1997.10474007].
While  resampling with replacement is commonly used to find models that are robust to overfitting ([@doi:10.1023/a:1010933404324; @doi:10.1198/0003130043277; @doi:10.1016/s0925-2312(01)00650-6; @doi:10.1016/j.neucom.2004.11.017; @doi:10.1002/sim.4780111607]); resampling without replacement, when applied to rare disease data generated confidence intervals for the model predictions by iteratively exposing the models to incomplete datasets (mimicking real world cases where most rare disease datasets are incomplete) [@doi:10.3390/genes11020226].

Stability in predictions can also be achieved by combining various ML methods together (ensemble learning). 
Ensemble learning methods like random forests use bagging of independent decision trees that use similar parameters but different paths to form a consensus about the important predictive features [@doi:10.1023/a:1010933404324; @doi:10.1186/1472-6947-13-134; @doi:10.1214/aos/1031689014; @doi:10.1177/2045894019890549 ; @doi:10.1016/s0031-3203(02)00169-3]. 
But such methods have shown limited success in rare disease datasets where the label-uncertainty can be high due to imperfect understanding of the disease (i.e. silver standard datasets). 
This has led to the adoption of cascade learning, where multiple methods leveraging distinct underlying assumptions are used in tandem. The methods may be augmented with algorithms like AdaBoost (boosting) to capture stable patterns existing in the silver standard data [@doi:10.1109/cvpr.2001.990537; @doi:10.1007/978-3-540-75175-5_16; @doi:10.1109/icpr.2004.1334680]. 
Cascade learning implemented to identify rare disease patients from electronic health records from the general population utilized independent steps for feature extraction (using natural language processing based word2vec [@https://arxiv.org/abs/1301.3781v343]), preliminary prediction (using an ensemble of decision trees with penalization for excessive tree-depth), and prediction refinement (using similarity of data points to resolve sample labels) [@pmid:30815073]. 
Combining these three methods resulted in better performance than other methods when implemented on the silver standard dataset in isolation.
The presence of multiple phenotypes (or classes) in rare disease datasets further decreases the available data-points per class. 
In such cases, a one-class-at-a-time cascade learning approach (where at each stage a binary classifier predicts a specific class against all others) has been found to produce simpler models that perform better compared to multi-class ensemble classifiers [@doi:10.1093/jamia/ocy109]. 

Simplification of models by making the feature space proportionate with the sample space can also be achieved through regularization. 
Regularization can not only protect ML models from poor generalizability that results from overfitting (where the model performs well for the training data but poorly for new test data) [@doi:10.1073/pnas.1900654116], but also help penalize model complexity and reduce the feature space to build simpler models using limited datasets. 
Three popular  regularized methods include ridge regression, LASSO, and elastic-net. Ridge regression can minimize the magnitude of the features, but cannot remove unimportant features. 
LASSO regression, on the other hand, works well for selecting few important features since it can minimize the magnitude of some features more than the others [@doi:10.1038/nmeth.4014]. 
A combination of LASSO and ridge, elastic-net regression [@doi:10.1111/j.1467-9868.2005.00503.x] selects the most useful features, especially in presence of a large number of correlated features.
Studies leveraging regularization in rare variant discovery and immune cell signature discovery can provide important insights into its possible application in rare disease. 
In rare variant discovery, ridge regression has been utilized to combine rare variants into a single score to increase the signal of rare variants [@doi:10.1371/journal.pone.0044173], while LASSO was implemented along with group penalties to identify rare variants/low frequency predictors [@doi:10.1038/nrg2867; @doi:10.1093/bioinformatics/btq448]. 
Hybrid applications of LASSO have also been tested in rare variant discovery, including boosting the signal of rare variants by capturing combinations of variants [@doi:10.1016/j.ajhg.2008.06.024; @doi:10.1186/1753-6561-5-s9-s113], integration with a probabilistic logistic Bayesian approach [@doi:10.4137/cin.s17290], combining feature selection methods with a generalized pooling strategy [@doi:10.1371/journal.pone.0041694], and incorporating prior knowledge into the regularization step to select driver genes in a pathway of interest [@doi:10.1080/10618600.2012.681250]. 
In immune cell signature discovery, elastic-net regression has been used to reduce the feature space and was found to outperform other regression approaches [@doi:10.1111/j.1467-9868.2005.00503.x; @doi:10.1016/j.compbiomed.2015.10.008; @doi:10.1186/1471-2105-14-198 ; @doi:10.1186/s12859-019-2994-z]. 
Regularization methods like LASSO or elastic-net have been methods of choice for making models simpler by reducing the feature space; these methods should be explored while working with rare disease datasets. 

Thus by employing bootstrapping, ensemble learning, and regularization methods, researchers may be able to better generate stable, simple models that identify reliable biological phenomena underlying rare diseases.


### Build upon prior knowledge and indirectly related data {.page_break_before}

Rare diseases often lack large, normalized datasets, limiting our ability to study key attributes of these diseases. 
Thus evaluating genotype-phenotype relationships or repurposing drugs using knowledge graphs can greatly benefit rare disease. 
Knowledge graphs (KGs) integrate related-but-different data types, creating a rich data source (e.g. Monarch Graph Database[@doi:10.1093/nar/gkw1128], hetionet[@doi:10.7554/elife.26726], PheKnowLator[@doi:10.1101/2020.04.30.071407], and the Global Network of Biomedical Relationships[@doi:10.1093/bioinformatics/bty114], Orphanet[@http://www.orpha.net]). 
These graphs connect genetic, functional, chemical, clinical, and ontological data to enable the exploration of relationships of data with disease phenotypes through manual review[@doi:10.1093/database/baaa015] or computational methods[@doi:10.1101/727925; @doi:10.1186/s12911-019-0938-1].
KGs may include  links or nodes that are specific to the rare disease of interest (e.g. an FDA approved treatment  would be a specific disease-compound link in the KG ) as well as links that are more generalized (e.g. gene-gene interactions noted in the literature for a different disease). 

Rare disease researchers can leverage the entities and relationships outside of the specific disease-context, including common comorbidities that are more prevalent conditions[@doi:10.1101/727925], for prediction. 
Such approaches have been used in rare disease research in areas such as drug repurposing[@doi:10.1101/727925] and disease classification[@doi:10.1186/s12911-019-0938-1]. 
Identifying KG encoding methods that can provide actionable insights for a specific rare disease application is an active area of research. 
Other approaches that build upon prior knowledge and large volumes of related data include transfer learning, multitask learning, and few-shot learning approaches. 
These approaches leverage shared features, e.g. normal developmental processes that are aberrant in disease, or an imaging anomaly present in rare and common diseases, for advancing our understanding of rare diseases. 

Transfer learning, where a model trained for one task or domain (source domain) is applied to another related task or domain (target domain), can be supervised or unsupervised. 
Among various types of transfer learning we will mainly focus on feature-representation-transfer. 
Feature-representation-transfer approaches learn representations from the source domain and apply them to a target domain.[@doi:10.1109/tkde.2009.191]
For example, low-dimensional representations can be learned from tumor transcriptomic data and transferred to describe patterns associated with genetic alterations in cell line data [@doi:10.1186/s13059-020-02021-3].

Other approaches related to transfer learning, multitask and few-shot learning, are forms of supervised learning that often  rely on deep neural networks. 
In multitask learning classifiers use shared representations to learn multiple related but individual predictions (tasks) simultaneously [@doi:10.1023/a:1007379606734]. Few-shot learning on the other hand generalizes a model trained on related tasks to a new task with limited labeled data (e.g., the detection of a patient with a rare disease from a low number of examples of that rare disease). 
While various approaches and architectures underlie multitask and few-shot learning (see [@https://arxiv.org/abs/1706.05098; @https://arxiv.org/abs/1707.08114v2; @https://arxiv.org/abs/1904.05046v3] for an overview), we will delve into a few selected studies to illustrate potential uses and limitations of these approaches in rare disease.

Examination of the effects of dataset size and task relatedness on multitask learning performance improvements (“multitask effect”) in drug discovery showed that smaller datasets tended to benefit most from multitask learning and the addition of more training data did not guarantee improved performance for multitask models [@https://arxiv.org/abs/1606.08793]. 
Another study demonstrated that performance gains were context-dependent, i.e., multitask neural networks outperformed single-task networks for predicting complex rare phenotypes from EHR data, but not common phenotypes [@https://arxiv.org/abs/1808.03331]. The top-performer in a recent DREAM challenge for predicting drug sensitivity in cancer cell lines, including cell lines from rare cancers, was a multitask learning approach [TODO: CTD-squared Chemogenomic DREAM Challenge citation]. 
From these studies and others, it is clear that multitask learning is a promising approach for rare disease research albeit with some important, context-specific limitations.
In contrast, one-shot or few-shot learning uses prior knowledge to generalize a distance metric learned from input data to compare with a low number of new examples for prediction [@https://arxiv.org/abs/1904.05046v3, @doi:10.1021/acscentsci.6b00367], e.g. a method developed for predicting small molecule activity learned a meaningful distance metric over the properties of various compounds [@doi:10.1021/acscentsci.6b00367]. 
But the authors’ results also suggest that structural similarity among compounds was a requirement for this desired performance boost. 
In a study of rare pathologies in fundus photographs, a few-shot learning approach had a performance advantage over multitask learning, since predicting common conditions simultaneously resulted in a loss of performance for the multitask learner [@doi:10.1016/j.media.2020.101660]. 
Thus transfer, multi-task, and few-shot learning are appealing for the study of rare diseases, conditions, or phenotypes, but their limits and potential utility are still open research questions. 
Nevertheless, selecting an appropriate model for a given task and evaluations that are well-aligned with a research question are crucial for applying these approaches in rare diseases.


#### Using composite approaches can be a powerful strategy

We have described multiple approaches for maximizing the success of machine learning applications in rare disease research throughout. 
In practice, it is rarely sufficient to use one of these techniques in isolation. 
Below, we highlight  two recent works – one supervised and one unsupervised – in the rare disease domain that draw on multiple concepts covered in the earlier sections. 
Feature-representation-transfer, which incorporates dimension reduction through representation learning,  prior data, and regularization underlie both approaches.

In a method termed DeepProfile, publicly available acute myeloid leukemia (AML) gene expression data was leveraged to improve the prediction of in-vitro drug responses [@doi:10.1101/278739]. 
The authors trained a variational autoencoder (VAE), on AML data that had been collected over time without the desired phenotypic information (drug response). 
The authors used the learned attributes to encode a low-dimensional representation of held-out AML data with drug response labels, and used this representation as input to a supervised classifier that predicted in vitro drug response, which outperformed using the original features. 
The study by Dincer and colleagues highlights another challenge: samples collected as part of multiple studies may not be associated with the deep phenotypic information that would maximize their scientific value, but this does not preclude the use of these samples in learning representations.

In an unsupervised case, Taroni et al. trained Pathway-Level Information ExtractoR (PLIER) [@doi:10.1038/s41592-019-0456-1] on a large generic collection of human transcriptomic data (recount2 [@doi:10.1038/nbt.3838]) and used the latent variables learned by the model to describe transcriptomic data from the unseen rare diseases antineutrophil cytoplasmic antibody (ANCA)-associated vasculitis (AAV) and medulloblastoma in an approach termed MultiPLIER [@doi:10.1016/j.cels.2019.04.003] (Here "unseen" refers to the fact that these diseases were not in the training set).
PLIER is a matrix factorization approach that takes prior knowledge in the form of gene sets or pathways and gene expression data as input. 
PLIER includes constraints (regularization) such that some latent variables learned by the model will align with input gene sets and ideally latent variables will only be associated with a low number of related gene sets [@doi:10.1038/s41592-019-0456-1], which make it suitable for biological discovery or description of rare disease data. 
MultiPLIER allows us to use one model to describe multiple datasets instead of reconciling output from multiple models, which is highly beneficial when identifying commonalities among disease manifestations or affected tissues is a research goal.

Taken together, DeepProfile [@doi:10.1101/278739] and MultiPLIER [@doi:10.1016/j.cels.2019.04.003] suggest a combination of the techniques discussed throughout this article can be capitalized on for rare disease research. 
In cases where we have few samples from our disease of interest with the required phenotypic labels, we can leverage existing collections of data and knowledge if we select the models with the right attributes. 

The utility of DeepProfile and MultiPLIER stem from the fact that biological processes can be shared between biological contexts and that the methods underlying the approaches can effectively learn about those processes. 
In the natural images field, researchers have demonstrated that the transferability of features depends on relatedness of tasks [@arxiv:1411.1792]. 
The limits of transfer learning for and the concept of relatedness in high-dimensional biomedical data assaying rare diseases are open research questions. 
In the authors' opinion, selecting an appropriate model for a given task and evaluations that are well-aligned with a research goal are crucial for applying these approaches in rare diseases .


## Definitions {.page_break_before}

### Unsupervised learning: 
Machine learning algorithms which can learn features from unlabeled training data (e.g. datasets where the samples do not have disease or phenotype labels) to predict the class or phenotype of new or unseen test data are part of unsupervised learning. Examples of unsupervised learning include principal component analyses, multidimensional scaling, UMAP, t-SNE, k-means clustering etc [TODO - add REFs].

### Supervised learning: 
Machine learning algorithms that require training data with specific phenotype labels are part of supervised learning. 
Such algorithms learn correlations of features with the phenotype labels and use the learned correlations to predict the phenotype labels of unseen or new test data.

### VAE: 
Variational Autoencoders or VAEs are unsupervised neural networks that use hidden layers to learn or encode representations from available data while mapping the input data to the output data. 
VAEs are distinct from other autoencoders since the distribution of the encodings are regularized such that they are close to a normal distribution, which may contribute to learning more biologically relevant signals [@doi:10.1186/s13059-020-02021-3].


### Outlook


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
